{
    "queries": {
        "fdefadc1-5386-4521-ba8c-b8415a7ee656": "How does AMIE, the AI system introduced in the document, aim to improve the accessibility, consistency, and quality of care in medicine?",
        "b8b3aeb6-72f9-4aef-b91b-9b3e1b376ba3": "What are the potential benefits of developing AI systems capable of clinical and diagnostic dialogue, as mentioned in the document?",
        "c9cff0af-315c-40c6-88b3-8057e9a738e3": "How does AMIE's self-play based simulated dialogue environment contribute to its capabilities in diagnostic dialogue?",
        "ee1c1b2d-f0be-47d9-824e-669cab8fe6d6": "In what ways does AMIE outperform Primary Care Physicians (PCPs) in the diagnostic dialogue, according to the blinded remote Objective Structured Clinical Examination (OSCE)?",
        "3c7e673a-bbaa-4043-b4d5-f962e3016fd9": "How does AMIE (Articulate Medical Intelligence Explorer) contribute to the development and evaluation of medical AI systems for history-taking and diagnostic dialogue in medicine?",
        "a96db4a4-3b5d-43de-8847-2025ebefccc2": "What were the findings of the blinded remote OSCE study comparing AMIE to PCPs in terms of diagnostic accuracy and communication skills?",
        "800d2057-eff4-43ce-be1d-5b986312dce7": "What types of real-world datasets were used to develop AMIE, an AI system for diagnostic dialogue?",
        "3c75daca-8f2d-4b40-93cc-651a4c1e3e01": "How many questions were included in the MedQA dataset used for medical reasoning in the training set?",
        "f8337577-849f-40d5-b3db-09c064b69ff5": "In the medical summarization dataset, how many notes were selected from each category and what were the criteria for their selection?",
        "ed1f6c92-7780-4fb5-82b5-9c2dc2e86723": "What challenges are faced when using real-world dialogue data for training LLMs for medical conversations, and how were these challenges addressed in the study?",
        "bb3dee80-5e01-4ba2-af5d-b3e8c2860eea": "What are the three key components of the multi-agent framework used to generate simulated dialogues in the medical context?",
        "af127072-d920-404b-93b2-b8c99903b99c": "How does the vignette generator contribute to the generation of high-quality simulated dialogues in the medical context?",
        "5d88b6c0-10ce-478f-a391-d3d2bcd60a77": "In the simulated dialogue generator, what are the roles and objectives of the patient agent, doctor agent, and moderator?",
        "b7a634a6-5b2b-4c96-855e-d4bde37ea318": "How does the turn-by-turn dialogue simulation progress in the simulated dialogue generator?",
        "1cd09ee7-8f41-4e94-a00b-3e94c8dce05d": "As a teacher, I would like to assess your understanding of the doctor-patient conversation. Can you explain the purpose of the self-play critic in the context of improving diagnostic conversations?",
        "77594f0e-5efb-46c2-bb2d-1106356c0c6b": "In the context of medical dialogues, what are the criteria that the critic agent uses to evaluate the doctor agent's responses?",
        "ab3c4db3-8770-419c-9afb-852bc6efa1b5": "In the chain-of-reasoning strategy employed by AMIE for diagnostic dialogue, what are the three sequential steps involved? How does this strategy enhance diagnostic accuracy and maintain positive rapport with the patient?",
        "b7d1e16b-dbcf-4f60-a4d2-314429a8e51b": "How did the evaluation framework for assessing the quality of physicians' expertise in history-taking in clinical consultations differ from prior works? What criteria were used in the framework and how were they derived?",
        "48d0f02e-5861-41b5-8e7f-f5a6e00dac3f": "In the Objective Structured Clinical Examination (OSCE), what is the main difference between traditional written or oral exams and the OSCE format? How does the OSCE assess clinical skills and competencies?",
        "9cf9828c-bde1-408a-b96e-6e9b19413daf": "In the remote OSCE study, what were the roles and responsibilities of the primary care physicians (PCPs) and patient actors? How were the scenario packs and simulated patients prepared for the study?",
        "fddb491f-99e9-42a6-ac1b-85f9237a464a": "In the online text-based consultations, what were the instructions given to the PCPs and patient actors to ensure familiarity with the interface and experiment requirements?",
        "daeef8a1-4a74-464a-be97-f1cc6fdfa0a5": "How were the post-questionnaires used to evaluate the consultations? What specific components were included in the post-questionnaire for patient actors and the OSCE agent?",
        "dc694428-f64b-4a95-b5f6-06e152f294a2": "In the context of the study, what is the purpose of the auto-evaluation analyses conducted? Provide examples of the specific analyses performed and their objectives.",
        "aa627c43-43cb-4d50-ae15-f3fda18b7715": "Compare the diagnostic accuracy of AMIE and PCPs under specialist physician evaluation. How does AMIE's performance compare to that of PCPs, and in which medical specialties did AMIE show the most improvement?",
        "579352dc-625b-4e1b-a9ad-12be9698a63b": "In the context of the study, what is the significance of comparing AMIE's diagnoses based on its own consultations with AMIE's diagnoses generated from the corresponding PCP consultations?",
        "df50194a-1c8a-41cb-9ca2-4261122c8443": "How did the conversation quality between AMIE and PCPs differ according to the assessments of patient actors, specialist ratings, and auto-evaluation outputs?",
        "741870b4-aee2-4cac-a611-7a3309f6d95d": "In the context of patient-centered communication, what are some key qualities that contribute to a positive patient experience? Provide examples from the given context information to support your answer.",
        "a39ccc99-a274-45c2-b963-99f315f430e8": "How are patient actors' ratings used to assess the quality of consultations? Explain the rating scale used and how it is adapted from various assessment tools mentioned in the context information.",
        "05e0217a-d6a5-48e6-8de8-8b39cd431dab": "In the context of the study, what were the main differences in ratings between AMIE and PCPs in terms of conversation qualities assessed by patient actors and specialist physicians?",
        "cd85269a-555e-4a8a-afc3-87d6dbba56ff": "How did the auto-evaluation method demonstrate the effectiveness of inner self-play for AMIE in terms of simulated dialogue quality improvement?",
        "29bff076-1264-46a9-9a9f-9287238af717": "In the context of patient-centered communication, what are some key qualities that specialist physicians should possess? Provide examples of these qualities and explain their importance in healthcare settings.",
        "11cb276f-671c-469a-a788-8e39cdba5f1f": "How are specialist physicians' ratings determined in terms of conversation and reasoning qualities? Discuss the rating scales used and their significance in assessing specialist physicians' performance.",
        "0b463137-7387-4072-b922-03a4f21466ec": "How have previous studies evaluated the performance of AI systems in diagnostic dialogue, and how do these evaluations compare to the criteria used to assess human physicians' dialogue and communication skills?",
        "fbad2c5d-42fe-4612-b9bb-9158b3715275": "In the study, how did the diagnostic performance of the AI system, AMIE, compare to that of board-certified PCPs? What factors contributed to AMIE's ability to provide accurate and complete differential diagnoses?",
        "b603a3d3-f18a-4986-a740-305f0a26c5ae": "In the study on oversight for AI systems in safety-critical settings such as medicine, what were the observed differences in performance between PCPs and AMIE in obstetric/gynecology and internal medicine scenarios compared to other specialties? Explain the significance of these findings.",
        "bfd2d060-2f1e-4322-b8ad-8f34495918f7": "How did the performance of both AMIE and PCPs vary between consultations performed in the Canada OSCE lab and those enacted in the India OSCE lab? Were the differences statistically significant? Discuss the implications of these findings in terms of diagnostic accuracy.",
        "58728f8a-a122-4929-ab8c-1719a7ea8b64": "In the context of evaluating AI-generated clinical dialogue, what are the limitations of the evaluation framework used in this study? How can future research improve upon this framework?",
        "ed37dfe2-46ca-4005-96ec-8b58e1b1467f": "How does the evaluation framework used in this study differ from prior works on AI-generated clinical dialogue? What specific criteria were considered in the evaluation, and how did the inclusion of these criteria increase the clinical relevance of the research?",
        "0c9555da-dcc9-4f19-9955-970eb520a1aa": "How does the inability to incorporate non-verbal symptoms, signs, and communication features impact the paradigm of \"virtual OSCE\" in healthcare assessments?",
        "e29c8074-925e-4a34-be8b-8cd75830ba4a": "What are the potential risks and challenges associated with using language models in medical diagnostic dialogue, particularly in terms of fairness, bias, and equity?",
        "29c31df6-4f41-4510-bdda-672b2b5f0d5f": "What are some potential ethical considerations that need to be addressed when deploying medical AI systems, according to the context information?",
        "972fd9ff-2eda-4b97-bccf-41d4749df1f7": "How can medical AI systems be improved to better interact conversationally and ensure the safety, reliability, fairness, efficacy, and privacy of the technology, as mentioned in the context information?",
        "cb52868d-bd4e-40a6-8a53-4b7a60118ba3": "According to the references provided, what are the relative contributions of history-taking, physical examination, and laboratory investigation in making medical diagnoses?",
        "10b32b48-2b63-439c-b56d-22506e45913a": "Can you explain the significance of iterative hypothesis testing in teaching clinical medicine, as mentioned in one of the references?",
        "6b3ce94b-e7fb-4b69-a4ce-850f19306c7b": "How does the Clinical Camel medical language model differ from other medical language models mentioned in the context information?",
        "83d81df1-08ce-4a4f-91bc-70f9eda8cb4a": "What are the potential applications of the MEDITRON-70B medical pretraining model mentioned in the context information?",
        "00711dd8-93b3-4ec7-be8f-0e82b4751674": "In the context of medical dialogue generation, what is the significance of incorporating medical knowledge into transformer-based language models? Provide examples from the document to support your answer.",
        "9ab6089f-6ab2-4f75-ad41-7e7ed100da40": "How does the objective structured clinical examination (OSCE) contribute to competency-based evaluation in the medical field? Discuss the advantages and limitations of using OSCE as a method for assessing professional competence, referring to the information provided in the document.",
        "51e8b732-c5bf-4fec-b5ef-dfaac4f61fce": "How does communication skills education impact medical student performance, according to the research cited in the context information?",
        "5cc70a3c-a5b6-4b32-9427-b7a79685ba73": "What are the key components of a communication curriculum in undergraduate medical education, as outlined in the UK consensus statement mentioned in the context information?",
        "86fdcc1f-3c9c-43c4-9a6e-ff7bf6464725": "In the context of task-oriented dialogue modelling, what is the significance of the Multiwoz dataset mentioned in reference 50?",
        "4fc436b0-7301-460c-a151-e047c261c876": "How does the paper referenced as 52 propose to address the limitations of traditional neural networks in natural language processing?",
        "e785b065-2d2d-4ed7-ab38-4ab7c554dcce": "In the context of medical question answering, what was the objective of the medical question answering task at TREC 2017 LiveQA? Provide a brief overview of the task and its significance.",
        "fc65f087-a9f9-488f-8146-f72d8e2904fd": "How can digital and online symptom checker tools be evaluated in terms of diagnostic and triage accuracy? Discuss the findings of the systematic review mentioned in the context information.",
        "d8210816-1758-4612-af3f-ce2e9f66166d": "In the context of healthcare, what are the potential benefits and challenges associated with using artificial intelligence models for differential diagnosis? Provide examples from the given context information to support your answer.",
        "59517675-877b-4126-bfe7-ae7b7379bca2": "Discuss the effectiveness of teleconsultations in primary care based on the systematic review mentioned in the context information. What are the advantages and limitations of this approach?",
        "68bf9842-5653-44bc-96f5-75e153ab5ed7": "In the field of medicine, what is the significance of patient-centered interviewing and how does it contribute to the overall quality of care? Provide examples from the given context information to support your answer.",
        "6e0df301-604b-4926-8b9c-e85b3bb11333": "Discuss the potential risks and challenges associated with the use of language models in the medical field, as highlighted in the provided context information. How can these risks be mitigated to ensure fairness and accountability in healthcare settings?",
        "fd3eb574-9d9c-4b89-910f-a1995785c2b0": "How do red teaming language models contribute to reducing harms and improving safety in the field of natural language processing? Provide examples from the given context information.",
        "0fb4f0a8-695f-4fa5-827c-426162a91338": "Compare and contrast the approaches of model reporting and model documentation in ensuring fairness, accountability, and transparency in the development and deployment of language models. Support your answer with references to the relevant papers mentioned in the context information.",
        "bd565145-9af9-482b-9b5d-21128c746f70": "In the context of responsible AI, what is the significance of data cards as discussed in the paper by Pushkarna, Zaldivar, and Kjartansson?",
        "0c4fd24b-3152-4ffa-b6ca-5f6c8ff4f40b": "How do multilingual pre-trained language models fare in terms of linguistic fairness, according to the research presented in the paper by Choudhury and Deshpande?",
        "dbe7f47f-2b3f-486c-ac84-ea593bee0a7a": "What is the main focus of the paper \"Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\" by Chen and Mueller?",
        "c9b542ab-a5a0-4310-a578-f7e72dbf87ad": "How does the paper \"Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models\" by Huang et al. contribute to the understanding of uncertainty in language models?",
        "33e49c56-578b-4fcc-8f3a-67f1cf71ac7b": "How does the General Medical Council Patient Questionnaire (GMCPQ) assess the performance of doctors? Provide examples of criteria that are evaluated.",
        "17d9c38c-c79e-4d6c-9330-7b5e71c28b73": "What are the different scales used in the GMCPQ to rate the doctor's performance? Explain the options available for each criterion.",
        "ca5f01dd-1a6e-4822-9027-a1214e6983ee": "In the Practical Assessment of Clinical Examination Skills (PACES), what are the key aspects assessed under the category of Clinical Communication Skills? Provide examples of the different scales used to rate the doctor's performance in eliciting various types of information from the patient.",
        "4715a883-a7f7-42a7-8b6f-5567e0daf660": "In the PACES rubric, what are the different areas evaluated under the category of Maintaining Patient Welfare? Explain the scale used to rate the doctor's performance in terms of maintaining the patient's welfare and provide an example of a scenario where the doctor's actions would be considered as jeopardizing patient safety.",
        "ef731992-9c64-4cdc-b7de-9944569dd444": "In the context of patient-centered communication, what are some criteria that assess a doctor's behavior in fostering a relationship with the patient? Provide examples of how a doctor can demonstrate these criteria.",
        "dfa26bb0-15e7-48f6-99be-cfc9083aecc2": "How does a doctor gather information from a patient in a patient-centered communication approach? Discuss the importance of open-ended questions and active listening in this process.",
        "d6c2c799-6d89-414f-b1bc-7503eb408f3a": "In the context of diagnosis, explain the difference between the appropriateness and comprehensiveness of a doctor's differential diagnosis. Provide examples to support your explanation.",
        "5b19913a-98dd-4041-ace0-661c622a5caa": "Discuss the importance of appropriate investigations and treatments in the management of a patient's condition. Explain why it is crucial for doctors to avoid inappropriate investigations and treatments. Provide examples to illustrate your points.",
        "b09bb4e8-2254-4d35-aec6-819e128b3e5e": "In the case study, what are the common symptoms experienced by the 55-year-old female with carpal tunnel syndrome? How do these symptoms affect her daily activities?",
        "ce4721cc-c4ff-4c3b-9052-44c575ecd297": "Based on the dialogue, what are the potential causes of the patient's symptoms? How can these conditions be managed or treated?",
        "3aab7f9b-0399-4ed1-9bc4-cdc18522b074": "How can wrist splinting at night help alleviate symptoms of hand numbness and tingling?",
        "df9e7150-a0c3-4a8f-8568-8cb458a08311": "What are some potential causes of hand weakness and pain, and how can an ergonomic evaluation of the workstation help address these symptoms?",
        "eaa8f55e-7537-40cc-94ff-dc2f2005b4f5": "How does carpal tunnel syndrome typically manifest and what are the potential causes?",
        "32c28d4e-4219-44c9-9835-c8354236859f": "What are the recommended conservative measures for treating carpal tunnel syndrome and when should surgery be considered as an option?",
        "a83c0429-4cff-4db4-8f10-1795bcbc1f01": "How does the interface for online text-based consultation in Figure A.2 differ from the interface for patient actor ratings in Figure A.3?",
        "ae16ee96-78d1-4f52-87ae-052313bc2541": "What is the purpose of the interface shown in Figure A.3 for patient actor ratings?",
        "d4ab33a8-73c1-4304-bf9e-51238441cdbe": "How does the interface for Specialist Physician Evaluation contribute to the overall healthcare system?",
        "27fc4886-9192-4896-94c0-627dad7c3df7": "What are the key features or functionalities that the interface for Specialist Physician Evaluation offers to specialist physicians?",
        "87068e7f-3f5e-4411-a231-5d324ee90153": "Based on the consultation between the patient actor and AMIE, describe the patient's symptoms of chest pain and discomfort in the upper stomach. How did the patient describe the chest pain and what other areas of the body did they experience discomfort in?",
        "5e7c6ff4-b9d8-43e2-bd06-630206386d41": "In the consultation, AMIE suggests two possible causes for the patient's symptoms: angina and GERD. Explain the difference between these two conditions and how their symptoms may overlap with each other.",
        "813709cb-da5d-4197-9ed0-fc8c91f48760": "What are the potential causes of chest pain mentioned in the context information? Provide at least three examples and briefly explain each one.",
        "c52910bf-b22a-4d20-92f8-f408a6651e2d": "Explain the importance of seeking immediate in-person evaluation for chest pain, even if it turns out not to be a heart attack. Discuss the potential risks and the need for prompt attention to rule out serious causes.",
        "27f8c9cc-6a90-434a-ab1c-36e2cf25c94d": "In the example PCP consultation, what are the potential risk factors for the patient's chest pain? Provide specific details about the patient's health history and family history that contribute to these risk factors.",
        "4cedc87a-d136-4594-807b-db528ca04729": "Based on the patient's symptoms and medical history, what are the probable and plausible alternative diagnoses for their chest pain? List at least two probable diagnoses and explain why they are likely, as well as one plausible alternative diagnosis.",
        "e143f232-95bf-4a0b-9769-418bca253231": "What are the different degrees of matching used to evaluate the specialist's differential diagnosis (DDx) in the OSCE study? Explain each degree of matching and provide an example for each.",
        "14d5c7d6-ba30-48be-a663-dc76bd18f599": "Compare the DDx accuracy between consultations conducted by AMIE and a PCP in the OSCE study. Which matching levels showed statistically significant differences in accuracy between the two? Explain the significance of these findings in the context of the study.",
        "73f0b1e4-542c-4bdf-a83d-5e1920981973": "In which specialty did AMIE's performance match or surpass PCPs performance for all scenarios?",
        "e7f70748-c2a0-4d52-801b-4a592184d9cc": "What were the top 3 matches for AMIE and PCP in the Gastroenterology specialty based on specialist ratings?",
        "95b66e27-08e3-4f72-9d70-8620dc50044a": "In the context of auto-evaluation on DDx, what is the purpose of using Med-PaLM 2?",
        "d90cf5a2-8c5a-4851-88ac-038800279253": "How does the auto-evaluator's performance align with specialist assessments in terms of DDx accuracy?",
        "953ff682-9577-4ad8-96af-6acc94e7adcc": "How did the researchers compare the diagnostic accuracy of AMIE based on its own consultations versus consultations with the PCP?",
        "635be117-1243-4b31-80e0-6796ae87b835": "According to the results shown in Figure A.10, what was the finding regarding the diagnostic quality of AMIE's differential diagnoses when processing information from its own dialogue versus the PCP's conversation?",
        "f727c619-fc05-4664-9433-d3e0df045072": "How does the distribution of words and turns in OSCE consultations affect the fairness between the two agents in making a diagnosis?",
        "85316968-6d3c-4985-850c-0882cd4f60b9": "What does the graph in Figure A.12 indicate about the relationship between the number of turns and the diagnostic accuracy of AMIE-generated differential diagnoses?",
        "67100f99-ffdb-4869-a65e-b955c6d4081f": "In the study comparing specialist ratings for scenarios conducted in Canada and India, what were the findings regarding the diagnostic performance of AMIE and PCP in each location?",
        "c3dbb3de-0b2c-429b-94a8-6b8decad05af": "How did the auto-evaluation ratings of AMIE and PCPs compare for the scenarios conducted at both testing locations?",
        "796e97bd-fea9-46f1-8c25-f587d8674f61": "How did the researchers validate the auto-evaluation rankings in relation to the specialist ratings on the clinical criteria? Provide examples from the study.",
        "ddb439f8-c48c-4623-8332-3cfe95d0464b": "What were the different prompting strategies used in the auto-evaluation procedure? How did the two-step process compare to other methods in terms of performance?",
        "9d4e7ba4-ccba-40b8-be30-bae7e39710bf": "In the prompt for explanation generation, what are the criteria used to rate the quality of the doctor-patient dialogue?",
        "4b6468de-9d77-48e6-86c1-f0ea749bb075": "Can you provide an example of a good aspect and a bad aspect of a dialogue with respect to the criterion of maintaining patient welfare?",
        "a9fe7ab0-89f6-4c7b-b1da-b7af8c5317ab": "How does the auto-evaluation prompt for qualitative criteria in the doctor-patient dialogue work? Provide an example of a rating and explanation for a dialogue.",
        "7aecad5b-40ef-430e-8255-fdea74c6b975": "As a teacher/professor, how would you design a question that assesses the students' understanding of the rating scale used in the auto-evaluation prompt for the dialogue?",
        "01a60666-dcfc-46d2-9d76-100287dd0cd8": "What is the purpose of the \"Rank-order Agreement of Auto-evaluation to Specialist\" in the context information? How does it contribute to the overall evaluation process?",
        "62972067-e827-4f76-9bd0-9aeceef7b778": "Can you explain the significance of the alphanumeric codes used in the context information? How do they relate to the evaluation process?",
        "4659ddce-7893-47aa-8908-60508b93597a": "What is the significance of the characters \"ni0000004d/uni00000052/uni0000004b/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000025/uni00000048/uni00000048/uni00000056/uni00000049/uni00000057/uni00000057/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000027/uni00000053/uni00000052/uni00000047/uni00000049/uni00000056/uni00000052/uni00000057\" in the given context information?",
        "d5335fa5-f87c-4841-ad5f-142be63ce890": "How does the sequence \"/uni00000031/uni00000045/uni0000004d/uni00000052/uni00000058/uni00000045/uni0000004d/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000034/uni00000045/uni00000058/uni0000004d/uni00000049/uni00000052/uni00000058/uni00000004/uni0000003b/uni00000049/uni00000050/uni0000004a/uni00000045/uni00000056/uni00000049\" contribute to the overall context information?",
        "ee8f76f6-9c44-4d42-9174-b4a14a121d41": "Based on the context information provided, what is the significance of leveraging the self-CoT technique for the auto-evaluation of clinical criteria?",
        "d397d07e-41c8-442b-9399-de16696081c1": "How does the rank-order agreement to specialist ratings of all 149 dialogue pairs compare when using different auto-evaluation prompting techniques?",
        "ed997b97-4adc-4e78-9d43-6cc77f0d5294": "In the given context information, what technique was chosen for the auto-evaluation of clinical criteria? Why was this technique selected?",
        "8a2c84cf-9282-417d-a43f-31967bf382b9": "Based on the figure provided, what can be inferred about the rank-order agreement to specialist ratings of dialogue pairs when comparing various auto-evaluation prompting techniques?",
        "5b7d59e7-1b91-48a4-8cd4-9d309bd1b28a": "What is the significance of the sequence \"/uni00000037/uni00000049/uni00000049/uni0000004f/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000025/uni00000048/uni00000048/uni00000056/uni00000049/uni00000057/uni00000057/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000027/uni00000053/uni00000052/uni00000047/uni00000049/uni00000056/uni00000052/uni00000057\" in the document? Explain its relevance and any possible implications.",
        "27d6b506-1801-4701-a22a-9ec71236c656": "How does the sequence \"/uni00000031/uni00000045/uni0000004d/uni00000052/uni00000058/uni00000045/uni0000004d/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000034/uni00000045/uni00000058/uni0000004d/uni00000049/uni00000052/uni00000058/uni00000004/uni0000003b/uni00000049/uni00000050/uni0000004a/uni00000045/uni00000056/uni00000049\" contribute to the overall understanding of the document? Discuss its possible implications and any connections to other parts of the text.",
        "39ad5a50-c794-4018-a27a-e1e2754254bc": "Based on the given context information, what is the significance of the sequence \"uni00000056/uni00000049/uni0000005a/uni00000045/uni00000050/uni00000049/uni00000052/uni00000047/uni00000049\"?",
        "593503b0-7fdf-4666-84bb-7364d12aafd7": "How does the sequence \"uni00000025/uni00000050/uni00000058/uni00000049/uni00000056/uni00000052/uni00000045/uni00000058/uni\" relate to the overall context information provided?",
        "8dad0839-c81c-4e78-925d-789233fe4ba9": "In the given context, what does the rank-order agreement to specialist ratings indicate about the self-CoT auto-evaluation technique compared to alternative specialists?",
        "f180f098-3ce3-4b27-b19b-b2566d182db9": "How does the rank-order agreement with a strategy of randomly guessing according to the distribution of specialist preferences for each criteria compare to the rank-order agreement obtained with a random ranking of the AMIE and PCP dialogues, as shown in Figure A.18?",
        "22baa103-1278-4567-9615-16cfb6c8ff6b": "In the context of auto-evaluation of simulated dialogues with self-play, what is the purpose of using self-play in the evaluation process? Provide a brief explanation.",
        "a2350142-2c74-4c23-b0df-b059b62c2906": "How does the use of simulated dialogues contribute to the assessment of auto-evaluation in the context of self-play? Explain the role of simulated dialogues in this process.",
        "77cd36a0-5a4c-4e2d-8efc-4dc916327978": "What is the significance of the characters \"uni00000049/uni00000048/uni0000004b/uni00000049/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000039/uni00000052/uni00000048/uni00000049/uni00000056/uni00000057/uni00000058/uni00000045/uni00000052/uni00000048/uni0000004d/uni00000052/uni0000004b/uni00000031/uni00000045/uni0000004d/uni00000052/uni00000058/uni00000045/uni0000004d/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000034/uni00000045/uni00000058/uni0000004d/uni00000049/uni00000052/uni00000058/uni00000004/uni0000003b/uni00000049/uni00000050/uni0000004a/uni00000045/uni00000056/uni00000049/uni00000037/uni00000049/uni00000049/uni0000004f/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000025/uni00000048/uni00000048/uni00000056/uni00000049/uni00000057/uni00000057/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000027/uni00000053/uni00000052/uni00000047/uni00000049/uni00000056/uni00000052/uni00000057/uni00000037/uni0000004c/uni000000\"?",
        "d337c508-08f3-4c34-80a7-f46e2e1db79c": "How does the context information provided relate to the field of study or subject matter being discussed?",
        "f9fa1b53-cf38-4429-b844-d04bcd19e50a": "Based on the given context information, what is the significance of the sequence \"ni00000057/uni00000057/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000027/uni00000053/uni00000052/uni00000047/uni00000049/uni00000056/uni00000052/uni00000057/uni00000037/uni0000004c/uni00000053/uni0000005b/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000029/uni00000051/uni00000054/uni00000045/uni00000058/uni0000004c/uni0000005d\"? Explain its purpose and any patterns or meanings it may hold.",
        "f7098e06-76d3-4433-b38c-fd99a91f0e28": "In the given context information, there are multiple sequences of characters represented by \"uni\" followed by a series of numbers. What could be the possible significance of these sequences? How do they contribute to the overall understanding of the document?",
        "4218c815-44c8-40d9-8ec6-c6be37407eb6": "How does the auto-evaluation assess the quality of the self-play dialogues after one round of critique compared to the baseline dialogue generated without revision?",
        "c976af23-9e31-4be6-80fb-e18de53f2887": "Can you explain the significance of the preference shown by the auto-evaluation for the self-play dialogues after one round of critique in terms of dialogue quality?",
        "f3819fb5-eb88-481a-8be7-62538c4f79a1": "What is the primary use of the AMIE model in the medical field and what are some areas of research it can contribute to?",
        "3ef4b6d8-6407-4269-8f39-596f968914c7": "How was the AMIE model initialized and fine-tuned, and what datasets were used for training and evaluation purposes?",
        "8fd16141-14b8-426e-b6bd-562d7fe21291": "How does Mask R-CNN extend the Faster R-CNN framework for object instance segmentation? Explain the key components and advantages of Mask R-CNN compared to existing methods.",
        "7c27536c-8213-466d-bfdc-43e6bb807eae": "What challenges does instance segmentation pose and how does Mask R-CNN address them? Discuss the importance of the mask branch and the proposed RoIAlign layer in achieving accurate segmentation results.",
        "b6615ff5-0a31-41f2-8e48-01aa8478d62f": "In the context of the Mask R-CNN results on the COCO test set, what is the significance of RoIAlign and how does it impact mask accuracy?",
        "72326ffd-4744-48be-b1e4-93c3fdcb0c66": "How does Mask R-CNN differ from FCNs in terms of predicting masks and class categories, and what are the advantages of decoupling these predictions?",
        "20909419-bd1e-4fc8-a6e7-089162987cba": "How does Mask R-CNN compare to the winner of the 2016 COCO keypoint competition in terms of performance and speed?",
        "7fdf6387-5ead-467e-8ca1-c82038568de8": "What are some advantages of using the R-CNN approach for instance segmentation compared to other methods mentioned in the document?",
        "1fe765e3-d2ab-493a-bea8-de4628bcc420": "How does Mask R-CNN differ from Faster R-CNN in terms of its outputs and requirements for object detection?",
        "e6b2a542-bc93-4c58-8edf-b017bdb0b39b": "What is the significance of the RoIAlign layer in the mask prediction process of Mask R-CNN?",
        "f96cf066-8346-4e7c-9909-57f14162b202": "How does the RoIAlign layer address the misalignment issue introduced by RoIPool in predicting pixel-accurate masks? Provide a detailed explanation of the proposed change and its impact on the extracted features.",
        "49573868-aca3-4b9f-a047-cf167fd7c6a2": "Compare and contrast the RoIAlign and RoIWarp operations in terms of their approach to alignment in feature extraction. Discuss the advantages and disadvantages of each method based on the experimental results mentioned in the document.",
        "40be646b-ecea-4086-adf7-f6ed8d81f308": "What are the two types of network architectures used in the Mask R-CNN approach? Explain the difference between them.",
        "bd98f058-2358-4668-9580-76e3a3ffa308": "How does using a ResNet-FPN backbone for feature extraction improve the accuracy and speed of Mask R-CNN? Provide details on the architecture and its benefits.",
        "79170a04-72a1-4bdd-8f51-8c7e6639f841": "What are the differences between the head architectures used for the ResNet C4 and FPN backbones in the Faster R-CNN model?",
        "e655b2c1-5151-42ae-84d7-8c73b499b932": "How does the mask branch in the Mask R-CNN model affect the inference process and what is its purpose?",
        "78602528-d0c5-4ca4-90ae-97ba8571a8dc": "In the given context information, identify three different types of vehicles mentioned. Provide their respective confidence scores as well.",
        "8724a663-3cfa-4c0b-a393-d353a4484e08": "Among the objects mentioned in the context information, which one has the highest confidence score? Explain why this object stands out compared to others in terms of its confidence level.",
        "587d88b6-2dc0-44d5-a215-df5fdc142b9b": "Based on the given context information, identify three objects that are commonly found in a dining area. Explain their relevance in a dining setting.",
        "adac1e31-bc72-4169-828d-b4d9411a1e6d": "In the given context, there are mentions of various animals. Choose two animals mentioned and describe their characteristics and habitats.",
        "8f4b623c-5aed-4b0f-9f22-46c29faaa33d": "In the context of instance segmentation, compare the performance of Mask R-CNN with ResNet-101-FPN backbone to the winners of the COCO 2015 and 2016 segmentation challenges, MNC [7] and FCIS [21]. How does Mask R-CNN outperform these models?",
        "58f5eced-1312-4fab-a942-ba1592662144": "Explain the training and evaluation process of Mask R-CNN in the context of instance segmentation. How does Mask R-CNN utilize the trainval35k and minival subsets of val images, and what is the significance of test-dev [23]?",
        "e2a07764-a98b-4ba0-a296-589bd8b7b150": "What are the main results of the Mask R-CNN model compared to previous state-of-the-art models in instance segmentation? How does it perform against MNC and FCIS, the winners of the COCO 2015 and 2016 segmentation challenges?",
        "118d1856-990c-4e5b-96c7-64749e8caa47": "What ablation experiments were conducted to analyze the Mask R-CNN model? How did the different architectures, such as various backbones and advanced designs, impact the performance of the model?",
        "a4210546-5b8b-44a6-a063-8256af1ce2da": "In the context of object detection models, what are some factors that contribute to improved performance? Provide examples from the given document.",
        "2c7de4f4-a274-4448-8828-2aaea284777f": "Compare and contrast the RoIPool and RoIAlign techniques in the Mask R-CNN model. How do they impact the accuracy of mask prediction?",
        "e0d2bc5d-55b3-4970-be46-dd9993ad8b9c": "What is the difference between using fully convolutional networks (FCN) and multi-layer perceptrons (MLP) for mask prediction in Mask R-CNN? How do FCNs improve the results compared to MLPs?",
        "23b815c7-e279-458f-8ef5-0714217210f7": "What is the purpose of the RoIAlign layer in Mask R-CNN? How does it improve the accuracy of mask prediction and address the challenge of using large-stride features for detection and segmentation?",
        "861187fc-6477-435c-9ea6-31da640b81cd": "What are some of the improvements made by Mask R-CNN over previous state-of-the-art models in object detection? Provide specific details and mention the gains achieved in terms of APbb.",
        "9c6f9d50-2f42-47b0-a77f-75c535241729": "How does the use of FCNs (Fully Convolutional Networks) in the mask branch of Mask R-CNN contribute to better segmentation results compared to MLPs (Multi-Layer Perceptrons)? Explain the performance gain observed and the choice of backbone for a fair comparison.",
        "bb6ab0f9-cb6f-489b-b008-5573b750e563": "What is the significance of multi-task training in closing the gap between object detection and instance segmentation in Mask R-CNN?",
        "aa2e8c51-c218-4c4b-baf7-09d2b928a09e": "How does the ResNet-101-C4 variant of Mask R-CNN impact the timing and practicality of the model?",
        "9aaf1708-2f9e-4fc0-9d9c-f813911b3a7b": "What is the keypoint AP of the Mask R-CNN model (ResNet-50-FPN) on COCO test, and at what frame rate does it run?",
        "66280694-34b1-4f7d-9ae5-016f0c7dfe87": "How does the person keypoint AP achieved by our method compare to the COCO 2016 keypoint detection winner, and what are the advantages of our method over the winner's approach?",
        "a5f97d4d-062d-4467-bdb3-1a12f0f51f1f": "How does the addition of the mask branch improve the performance of the box-only and keypoint-only versions in multi-task learning?",
        "661ecc31-6540-44e3-bc85-73318a46365e": "What is the significance of RoIAlign over RoIPool in keypoint detection, and why does it show a significant improvement in accuracy?",
        "2529befc-a624-4701-822b-2db69560ea48": "In the context of object detection and semantic segmentation, what are some key techniques and models discussed in the document? Provide examples and briefly explain their contributions.",
        "1f15780c-8a1c-41be-977f-e60518c00303": "How do convolutional neural networks (CNNs) play a role in various tasks such as human pose estimation, object detection, and semantic segmentation? Discuss the different CNN architectures and their applications mentioned in the document.",
        "05f205fb-79e2-40ed-a9e3-d39e35d94311": "How do spatial transformer networks contribute to effective detection proposals? Provide examples from the document to support your answer.",
        "404e0251-7537-430b-9fa8-82d9c0b205e1": "Compare and contrast the Faster R-CNN and the Fully Convolutional Networks (FCN) for object detection. Discuss their respective advantages and disadvantages based on the information provided in the document.",
        "fb1010a5-ada9-49f6-8cde-d727a4c9f0a8": "In the context of object detection, what is the significance of online hard example mining? Provide a brief explanation and mention the relevant paper from the given context information.",
        "f1280492-6191-4782-b20a-f0005102cccf": "How do residual connections impact learning in deep neural networks? Discuss the findings of the paper \"Inception-v4, inception-resnet and the impact of residual connections on learning\" mentioned in the provided context information.",
        "ae3cd914-f6dc-43d3-9398-ac480f7f75d4": "How does the Region Proposal Network (RPN) in Faster R-CNN contribute to reducing the computational bottleneck in object detection systems?",
        "adf72a86-544a-44d5-ba59-c5d3edea1c87": "What is the advantage of using convolutional feature maps from region-based detectors for generating region proposals in the RPN?",
        "6e73562f-20c7-4286-8974-e9633c42a6f3": "How do Region Proposal Networks (RPNs) differ from prevalent methods in generating region proposals for object detection?",
        "535c11f1-b2a4-4fc6-85d4-f94debf9873b": "Explain the training scheme proposed to unify RPNs with Fast R-CNN object detection networks.",
        "dd005e44-87c9-4fe3-9af0-67a88f7d0173": "What is the advantage of using the RPN (Region Proposal Network) in the Faster R-CNN method for object detection? How does it contribute to improving accuracy?",
        "31257999-ce86-4a7d-9971-4057985fea01": "Compare and contrast the object proposal methods based on grouping super-pixels (e.g., Selective Search) and those based on sliding windows (e.g., EdgeBoxes). Discuss their strengths and weaknesses in the context of object detection.",
        "c4f5ddf2-ad61-4cf7-ad36-e2087c885820": "How does the OverFeat method differ from the MultiBox method in terms of generating region proposals for object detection?",
        "0a183e86-0499-4a66-9511-bc002981e23d": "What is the purpose of the first module in the Faster R-CNN object detection system, and how does it contribute to the overall process?",
        "27c75eb8-3e91-4f30-88b0-0c465f76ba37": "What is the purpose of the Region Proposal Network (RPN) in the Fast R-CNN detector? How does it contribute to the overall object detection system?",
        "59a7b1f2-6eb0-4f42-a490-98127b2ff364": "Explain the concept of translation-invariant anchors in the context of region proposal networks. How does this property guarantee consistent proposal predictions regardless of object translation?",
        "856322f1-6eeb-4a26-836b-df557d780aef": "How does the translation-invariant property of the proposed method guarantee consistent predictions for object proposals even when the object is translated in an image? Compare this with the approach used in the MultiBox method.",
        "1a1ffe43-f73d-4346-8e18-b67a9ba367e4": "Explain the advantage of using multi-scale anchors as regression references in the proposed method. How does this approach address the issue of predicting objects at different scales and aspect ratios?",
        "41bdf854-9c81-4841-a5f7-10e1c34e2cc8": "What are the two main ways of addressing multiple scales in object detection algorithms, as described in the context information?",
        "448faaa6-a69c-4bb6-8261-87f944d229ea": "How does the anchor-based method differ from the traditional methods of addressing multiple scales in object detection?",
        "a6f047c9-55c6-49d2-9130-cb29bb8617b6": "Explain the loss functions used in the Faster R-CNN model for object detection. How is the classification loss different from the regression loss?",
        "8073cd55-23a1-459e-b2b2-e05eca344658": "Describe the parameterizations used for bounding box regression in the Faster R-CNN model. How are the four coordinates of a bounding box represented and what do they signify?",
        "6b7c95cb-60ca-417d-b4e0-ac4b8015d9f3": "Explain the difference between the bounding-box regression approach used in the proposed method and the previous RoI-based methods. How does the proposed method handle varying sizes of bounding boxes?",
        "73b41da3-9982-40f6-94ad-bc4ac1dab7da": "Compare and contrast the \"Alternating Training\" and \"Approximate Joint Training\" techniques for training networks with shared features. What are the advantages and disadvantages of each approach?",
        "21246a5c-77a6-4e7b-b8dc-efe7f6c795cc": "What are the three different solutions discussed in the paper for training the RPN and Fast R-CNN networks?",
        "f4342617-f76b-48fe-ab3a-9dddb3aae3c0": "How does the four-step alternating training algorithm work in learning shared features in the Faster R-CNN model?",
        "b28ce2be-327d-4453-9f88-a82a2bcc76a9": "How does the use of anchor boxes with different scales and aspect ratios contribute to the efficiency of the proposed method? Provide examples from the context information to support your answer.",
        "1497b3f9-18d4-4f66-bb12-fbb6543aefb9": "Explain the purpose and significance of non-maximum suppression (NMS) in reducing redundancy in the proposal regions. How does NMS affect the number of proposals and the overall detection accuracy?",
        "4296cf94-345e-465e-9ba1-3c242d7dd1e9": "How does using the RPN (Region Proposal Network) in the Fast R-CNN framework improve the speed of the detection system compared to using SS (Selective Search) or EB (EdgeBoxes)? Explain the reasons behind this improvement.",
        "4aef2652-c1f8-4af8-a79c-5001cbbbb99c": "In the ablation experiments on RPN, what were the results when the RPN's cls (classification) layer and reg (regression) layer were turned off separately at test-time? How did these results demonstrate the importance of each layer in achieving accurate and high-ranked proposals?",
        "9b5eddf9-28bc-4c5d-96c6-8472169e5ce1": "What is the impact of using more powerful networks, such as VGG-16, on the proposal quality of the Region Proposal Network (RPN)?",
        "97b1d459-a3fe-4f9c-8bbc-564800ed2c77": "How does the performance of the VGG-16 network compare to the Selective Search (SS) baseline in terms of both proposal and detection accuracy?",
        "fb691a43-1cd5-41b3-ab15-efe87ed4f522": "What is the frame-rate achieved by the system with the ZF net? How does it compare to the frame-rate achieved with the VGG-16 net?",
        "8075d975-2185-43d7-aab9-b628b1108316": "How does the mean average precision (mAP) change when using different settings of anchors in the RPN?",
        "c145509d-30ff-40c1-a5ad-1fad73f01445": "What is the purpose of the RPN (Region Proposal Network) in the Faster R-CNN model?",
        "ee902dd9-5c4c-4348-9a2f-e5df82f7cf07": "Which datasets are used for training the Faster R-CNN model, and what is the difference between \"07\" and \"07++12\"?",
        "e53e4dfe-5289-43e1-b5d6-8cbcfabffb47": "How does the use of anchors of multiple sizes contribute to the effectiveness of the regression references in object detection? Provide evidence from the context information to support your answer.",
        "2c38636b-8cc0-41e5-8f2a-111fe346992b": "Explain the significance of the Recall-to-IoU metric in evaluating proposal methods for object detection. How does it differ from measuring the ultimate detection accuracy? Use examples from the context information to illustrate your explanation.",
        "91d0419e-c453-4393-910f-57e4367bd543": "What are the mAP (mean Average Precision) scores for the \"SS 2,000 07\" and \"RPN 300 07+12\" methods in the PASCAL VOC 2007 Test Set with Fast R-CNN Detectors and VGG-16?",
        "6a6a2716-8b81-4f5e-9180-ad4b61019617": "What is the difference in mAP scores between the \"RPN 300 07\" and \"RPN 300 COCO+07+12\" methods in the PASCAL VOC 2007 Test Set with Fast R-CNN Detectors and VGG-16?",
        "554d2f45-27c5-45ec-89ca-87755c4d3d69": "What are the different settings of anchors used in the detection results of Faster R-CNN on PASCAL VOC 2007 test set? How does the mAP (%) vary with these different settings?",
        "cd4ed145-c713-402d-915c-f7e856a423df": "What is the default setting for the number of proposals used in testing for Faster R-CNN on PASCAL VOC 2007 test set? How does the mAP (%) change with different numbers of proposals?",
        "615236f6-b353-4e85-86e7-38ea535b722a": "What is the purpose of the IoU overlap ratio in object detection, as shown in Figure 4?",
        "b76787bc-48d6-4ba0-a407-aa32d5a08178": "How does the Faster R-CNN system, using the VGG-16 model and 07+12 trainval training data, achieve real-time object detection, as described in Figure 5?",
        "cef3bb2f-7723-4a20-96de-6ac9e08eb664": "Explain the difference between the RPN method and the SS, EB, and MCG methods in terms of their behavior when the number of proposals decreases. How does this behavior contribute to the RPN's good ultimate detection mAP?",
        "3c5f3cf3-9c02-4bfa-ba51-3d99d073fffa": "Compare the one-stage detection method proposed in the OverFeat paper with the two-stage proposal + detection method used in Faster R-CNN. What are the main differences between these two approaches? How does the use of adaptive pooling in the second stage of the cascade contribute to more accurate detections?",
        "38bfeef8-49f4-435c-966a-b18b5afadd40": "How does the modification of the learning rates in the RPN and Fast R-CNN steps affect the training process?",
        "8f1b552e-3f4b-4986-8617-9e7b718def3c": "What is the impact of including negative samples in the interval of \u00bd0;0:5\u00de in the Fast R-CNN system on the mAP@0.5 metric for the COCO dataset?",
        "104b2359-451c-42ef-80cd-079f03fb9541": "What is the main difference between the Fast R-CNN and Faster R-CNN systems in terms of their performance on the COCO dataset?",
        "c1b9f9d0-33b0-4c49-9a6c-4261ac012304": "How does the Region Proposal Network (RPN) contribute to improving the localization accuracy in the Faster R-CNN system?",
        "8af400c0-2b18-4e8c-93ac-678fc7d13d9f": "How does the use of a 101-layer residual net (ResNet-101) in the Faster R-CNN system improve the mean average precision (mAP) on the COCO val set compared to using VGG-16?",
        "886f777f-83b4-42bf-a30d-17627aad74f8": "What is the impact of fine-tuning the COCO detection model on the PASCAL VOC dataset? How does it affect the mAP on the PASCAL VOC 2007 test set?",
        "64dab92e-6978-4b7d-af22-e19883f159cd": "How does the learned RPN in the object detection system improve the overall accuracy of object detection?",
        "7f3b88d2-b084-4805-b9c9-dd66159ea757": "What is the significance of using the COCO trainval set for training the VGG-16 model in the context of object detection?",
        "3cfaa3ef-a627-41a8-b586-1f79ac63da4e": "What is the mAP (mean Average Precision) of the Faster R-CNN model on the PASCAL VOC 2007 test set when trained with the VOC07+12 training data?",
        "90d0b3f5-7515-4a82-95c8-9147abd45068": "What is the difference in detection mAP between the Faster R-CNN model trained with COCO (no VOC) and the model trained with COCO+VOC07+12 when evaluated on the PASCAL VOC 2012 test set?",
        "c23cbf8e-18a6-4538-a6a5-da690dfb09ba": "In the field of computer vision, what is the purpose of the Microsoft COCO dataset mentioned in reference [12]? How does it contribute to object recognition and understanding in images?",
        "10d73967-4101-4e6e-9b31-00d124ae3fbd": "Reference [18] introduces a deep learning technique called deep residual learning for image recognition. Explain the concept of deep residual learning and discuss its advantages over traditional deep neural networks in terms of image recognition accuracy.",
        "4cb83885-0c84-41b6-a3c9-1e83afc8b62e": "In the field of computer vision, what are some of the key advancements made in object detection using deep neural networks? Provide specific references from the given context information to support your answer.",
        "84394d90-965f-47ad-8757-fcb3041dd731": "Explain the concept of attention-based models and their application in speech recognition. How do they differ from traditional models? Cite relevant research papers from the provided context information to support your explanation.",
        "e136b2d5-912d-496e-809e-4d7f9080db0d": "What are the key contributions of Ross Girshick in the field of computer vision and object detection? Provide specific examples of his achievements and collaborations.",
        "90408559-9f2b-4c81-a4d6-9ad455389f37": "Explain the concept of Region-based Convolution Networks (R-CNN) and its significance in improving object detection methods. How has this framework impacted the accuracy of previous state-of-the-art object detection techniques?",
        "e1498ea3-1847-443d-bcfa-3f92cdaee0f2": "How does the domain shift affect object detection in practice? Provide examples from the document to support your answer.",
        "dafafca3-25e4-4023-9f01-ad1c95e33ab0": "Explain the approach proposed in the paper to improve the cross-domain robustness of object detection. How do the two domain adaptation components work together to reduce the domain discrepancy?",
        "e43688ce-d2d8-4b0f-adbe-06afcee89b02": "How does the Domain Adaptive Faster R-CNN model address the domain shift problem in object detection?",
        "bc8a59c9-5ba9-472c-aed5-8d871cc5d117": "What are some conventional methods and recent works in domain adaptation for image classification in computer vision?",
        "93fd5596-d893-4614-8089-5d31a3a0a45d": "How does domain adaptation differ in the context of object detection compared to image classification in computer vision? Provide examples of conventional methods used for domain adaptation in object detection.",
        "3291ca83-c7b4-4468-9800-c0420a552f98": "What are some recent advancements in domain adaptation for deep neural networks in the field of object detection? How do these advancements address the challenges of predicting both object location and category in object detection tasks?",
        "ededd164-aac5-4151-82c4-f5f559dc4a97": "Explain the architecture of the object detection model described in the document. What are the main components and how do they work together?",
        "c52b2c70-df93-419e-9576-06e3f541f85a": "How does the H-divergence measure the distance between two domains in the context of domain adaptation? Explain the formula and its implications for distinguishing between domains.",
        "a7ca805c-68af-48fe-a93d-1cc29e13819d": "Explain the concept of image-level adaptation in the context of object detection. How does it address the domain shift problem?",
        "69e29397-690c-4ce1-b7eb-6f2f7ef11c96": "Describe the decomposition of the joint distribution in object detection as mentioned in the document. How does instance-level adaptation play a role in handling the domain shift problem?",
        "836a55eb-5056-4dde-a559-f4bd63241e1e": "What is the purpose of performing domain distribution alignment on both the image and instance levels in the Domain Adaptive Faster R-CNN model?",
        "8fb15e49-9be1-4d03-8b7e-995dd2dbe398": "How does the consistency regularization help alleviate the bias in estimating the domain-dependent bounding box predictor in the context of object detection?",
        "9fbe61d6-57a1-4ee6-b188-5e7a40192541": "How does the domain adaptation component for image-level adaptation in the Faster R-CNN model help eliminate the domain distribution mismatch on the image level?",
        "e5a05106-1949-4ca8-b166-fdec55371bda": "Why is it necessary to enforce consistency between two domain classifiers in order to learn the bounding box predictor P(B|D,I) in practice?",
        "ae6e8c34-d80c-400d-943c-452fa2112012": "How does the patch-based design in the image-level adaptation of the Domain Adaptive Faster R-CNN model help to reduce the shift caused by global image differences? Provide two benefits of using this design.",
        "3470d8d1-7c21-4cdb-903e-5afba4b8757a": "Explain the purpose and function of the consistency regularizer in the Domain Adaptive Faster R-CNN model. How does it contribute to learning cross-domain robustness of the bounding box predictor?",
        "ba0e6068-1742-4108-8cec-235943d90192": "What are the three novel components introduced in the Domain Adaptive Faster R-CNN model?",
        "556529e6-a984-43b8-ab5a-7532962ac7da": "How can the proposed network be trained in an end-to-end manner?",
        "88e37aa4-4260-4217-a09f-b39f322b02dc": "How does the proposed Domain Adaptive Faster R-CNN model improve object detection in cross-domain scenarios? Provide specific details about the components used in the model and their impact on the results.",
        "74d24df4-a5e1-47a8-8ac2-04b28b5fb17c": "In the learning from synthetic data scenario, what datasets are used as the source and target domains? Explain the performance gap between models trained on synthetic data and real data, and how the proposed method addresses this issue.",
        "0aee4500-2d05-4610-a097-9bfe3e26bbe6": "How does the proposed image-level adaptation component contribute to reducing domain shift in the driving scenario dataset? Provide specific performance gains achieved compared to the Faster R-CNN model.",
        "866e5c3b-4074-49e7-9718-ffb4e02393f9": "In the context of autonomous driving systems, why is it important for a detection system to perform faithfully in different weather conditions? Explain how the experiment on adapting a model from normal to foggy weather in the Foggy Cityscapes dataset helps evaluate this ability.",
        "baeb7719-f9f3-4b2e-b9d3-c568af460d75": "How does the proposed Adaptive Faster R-CNN model improve the baseline Faster R-CNN model in terms of performance on the Foggy Cityscapes dataset? Provide specific quantitative results from Table 2 to support your answer.",
        "231b7413-36ff-4f0e-ba70-862cca0471d9": "Explain the significance of studying adaptation between two real datasets, KITTI and Cityscapes, as mentioned in the document. How does the proposed method, Adaptive Faster R-CNN, perform in terms of adaptation between these two datasets? Provide specific quantitative results from Table 3 to support your answer.",
        "24e0daf4-3a65-4e35-bb0d-020d4ecf074b": "How does the proposed Adaptive Faster R-CNN model compare to other baselines in terms of performance improvement? Provide specific details from the context information to support your answer.",
        "7210af41-8fcd-4caa-9fe1-e11721c35e30": "Explain the three error types (correct, mis-localized, and background) used in the error analysis of the top ranked detections. How do image-level and instance-level adaptation contribute to reducing these errors? Use examples from the context information to illustrate your explanation.",
        "89d9b1d3-d3f1-4ad2-9149-5f110574bbe0": "How does image scale affect the performance of the Faster R-CNN model in cross-domain object detection? Explain the impact of image-level and instance-level alignment on the model's robustness to scale change.",
        "cc098fea-84c0-463a-88d8-9f5ffbd5417f": "Discuss the role of consistency regularization in improving the performance of the Region Proposal Network (RPN) in the Domain Adaptive Faster R-CNN model. How does the consistency regularizer encourage the RPN to be more robust?",
        "e3c8c1b0-9cad-4554-8c92-e8fd22cf813a": "How does the Domain Adaptive Faster R-CNN model improve upon the vanilla Faster R-CNN model in terms of mIoU? What components are introduced in the Domain Adaptive Faster R-CNN model to address the performance drop caused by domain shift?",
        "4326aadb-da7a-4cf0-a415-ee4faf8ea657": "What is the role of the consistency regularizer in the Domain Adaptive Faster R-CNN model? How does it contribute to improving the performance of the Region Proposal Network (RPN)?",
        "721ba703-8977-4279-aa92-deafd98247f1": "In the context of image segmentation, what is the significance of scale-awareness? Provide examples from the references to support your answer.",
        "e26b49e3-51b8-4cb4-8613-a6aa00327c3f": "How does unsupervised domain adaptation contribute to the field of computer vision? Discuss the approaches mentioned in the references and their implications for real-world applications.",
        "c4abc08c-da4f-498a-9a50-72f17082385a": "What is the main focus of the paper \"Fine-grained recognition in the wild: A multi-task domain adaptation approach\"?",
        "80167ffd-d2a8-4e9f-b6df-3b288c8a5133": "How does the KITTI dataset contribute to the field of vision and robotics?",
        "bff4dbe2-ef57-4ce8-b187-1f269b7a5882": "In the context of deep learning, what is the significance of the paper \"Imagenet classification with deep convolutional neural networks\" by Alex Krizhevsky et al. in NIPS 2012?",
        "224fce3e-0fe8-40ee-a444-e46cce478cb8": "How does the paper \"Unsupervised image-to-image translation networks\" by Ming-Yu Liu et al. in NIPS 2017 contribute to the field of computer vision?",
        "ed5e8163-a894-46ef-8431-6525e7bd34dc": "In the context of object detection, what is the significance of the Faster R-CNN algorithm? How does it contribute to real-time object detection?",
        "d001f16f-f2f9-469e-8b49-d48f99fdd427": "How does domain adaptation play a role in deep learning models? Explain the concept of transferring representations for unsupervised domain adaptation.",
        "b94c9cfb-fcd3-4c27-9154-14734c74177c": "What are the advantages of the Transformer network architecture compared to recurrent or convolutional neural networks?",
        "592eb0ae-dbc0-4c11-940e-860a06b23f57": "How does the Transformer model achieve superior quality in machine translation tasks while being more parallelizable and requiring less training time compared to existing models?",
        "5f915c74-6299-41f7-945b-953a496111f6": "How does the Transformer model architecture differ from recurrent models in terms of computation and parallelization?",
        "150c7599-a4fb-46db-8189-fa5580b74497": "What is the advantage of using self-attention in the Transformer model compared to other models like Extended Neural GPU and ConvS2S?",
        "ee1000c5-ac1f-4632-a7b8-921ae85d2451": "What is the purpose of the residual connection in the Transformer model architecture? How does it contribute to the overall functioning of the model?",
        "20f73c85-61c9-4b15-9ee3-b4aeb63560d2": "Explain the concept of attention in the Transformer model. What is the Scaled Dot-Product Attention and how does it compute the weighted sum of values?",
        "6d66be93-5ab6-40d2-a631-52a129c70b1b": "What are the two most commonly used attention functions in the context of scaled dot-product attention and multi-head attention? How do they differ in terms of their theoretical complexity and practical efficiency?",
        "ea4ff6e7-5d6b-4b4b-bb2f-72c08a2fe154": "How does multi-head attention differ from a single attention head in terms of its ability to attend to information from different representation subspaces at different positions? How does this feature benefit the model's performance?",
        "6404b684-3ed1-4582-9726-10e0c8e2a3f0": "How does the Transformer model use multi-head attention in the encoder-decoder attention layers? Explain the role of queries, memory keys, and memory values in this process.",
        "4ebab669-f2fb-4d21-8526-49dd3ad5b1c0": "What is the purpose of positional encodings in the Transformer model? How do they enable the model to make use of the order of the sequence?",
        "eec07bc8-a2fe-40ee-b8f8-ddb20ced60dd": "What are the three desiderata considered when comparing self-attention layers to recurrent and convolutional layers?",
        "f3baf48a-5c5c-42ff-8d68-2cd833f676a1": "How does the computational complexity of self-attention layers compare to recurrent layers when the sequence length is smaller than the representation dimensionality?",
        "da166ca7-3f85-4e08-9515-a1724cd7b836": "How does the use of self-attention in convolutional layers contribute to the complexity of the network compared to recurrent layers?",
        "71b09207-530d-46b6-bc9e-faa23fb9290f": "What types of regularization techniques are employed during training in the model described in the context information?",
        "fc2ba4ae-1fce-4f08-8b12-9148ff46ac37": "What is the significance of label smoothing in the Transformer model's training process? How does it affect the model's perplexity, accuracy, and BLEU score?",
        "94ccee83-299b-4751-86a9-25f09dc5c29c": "Compare the performance of the big Transformer model with the previous state-of-the-art models on the WMT 2014 English-to-German and English-to-French translation tasks. How does the big model outperform the previous models in terms of BLEU score and training cost?",
        "bb3937a9-c457-43b6-a927-7be113378ccf": "How does reducing the attention key size affect the quality of the model in the Transformer architecture?",
        "336aa023-503d-4d52-8591-2593363f0b15": "What is the impact of using learned positional embeddings instead of sinusoidal positional encoding in the Transformer model?",
        "6b748eef-522c-411c-b03b-702ec72d8eb7": "What is the significance of layer normalization in deep learning models? Provide a brief explanation based on the reference [1].",
        "7f77ad0f-a828-4dd3-8d65-b36610f5fe0e": "Compare and contrast the neural machine translation architectures discussed in references [2], [5], and [15]. What are the key differences and similarities between these approaches?",
        "bd88861b-37d7-4920-8d02-71e81ac078dc": "In the context of neural machine translation, what are some effective approaches to incorporating attention mechanisms? Provide examples from the given context information.",
        "a118884e-0844-49c6-b074-2cf3fba8db24": "How does the use of subword units contribute to improving neural machine translation of rare words? Explain with reference to the relevant paper from the provided context information.",
        "11aac493-93c4-413f-8f40-32587e77cc9d": "What were the top-1 and top-5 error rates achieved by the large, deep convolutional neural network in classifying the high-resolution images in the ImageNet LSVRC-2010 contest?",
        "6b51f5a0-4ac9-4917-a906-a74c2b315cf0": "How did the use of a recently developed regularization method called \"dropout\" help in reducing overfitting in the fully connected layers of the neural network?",
        "c4df6e78-bc37-4712-a206-5b20a61669be": "What were the challenges faced by backpropagation in the 1980s and why did it not meet the high expectations of its advocates?",
        "e42f5e53-b429-40c6-b670-e9b771e33fb9": "How did the limitations of small image datasets impact the performance of object recognition tasks, and what recent developments have made it possible to overcome these limitations?",
        "5f6f15d9-f13d-475b-99a7-c09c0989af82": "How do convolutional neural networks (CNNs) differ from standard feedforward neural networks in terms of their architecture and training efficiency? Provide examples from the given context to support your answer.",
        "c101a532-847c-4e80-8f5f-a45e860995d9": "Explain the significance of the ImageNet dataset and its role in training large-scale convolutional neural networks (CNNs). Discuss the challenges faced in applying CNNs to high-resolution images and the techniques used to prevent overfitting.",
        "6f2a6aaf-4b02-4ceb-a989-5963c2940b78": "What is the purpose of the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) and how does it contribute to the field of computer vision?",
        "63d39e61-645b-43cb-8d1e-d70a3dfec031": "Explain the significance of using Rectified Linear Units (ReLUs) as the nonlinearity in the neural network architecture described in the document. How does it compare to traditional saturating neuron models?",
        "cc1007b9-4dad-4c74-91f6-7907ffed1444": "How does the use of ReLUs in training large models on large datasets affect the learning process and performance?",
        "9f01096d-b723-416f-b138-bbae8fddd3f2": "What is the purpose and effect of local response normalization in convolutional neural networks?",
        "833713f0-200a-4371-a643-b5bae5af33d2": "How does response normalization affect the error rates in CNNs? Provide specific numbers to support your answer.",
        "26537f5d-b641-47bc-a977-3f42a6fcd236": "What is the difference between traditional local pooling and overlapping pooling in CNNs? How does the choice of pooling strategy impact the performance of the network?",
        "00421cc6-11e7-4798-9aaf-866a2e467f29": "How does the use of data augmentation help in reducing overfitting in neural networks? Provide examples of the specific data augmentation techniques mentioned in the document.",
        "80c18fb1-1af7-414e-b4b0-8d86c4d293ef": "Describe the overall architecture of the convolutional neural network (CNN) discussed in the document. Include details about the number of layers, types of layers, and the purpose of each layer in the network.",
        "d7fa089f-18df-44c1-81a8-ae891404abd6": "What is the purpose of response-normalization layers in a convolutional neural network? How are they applied in the architecture described in the context information?",
        "26cb7140-5dfd-4edb-8c58-d859cfdfb973": "Describe the connectivity pattern between the kernels in the third convolutional layer and the kernel maps in the second layer. How does this connectivity differ from the connectivity between the fully-connected layers?",
        "527bb7ef-2e96-46d9-ae3e-daa2e504a80d": "How does the second form of data augmentation alter the intensities of the RGB channels in training images? How does this scheme reduce the top-1 error rate?",
        "60d835f8-cf1c-4bea-8e7f-91c23276c7e8": "What is dropout and how does it contribute to reducing test errors in big neural networks? How does dropout affect the convergence of the network during training?",
        "c35e19bb-d5e6-400c-97ab-6d390b33338d": "How did the initialization of neuron biases in different layers of the network contribute to the learning process in the convolutional neural network?",
        "5474dffc-211b-4a25-a822-43d6d60c3bf2": "Compare the test set error rates achieved by the CNN model in the ILSVRC-2010 and ILSVRC-2012 competitions with the best results achieved by other approaches.",
        "64aa3eaa-b06e-47fc-92c5-8fe45c4be1a5": "How does the network assess its visual knowledge by considering the feature activations induced by an image at the last hidden layer? Provide an example from the document to support your answer.",
        "daad9b85-6a4f-48d0-9fd0-b717ab6df408": "Compare the error rates achieved by the CNN described in the paper with other approaches in classifying the ImageNet dataset. Provide specific numbers and discuss the significance of these results.",
        "bb1a9f3b-5057-4486-a2df-a1ce094b5ba9": "How does the restricted connectivity described in Section 4.5 affect the specialization exhibited by the two GPUs?",
        "6365c693-4f5b-4b58-9ed5-fed5bde5147f": "What is the significance of the top 48 kernels being learned on GPU 1 and the bottom 48 kernels being learned on GPU 2, as mentioned in Section 7.1?",
        "a730eb4a-d5e4-46cd-a4d8-ac50a64b44d0": "How did the use of deep neural networks revolutionize the field of computer vision, and what role did FeiFei et al. play in this revolution?",
        "0d4f9c80-2d6b-41f5-97fb-1df8c3aa10f9": "Explain the significance of the depth of a large, deep CNN in achieving record-breaking results on a challenging dataset. How does the removal of a single convolutional layer affect the network's performance?",
        "5bd81556-efbd-48fc-9459-d6e2f72e876a": "What is the significance of the ImageNet database in the field of computer vision and object recognition? Provide a brief overview of its purpose and impact.",
        "f508c582-43aa-4e85-9d7e-8d3897ba80c2": "Compare and contrast the Neocognitron model proposed by Fukushima in 1980 with the deep residual learning approach introduced by He et al. in 2015. Highlight the key differences and advantages of each model in the context of pattern recognition.",
        "4579a9e6-064d-4fd2-94f0-031d22b6f468": "In the context of image classification, what is the purpose of metric learning and how does it contribute to large-scale image classification?",
        "0c933105-750a-49ff-b635-2c8623296be7": "How do convolutional networks learn to generate affinity graphs for image segmentation, and what are the potential applications of this technique?",
        "d4050685-0887-4efb-a28a-426bc7fc4d80": "How does Fast R-CNN improve upon previous methods for object detection, such as R-CNN and SPPnet?",
        "aa6ea049-dab9-42a2-a59b-a0c5f29d1747": "What are the drawbacks of the R-CNN method and how does Fast R-CNN address these drawbacks?",
        "0b6b57e0-edbb-431d-97ff-d3d3e76ad21a": "How does the SPPnet method accelerate the R-CNN object detection process?",
        "4074b2b6-f32f-4a70-9e9d-ce42124b4db2": "What are the challenges associated with using deep networks like VGG16 for object detection, and how does it impact the overall performance?",
        "0f63060c-461b-419c-b84a-13b8703d9ee4": "What are the limitations of the SPPnet algorithm and how does Fast R-CNN address them?",
        "03333244-f1aa-4c8b-9dc7-64bd4b16976f": "Explain the RoI pooling layer in the Fast R-CNN architecture and how it converts features within a region of interest into a fixed-size feature map.",
        "2ff87597-a4c2-4ef9-bea9-79abfb7df808": "How does the RoI max pooling layer work in the Fast R-CNN architecture? Explain the process and its purpose in object detection.",
        "6b77e508-7a34-4f4b-a4f3-1c8abdb354e6": "Why is back-propagation through the spatial pyramid pooling layer inefficient in training networks like R-CNN and SPPnet? Discuss the limitations and challenges faced in updating weights below this layer.",
        "791c517e-81a4-44eb-a3a1-f142a90f5a55": "How does Fast R-CNN training take advantage of feature sharing during training?",
        "1373d884-b18d-466f-a966-9de01bdaaf9a": "What is the purpose of the multi-task loss function in Fast R-CNN and how is it calculated?",
        "bf38c8f6-97bf-457d-a910-dc3a65c300b5": "What is the purpose of using the smooth L1 loss function in bounding-box regression? How does it differ from the L2 loss used in R-CNN and SPPnet?",
        "f139797d-3499-4622-af90-68ab9114e33f": "Explain the mini-batch sampling strategy used during fine-tuning. How are foreground and background examples labeled? What is the significance of the lower threshold of 0.1 in selecting background examples?",
        "bfe55540-8d3b-4dc6-a6f3-67e1d59d1018": "Explain the concept of RoI pooling layer in Fast R-CNN and its role in the back-propagation process.",
        "d87f6f1e-8560-4f45-89c5-d3cb05725252": "Compare and contrast the two approaches for achieving scale invariance in object detection: \"brute force\" learning and using image pyramids.",
        "e4d63c6c-be52-47af-96f3-41b58edb5aa3": "How does the technique of truncated SVD contribute to faster detection in the context of object detection algorithms?",
        "5186638c-1957-4a0b-b2ee-8f9308666001": "What are the three main results presented in the paper that support its contributions?",
        "9d8eb893-37a2-421e-aa98-551dc9f0ee9c": "In the context of object detection, compare the average precision scores of the SPPnet, R-CNN, and FRCN methods for the \"train\" category. How do these scores differ across the different training sets?",
        "dd6319ce-88fe-4ce9-88fb-2946aa82036f": "Explain the significance of the \"07\\diff\" and \"07+12\" training sets in the context of VOC 2007 test detection. How do the average precision scores of the FRCN method vary when trained on these different sets?",
        "7d93be58-ab51-4b3a-b25a-78a09787cde2": "Compare the average precision of the BabyLearning method and the R-CNN BB method for the VOC 2007 test detection. Which method performs better overall and in which specific object categories?",
        "c4d80a00-0f67-456f-9cbf-8f6cf20c51fd": "What is the training set key for the FRCN [ours] method in the VOC 2010 test detection? How does the average precision of this method compare to the other methods in the table?",
        "24b4c47f-e30a-4912-8173-88325e7c541b": "Compare the performance of Fast R-CNN and other methods on the VOC 2012 test dataset. How does Fast R-CNN outperform the other methods in terms of average precision?",
        "0ebb882b-40b6-48bf-81c6-35932f151e8f": "Describe the differences between the three network models used in the experiments (model S, model M, and model L). How does the depth and width of these models affect their performance in object detection tasks?",
        "da6d049e-c155-4d30-9e4c-4bc2b60fcaf2": "What is the main advantage of Fast R-CNN over other methods based on the \"slow\" R-CNN pipeline?",
        "e44c3f33-1484-47ab-bc4f-ab169400ca3f": "How does SegDeepM achieve a higher mAP than Fast R-CNN on VOC10, and how can Fast R-CNN be further improved in this scenario?",
        "a77ee52b-3044-456f-98b7-6bcafc3ccc2d": "What are the advantages of using Fast R-CNN compared to R-CNN and SPPnet in terms of training and testing time? Provide specific numbers from the table to support your answer.",
        "99bf3524-18d9-4ebe-8960-0de946cced40": "Why is fine-tuning the convolutional layers important for VGG16 in Fast R-CNN? How does freezing the convolutional layers and only training the fully connected layers affect the model's performance?",
        "be3db9a1-9e55-4b9e-943f-ffea1df276e6": "According to the context information, what is the effect of fine-tuning only the fully connected layers in VGG16 on the mAP (mean average precision) in Fast R-CNN?",
        "71b4b5ea-3c06-4777-a017-7ce161d9655b": "Why is it not necessary to update all the conv layers in VGG16 during fine-tuning for object detection in Fast R-CNN?",
        "dede9cea-7c85-43ef-89fe-0285ed028c2a": "How does multi-task training improve the performance of object detection models? Provide specific evidence from the context information to support your answer.",
        "60dc53ff-2421-4b3f-8e4d-5528419e6eab": "Compare and contrast the strategies of brute-force learning (single scale) and image pyramids (multi-scale) for achieving scale-invariant object detection. Use examples from the context information to illustrate your points.",
        "3a0557e6-8be8-4a44-9219-96ef07ba8bb6": "According to the context information, what is the tradeoff between speed and accuracy when using multi-scale versus single-scale processing in object detection?",
        "167a2798-67ba-4751-b0a8-a5d55a1d6707": "How does enlarging the training set improve the mean average precision (mAP) in Fast R-CNN?",
        "7cb70657-8312-4889-99cb-cf0642e6b0e1": "How does the use of post-hoc SVM training with hard negative mining in Fast R-CNN compare to the use of softmax in terms of performance? Provide specific data from Table 8 to support your answer.",
        "cfdb9076-7777-4477-a97e-e48f3062ac08": "What evidence does the document provide regarding the impact of increasing the number of object proposals per image on Fast R-CNN accuracy? Explain the findings from the experiment and how they relate to the state-of-the-art measurement of object proposal quality.",
        "b4be092a-1a6f-467c-bccb-3ac7076588e4": "How does the addition of densely generated boxes affect the mean average precision (mAP) in Fast R-CNN? Provide specific details and results from the study.",
        "5cefa67c-7be5-4188-b61c-6ef6ce63750d": "What are the preliminary results of applying Fast R-CNN to the MS COCO dataset? Include information about the training process, evaluation metrics, and the achieved mAP scores.",
        "51f936bf-b7eb-418a-b07d-0aff454af1ea": "In the context of deep learning, what is the significance of second-order pooling and how does it contribute to semantic segmentation? (Refer to reference [1])",
        "52937241-4951-4924-8b5f-c904ddcc858d": "How does the concept of multitask learning improve machine learning models? Provide an example from the document. (Refer to reference [2])",
        "e5ed741b-eb0d-427a-8819-c9f30e81fade": "In the context of image recognition, what is the significance of the paper \"ImageNet classification with deep convolutional neural networks\" by A. Krizhevsky, I. Sutskever, and G. Hinton?",
        "c3c964bc-7af2-4102-87a6-09e103721c3f": "How does the paper \"Selective search for object recognition\" by J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders contribute to the field of object recognition?",
        "87fe4d33-5b9e-434c-a93a-4798c14b7386": "How does YOLO differ from traditional object detection methods, such as deformable parts models (DPM) and R-CNN? Provide specific details about the approach and its advantages.",
        "b4f42ff1-8bba-4210-98fa-1b9ea878c756": "Explain the concept of YOLO's unified architecture and its benefits in terms of speed and accuracy. Provide examples of applications where YOLO can be useful.",
        "0183a14d-4e57-40f7-ac09-958833f04766": "How does YOLO achieve real-time object detection with low latency compared to traditional methods?",
        "7c239a0d-324b-4f0f-bfcf-020ae218f6ae": "What are the benefits of using YOLO for object detection, particularly in terms of speed and accuracy?",
        "d98a39d8-1599-4831-af42-76c6e9501205": "How does YOLO differ from sliding window and region proposal-based techniques in terms of encoding contextual information about classes and their appearance?",
        "48908f72-951e-4be2-880e-7b464be0b9ff": "What are the tradeoffs of YOLO in terms of accuracy and precise localization of objects, especially small ones?",
        "71bd8594-4e00-4d59-afd3-625204b50206": "Explain the concept of detection as a regression problem in the context of the YOLO model. How does the YOLO model divide the image and make predictions for bounding boxes, confidence, and class probabilities?",
        "bd4c8bf0-bec9-494b-8f9e-e3f363184c2a": "Compare and contrast the network architectures of the YOLO model and the GoogLeNet model. How does the YOLO model differ in terms of the number of convolutional layers and the use of inception modules?",
        "78841751-511e-41c4-bb27-4fa4e63e5d25": "What is the purpose of pretraining the convolutional layers on the ImageNet dataset in the given context?",
        "d9bcb5a5-d0f1-404f-aa2a-00bf6fcb34fa": "How does the use of sum-squared error as the optimization function in the model affect the training process and the goal of maximizing average precision?",
        "97edaf4f-08b5-4b27-a87d-5b4a15328c0f": "How does YOLO address the issue of model instability during training?",
        "d307226e-f7ae-4ac4-a792-6fc5430cd8fe": "Why does YOLO predict the square root of the bounding box width and height instead of the width and height directly?",
        "3a7960b6-bfb3-408a-9732-6b79c85456d8": "Explain the components of the loss function used in YOLO and how they contribute to the overall objective of the model.",
        "390a5c57-d691-45f8-8280-56c33760a9be": "Discuss the limitations of YOLO in terms of spatial constraints and generalization to objects with different aspect ratios or configurations.",
        "9f188e2e-7465-477d-8fbd-cb0cb31fe670": "How does the spatial constraint in the model limit its ability to predict nearby objects? How does this affect the detection of small objects in groups?",
        "61990196-4838-4d50-bf1a-7054003195cd": "Compare the YOLO detection system to the Deformable parts models and R-CNN. What are the key differences in their approaches to object detection?",
        "c83d196a-cc9f-4945-9839-501ce7b49dd5": "How does YOLO differ from other fast detection systems, such as R-CNN and Fast R-CNN? Provide specific details about the differences in their approaches and performance.",
        "7751fa17-f1bd-441f-a940-24fc7b5ccca7": "Compare and contrast YOLO with the MultiGrasp system for regression to grasps. What are the main similarities and differences between these two systems in terms of their objectives and capabilities?",
        "2ad54142-6259-4836-a99c-c0008ce27cf7": "What is the main difference between YOLO and Fast R-CNN in terms of their error profiles and performance on VOC 2007?",
        "99ff9c2c-0f90-482d-9779-7bbfdca0239e": "How does YOLO compare to other real-time object detection systems in terms of accuracy and speed, and what are the tradeoffs available in object detection systems?",
        "22272e91-d3b5-476d-ae44-d3fde41348ec": "Compare the performance and speed of Fast YOLO and YOLO on the PASCAL VOC 2007 dataset. How much more accurate is YOLO compared to the fast version, and how does it compare to other real-time detectors?",
        "053a09e4-bdc9-471e-94a7-6c9e860096e7": "Analyze the error breakdown of YOLO and Fast R-CNN on the VOC 2007 dataset. Which type of error is more prevalent in YOLO's predictions, and how does it compare to Fast R-CNN in terms of localization and background errors?",
        "817d95d2-4ee5-420a-9a8c-b896025ee814": "What are the two main types of errors that YOLO and Fast R-CNN encounter in object detection? How do these errors differ between the two models?",
        "fb773fe8-61bb-4330-80b8-f678a9dab1fe": "How does combining YOLO and Fast R-CNN improve the performance of object detection? Provide specific details about the impact of this combination on the mAP (mean Average Precision) metric.",
        "16125b5a-2423-4c28-bd57-56f173e31365": "Compare the performance of the MR CNN MORE DATA model and the HyperNet VGG model in terms of mean Average Precision (mAP) for different object categories. Which model performs better overall and which specific object categories show significant differences in performance between the two models?",
        "20cfc0a6-5970-475a-ae62-8fc77f9437c5": "Discuss the performance of the Fast R-CNN + YOLO model compared to the Faster R-CNN [28] model. Which model achieves a higher mAP score and which specific object categories show notable differences in performance between the two models?",
        "84af0462-0c15-4b43-8e59-e57fd058166d": "Compare the performance of the \"UMICH FGS STRUCT\" model with the \"NUS NIN C2000\" model in terms of their average scores across all categories. Which model performed better and by how much?",
        "88e01ebb-1946-4d4b-b1c8-7ba7d986a128": "Identify the model that showed the highest improvement in performance compared to the \"R-CNN\" model. What was the percentage increase in their average scores?",
        "c2fdb392-1284-4b7b-b219-40390805d967": "In the PASCAL VOC 2012 Leaderboard, which detection method had the highest mean average precision and per-class average precision? How much did YOLO improve the performance of the Fast R-CNN method?",
        "b48176a6-7dcc-4a80-a5f4-e0abb63b85b2": "On the VOC 2012 test set, YOLO achieved a mean average precision of 57.9%. How did YOLO perform compared to the original R-CNN using VGG-16? Which categories did YOLO struggle with and which categories did it perform well on?",
        "a26a9ccd-29b5-42b2-875d-b1a286ab0d2a": "How does YOLO compare to other detection systems in terms of performance on the Picasso Dataset and the People-Art Dataset?",
        "dfb65897-3eb1-4fa9-8438-26fe6222b76e": "Why does YOLO have good performance on VOC 2007 and degrade less than other methods when applied to artwork?",
        "3965eac5-366e-45ae-b7f9-843cd6cc8d8f": "Compare the performance of YOLO, R-CNN, DPM, and Poselets in terms of precision, recall, and F1 score on the Picasso Dataset. Which algorithm achieves the highest F1 score and why?",
        "bac03fbd-21e1-48d2-8a56-6d206b004c7a": "Explain the advantages of YOLO as a unified model for object detection compared to classifier-based approaches. How does YOLO differ in terms of training and detection performance?",
        "305e5f32-d2c8-4641-a872-eae226ce4dce": "In the context of object detection, what is the significance of structured output regression? How does it differ from other approaches mentioned in the document?",
        "d7345aaf-24d1-4ed2-9919-ceba06e38413": "Explain the concept of poselets and their role in training body part detectors. How do poselets utilize 3D human pose annotations?",
        "ec674120-98eb-42d7-a53a-de67b4ee5840": "In the context of computer vision, what is the significance of the paper titled \"Detecting people in cubist art\" by S. Ginosar, D. Haas, T. Brown, and J. Malik? Provide a brief explanation of the key findings and contributions of the paper.",
        "391a4990-d591-4fb8-925c-7581bd766ce3": "Compare and contrast the Fast R-CNN approach proposed by R. B. Girshick in his paper \"Fast R-CNN\" with the Region-based segmentation and object detection approach discussed in the paper by S. Gould, T. Gao, and D. Koller. Highlight the similarities and differences in their methodologies and performance.",
        "dde7bc1d-078d-423b-b24c-9d0a362b6dec": "What is the main contribution of the paper by Ren et al. (2015) titled \"Faster r-cnn: Towards real-time object detection with region proposal networks\"?",
        "c43e9d3b-cd3f-4ef6-9c28-58fcb0a059e3": "How does the Darknet framework, developed by Redmon (2013-2016), contribute to the field of neural networks and object detection?",
        "26317eaf-3e78-4cad-8982-d8db906ad057": "In the context of object detection, what is the significance of dropout in pool5 feature maps according to the research paper by Z. Shen and X. Xue?",
        "dea87981-bc80-46ac-b62e-de8655e332c8": "Which paper introduced the concept of \"Selective search\" for object recognition and what are its key features?",
        "0357aae3-7164-4757-a4e8-c7eb4a3aa593": "How does the U-Net architecture differ from traditional convolutional networks in terms of its use of data augmentation and training strategy?",
        "e81bc760-f57f-4c25-b56f-7009c78ebba7": "What were the key findings and achievements of the U-Net network in the field of biomedical image segmentation, as mentioned in the document?",
        "5e55218a-dc30-4af3-ba95-38d484c2402b": "Explain the drawbacks of the strategy used in Ciresan et al.'s approach for image segmentation, as mentioned in the context information. How does the proposed approach in this paper address these drawbacks?",
        "ccf10143-449f-4439-ae1e-0513606dfeca": "Describe the main idea behind the \"fully convolutional network\" architecture mentioned in the context information. How does it differ from the traditional contracting network approach? How does it contribute to more precise segmentations?",
        "b5c4ed7d-d2c7-4d66-a084-0f7b07175fb6": "How does the overlap-tile strategy contribute to the seamless segmentation of arbitrary large images in the context of neuronal structure segmentation in EM stacks?",
        "91b12e78-49f4-445d-a885-e96217be022b": "What is the significance of using excessive data augmentation, specifically elastic deformations, in training the network for biomedical segmentation tasks?",
        "92cbc0b1-8691-4798-93ca-ea11bb5ee29e": "Explain the architecture of the network used for cell segmentation in the ISBI cell tracking challenge 2015. Include details about the contracting path, expansive path, and the number of convolutional layers.",
        "034d1aea-be41-40f0-8b08-3482d59a5368": "How is the input tile size selected in order to ensure a seamless tiling of the output segmentation map? Why is it important for the x- and y-size of the layer to be even when applying 2x2 max-pooling operations?",
        "e5dfd173-172a-4993-9593-3c3608dc20b8": "How is the weight map computed in the segmentation process? Explain the role of each component in the weight map formula.",
        "c51579ab-b46c-43df-8e1e-23613e34d329": "Why is data augmentation important in training deep networks? Discuss its significance in teaching the network desired invariance and robustness properties.",
        "63bfc778-548c-44e1-bd0d-41bd29194d38": "In the context of image segmentation, explain the concept of random elastic deformations and their importance in training a segmentation network with limited annotated images. How are these deformations generated and what role do they play in achieving shift and rotation invariance?",
        "6aa7445c-0584-4149-a953-9ff75a4627a9": "Compare the performance of the u-net model with the sliding-window convolutional network approach by Ciresan et al. in the task of neuronal structure segmentation. What evaluation metrics were used to assess the performance of these models? How does the u-net model outperform the sliding-window convolutional network in terms of warping error and rand error?",
        "753bc970-94bd-4ece-aac5-c28054343f48": "In the ISBI cell tracking challenge, what are the two data sets used for cell segmentation? Describe the characteristics of each data set and compare the performance of the u-net algorithm with the second best algorithm for each data set.",
        "ed5ff73e-6af1-4400-b168-67aa93323f99": "Explain the concept of \"intersection over union\" (IOU) in the context of cell segmentation. How does the u-net algorithm perform in terms of IOU for the \"PhC-U373\" and \"DIC-HeLa\" data sets? Compare the u-net's performance with the second best algorithm for each data set.",
        "72f2bf37-f6b3-45e7-bc0f-bf1d5fb06a8b": "How does the u-net architecture differ from traditional neural networks in terms of training time and annotated images required?",
        "b4011397-e70a-4ba1-b1a7-a8c93be8cc99": "What are some of the references mentioned in the context information and how are they relevant to the study?",
        "7fc7e696-3b2b-453a-9362-c7f582b32c51": "Explain the improvements made to YOLOv3 compared to its previous version. How does it achieve both accuracy and speed in object detection?",
        "27860a9a-f096-4dc6-bfbf-35db0f89fdb2": "Describe the process of bounding box prediction in YOLOv3. How does it use dimension clusters and anchor boxes to predict the coordinates of the bounding boxes?",
        "a601e64a-a608-486a-ac74-3ebb8f879524": "Explain the concept of bounding box prediction in YOLOv3. How does it use dimension clusters as anchor boxes?",
        "bac7e518-ba40-4999-8426-c6339909e7e6": "How does YOLOv3 determine the objectness score for each bounding box? What is the significance of this score in the detection process?",
        "f1361daf-3bd8-4c9c-aada-8a9ed95ba98f": "Explain the concept of bounding boxes in object detection and how they are predicted in the YOLOv3 system.",
        "715b0d2a-d546-4a6d-a68c-7681008fdb98": "Compare and contrast the use of softmax and multilabel classification for predicting the classes of bounding boxes in the YOLOv3 system.",
        "7d09bd3c-f996-4f37-b5b3-6770c603bdab": "Explain the process of dividing clusters across scales in the context of the COCO dataset.",
        "dff95118-9eb3-473b-9bdc-3d6d97a98655": "Describe the architecture and key features of the Darknet-53 network used for feature extraction.",
        "a472a9bf-49b6-4a80-bf9b-40b86847f6df": "Compare the performance of Darknet-53 with ResNet-101 and ResNet-152 in terms of accuracy, floating point operations, and speed. How does Darknet-53 outperform these networks?",
        "cd552f0f-4f27-4125-9a3a-cb546bc283b3": "Explain the significance of Darknet-53 achieving the highest measured floating point operations per second. How does this contribute to its efficiency and speed compared to other networks like ResNet-101 and ResNet-152?",
        "a5ae788a-5a15-4695-96cc-e0d6fc6d17b3": "How does YOLOv3 perform in terms of mAP at IOU = 0.5 compared to other detection systems like RetinaNet and SSD variants? Explain the significance of this performance in relation to YOLOv3's ability to produce accurate object boxes.",
        "7af1eb53-2fce-459e-b999-4f8497ba81a5": "What are the challenges faced by YOLOv3 in terms of object detection for different object sizes? Discuss the performance of YOLOv3 on small, medium, and larger size objects and the need for further investigation in this regard.",
        "06b20ee2-3f1d-4ecd-9e10-e4fe67250136": "What are some of the object detection methods mentioned in the document? Provide a brief explanation of each method.",
        "76a4ef90-2718-4967-aa3f-92daadb698ba": "Discuss the tradeoff between speed and accuracy in object detection, as mentioned in the document. How does YOLOv3 perform in terms of speed and accuracy compared to other methods?",
        "f1e73128-be61-46ed-b6df-26a286e2ff87": "What is the purpose of the Pascal Visual Object Classes (VOC) challenge mentioned in reference [2]? How does it contribute to the field of computer vision?",
        "5d153f15-368e-4669-bfdf-996c3fde464a": "How does the Focal Loss, introduced in reference [9], improve dense object detection? Explain its advantages over other methods.",
        "34522455-1723-466c-a65a-5913d3c0511c": "In the context of computer vision, what are some popular object detection algorithms mentioned in the document? How do they differ from each other?",
        "c394f616-7c6b-427b-bce5-7c0d89973ec8": "How does the concept of citizen science and photographic identification contribute to animal population censusing? Discuss the potential benefits and challenges associated with this approach.",
        "5bba1f81-a2ec-4a78-bfc8-bb285c44d3f9": "In the context of object detection evaluation metrics, what is the reviewer's main criticism of the COCO metric? How does the reviewer argue that the COCO metric de-emphasizes classification accuracy?",
        "0f75c514-8376-456e-af67-2a031c6231e4": "According to the reviewer, why does the mAP metric have limitations in accurately assessing the performance of object detectors? Provide an example from the document to support your answer.",
        "e5340873-7dd3-4ee4-be9d-17fbf6cc2433": "Explain the concept of mean average precision (mAP) and its limitations in evaluating object detection models. Provide an example from the given context to support your explanation.",
        "93b2ce25-b542-4c24-ba5b-75fbfc33f0cb": "Propose an alternative metric for evaluating object detection models that addresses the discrepancies between current metrics and real-world concerns. Discuss the advantages and potential challenges of implementing this new metric.",
        "a5cfc2be-baf8-41be-9199-426ec299067f": "Explain the concept of multiple imputation and its importance in epidemiologic research. How does the multivariate imputation by chained equations (MICE) method work? Discuss the potential limitations of parametric imputation models and how random forest imputation can overcome these limitations.",
        "f88280cf-ae29-4cc4-985d-a9f71766cd4b": "In the context of missing data in epidemiologic studies, what is meant by \"missing at random\" (MAR)? How does multiple imputation address the issue of missingness? Compare and contrast the parametric MICE method with the random forest-based MICE algorithm in terms of their ability to impute missing data and produce unbiased parameter estimates. Discuss the advantages of using random forest imputation in complex epidemiologic datasets.",
        "ca485435-fbfc-4e67-81a8-709aeb11d821": "How does the proposed imputation method using random forest aim to overcome the problems mentioned in the paper?",
        "fcd85c96-b0fc-4f5c-9bbf-0f2fbe99bb8e": "Can you explain the concept of random forest and how it is used in classification and regression trees?",
        "513206c6-a666-4698-a148-99891d8966ad": "In the context of missing data imputation, what is the difference between MICE with linear regression and MICE with random forest? How does random forest help in reducing bias in parameter estimates?",
        "7e423907-0fbb-4ab8-a2dd-7c9d059fc587": "How does the missForest algorithm differ from the standard implementation of MICE? What are the potential limitations of using missForest for imputing missing values in statistical models?",
        "73d7ed2b-0afc-493b-9c25-88c2f5f1ddaf": "In the context of missing data imputation, explain how random forest is used to impute missing values for binary or unordered categorical variables. Provide a brief overview of the process and its equivalence to choosing between 0 and 1.",
        "1674c69b-c370-4ec8-a8bf-a08fac84cc14": "Describe the CALIBER database and its components, including the sources of data and the approval process for conducting studies using the database. Explain the specific cohort used in this study and the composite endpoint of interest.",
        "ab6b8c31-48ba-4aab-84fe-eaddbbae66ab": "In the context of multiple imputation approaches, what is the significance of the MAR assumption? How does it relate to the missingness pattern observed in the actual data set?",
        "e3fa1d35-8e26-4960-a000-0520303bae57": "Describe the process of generating artificial missingness in the data sets for the resampling study. How were predictor variables made missing in the random samples?",
        "c7f2c36a-7b1b-4cbd-b0d8-402285a9de29": "Explain the concept of multiple imputation and its role in handling missing data in the context of the study. How were continuous and categorical variables imputed using different methods?",
        "34f3f54a-adc3-479f-91cd-4e5bcc3c7a7a": "In the simulation study with interactions, what were the characteristics of the generated data sets? How did the presence of nonlinearities in the association between predictor variables impact the performance of the methods?",
        "60999e51-3d1d-4dfc-b180-e8a5dd1d6ec9": "In the simulation study based on CALIBER data, what was the prevalence of missingness for lymphocyte counts and neutrophil counts? How did patients with missing data differ from those with complete data in terms of the primary endpoint of death or nonfatal myocardial infarction?",
        "b5aa0c62-5492-40a1-b936-5be18960fb77": "What was the missingness mechanism used in the simulation study? How did the imputation estimates compare to those obtained from the complete-record analysis?",
        "0393f2f3-4bac-4080-8ab8-df036a658b41": "What were the main findings of the resampling study on methods for handling missing data? Compare the performance of parametric MICE and random forest MICE in terms of bias, efficiency, and confidence intervals.",
        "681d72ce-8320-4372-9f6b-28b0e99d5520": "How did random forest MICE perform in estimating the coefficient for the partially observed variable (x3) in the simulation study with interactions? Compare the bias, confidence intervals, and coverage probability of random forest MICE with different numbers of trees to parametric MICE.",
        "d4248622-64fa-4562-9aa3-4433ab25bedf": "In the study on methods for handling missing data, what were the two techniques compared and which one produced more efficient estimates and narrower confidence intervals?",
        "a4f8e29a-13d2-46ff-8513-3ac3e8b37a57": "According to the logistic regression model in the CALIBER database study, which factors were significantly associated with having a complete record in patients diagnosed with stable angina?",
        "13220965-850a-47ae-9f07-dd36450e9328": "How does random forest imputation compare to parametric MICE in terms of bias and efficiency in handling missing data in complex epidemiologic datasets?",
        "3ab47285-590f-41a2-ab4b-31a5d43acdc9": "Why is it important to correctly specify imputation models in order to obtain unbiased estimates, and how can random forest imputation help avoid bias that can occur with parametric MICE?",
        "155f420d-7d1e-4ab8-8fda-a5e9fc70bea2": "In the study mentioned, what were the findings regarding the performance of standard parametric MICE and random forest MICE in imputing missing data? How did the inclusion of interactions impact the performance of these methods?",
        "42bb915e-04f5-49a0-8cea-58b014a9dc03": "Compare the bias, standard deviation, and coverage of the 95% confidence interval for the variable \"Neutrophils (109cells/L), per doubling\" among different methods of handling missing data. Which method performed the best in terms of bias and coverage?",
        "c051bb67-47f4-4272-9701-f69ef465feb1": "What is the purpose of measuring bias relative to estimates from the full data set in this study? How does this help in analyzing the results?",
        "6036b774-5a65-4e20-8351-4fb83f3813b3": "Explain the concept of the zscore in the context of this study. Why is it important for the zscore to lie within the interval of -2 to +2?",
        "1dbe1d19-dc75-4d91-a6df-e4599fa72def": "In the context of handling missing data, what is the advantage of using random forest for imputation compared to other methods? Provide a specific example from the given data to support your answer.",
        "b2ac4dd1-484d-4f78-9796-ec434f76dcb8": "Based on the results presented in the table, compare the performance of MICE logistic regression and missForest methods in handling missing data for the variable \"Smoking status: current vs. never\". Which method shows better bias, estimated log HR, and coverage of 95% CI? Justify your answer using the provided data.",
        "03a6c567-4e35-4a59-8887-298a93e815d0": "In the context of the study, what is the purpose of using multivariate imputation by chained equations (MICE) and missForest techniques?",
        "e22f9b88-36ce-47e0-a1e6-a8d45648feab": "How is bias measured in this study and what does a zscore of approximately -2 to +2 indicate?",
        "29d17e29-2984-4ad6-b7a7-51c2705e9c25": "Explain the phenomenon observed in the study regarding the relationship between the number of trees and bias in random forest imputation. How does the direction of bias, asymptotic limit, and optimal number of trees vary depending on the data?",
        "004acf5c-3513-4e2f-b6d5-32d2730e938d": "Discuss the advantages and disadvantages of using random forest for imputing continuous variables. How does the complexity and interpretability of the models in random forest affect its suitability for imputation? Additionally, what potential bias can arise when using random forest for imputation?",
        "c3ed82bd-b1bc-48a5-8a2d-34bbb7c4e6e8": "Explain the limitations of random forest imputation in predicting continuous variables at the extremes of their range. How does this bias occur and why is it important to consider in data analysis?",
        "1db8d5e8-fe4c-4033-9201-1bceb809b487": "Compare and contrast the computational efficiency of random forest MICE and parametric MICE for imputing missing values. Discuss the potential trade-offs between computation time and analyst time.",
        "6d7a20fd-4fe6-4679-9b79-951d7d700f17": "In the context of handling missing data in a survival analysis, what are the potential consequences of ignoring practice-level clustering at the imputation and analysis stages? How can these consequences be properly accounted for?",
        "aad1722f-57ec-45ab-b8a2-5c5fbcd1f9cc": "Based on the comparisons between different methods of handling missing data in a survival analysis, which method shows the least bias in estimating the hazard ratio? Provide the specific method and its corresponding bias value.",
        "0a5e3873-bfa2-43c6-a0d3-529c812f7bcb": "What is the recommended method for multiple imputation in electronic health-record data, according to the context information?",
        "845d5d2e-a747-4265-b400-31cb693e42ee": "What is the potential benefit of using random forest imputation in situations where MICE with default parametric imputation models produces biased results, as mentioned in the context information?",
        "39c407d4-3f37-4c60-a642-3438d7381bfc": "In the field of genetic epidemiology, what are some regression-based and machine learning methods that have been used? Provide examples from the document to support your answer.",
        "48e8da45-dd8a-42a3-b873-27209264899d": "How does the random survival forests method differ from the random forests method? Explain the potential applications of each method in the context of medical research.",
        "05548231-c177-456d-b3e5-58b9e2b62fb0": "In the context of cardiac revascularization, what were the findings of the systematic review mentioned in the document? Provide a brief summary of the study's results and its implications for cardiovascular diseases.",
        "ebbc936e-c559-4051-92e2-c91ea5b6e7c7": "Explain the concept of multiple imputation and its relevance in statistical analysis, particularly in the context of handling missing covariate data when fitting a Cox proportional hazards model. Discuss the advantages and limitations of multiple imputation methods, citing examples from the document.",
        "0c244b80-dfe6-43f6-a405-50164bb52e53": "What are the current guidelines for pancreatic cancer screening and who is eligible for screening according to these guidelines? How does the Prism model aim to extend early detection of pancreatic cancer beyond the current screening criteria?",
        "c2e3a2be-8fda-44b0-b8a3-a637f34fac8d": "How does the Prism model (both PrismNN and PrismLR) perform in predicting pancreatic cancer risk? What are the AUC values for each model and how were they validated? Additionally, what is the significance of the Standardised Incidence Ratio (SIR) in evaluating the performance of the PrismNN model in simulated model deployment?",
        "691479f1-5cb1-41bd-998e-3cee8e655067": "What are the current screening guidelines for pancreatic ductal adenocarcinoma (PDAC) targeting stage I cancers and high-grade PDAC precursors? How have these guidelines improved long-term survival?",
        "6d50fc0a-1e7e-46fc-af95-4b134bedf5ee": "What is the current status of PDAC screening guidelines for the general population? How do most PDAC risk models for the general population aim to integrate with Electronic Health Record (EHR) systems for clinical implementation?",
        "b2503c70-3f97-4905-a5bd-3d46c3798df2": "How did the researchers in this study use electronic health record (EHR) data to develop and validate pancreatic cancer risk prediction models for the general population? Explain the methodology and the significance of using a federated data network.",
        "cc7665d2-ebd2-4eac-9181-2aaf7c61d7ae": "What are the limitations of previous studies on pancreatic cancer risk prediction models for the general population, and how does this study address those limitations? Discuss the added value of using EHR data from 55 US Health Care Organizations (HCOs) and the importance of internal and internal-external validation in the development of PDAC risk models.",
        "8994bfcf-fe30-47bc-8654-f7dbb9292162": "How does the Prism PDAC risk model differ from previous models developed for pancreatic duct adenocarcinoma (PDAC) risk assessment?",
        "5a52ed90-c136-4901-98c5-c57df79db3cf": "What are the potential implications of the Prism PDAC risk model for primary care providers and PDAC screening programs?",
        "404149e8-b54e-4a93-87bf-520fe06d3863": "In the study population, how were the PDAC cases and control group obtained? What criteria were used to select the PDAC cases and control group?",
        "ebf71163-c4e8-49be-9bba-8baf3afef799": "How were the neural networks (PrismNN) and logistic regression (PrismLR) models trained and evaluated in the study? What was the purpose of the validation set in the model development process?",
        "4c70a4a8-3956-4ccf-81fe-165261098a57": "How did the researchers improve the interpretability of the model by feature selection?",
        "aff4e09f-426e-400c-acd6-af03e5840bb6": "What method did the researchers use to evaluate the model's calibration?",
        "620a9d40-7935-4d0e-adbe-5d755474675a": "In the simulated deployment of the model, how were new patients enrolled into the study and how was their PDAC risk evaluated periodically? What factors determined the follow-up period for each enrolled patient?",
        "c8bc29b9-afb9-4f78-b07e-c126f0a6d36c": "How did the researchers account for unbalanced data sampling in order to estimate the Positive Predictive Value (PPV) and Standardised Incidence Ratio (SIR) of the model? What dataset was used for this estimation and what specific metrics were computed?",
        "1aaf467c-4f2b-4ba0-b5e4-d3c5f3f9d9a3": "In the study, what was the method used to calculate the expected number of PDAC cases in the high-risk group? How was the SEER database utilized in this calculation?",
        "27d7b20c-23bd-44c6-8caf-fb42ccd2ff1a": "What were the demographic characteristics of the cancer group and the control group in terms of sex, age, race, and healthcare organization (HCO) location? Provide specific numbers and percentages for each category.",
        "5337f313-890e-42dc-8f31-8120177ff249": "In the given context, what do the abbreviations AIAN, Black, and NHPI stand for? Provide a brief explanation for each abbreviation.",
        "3a07366a-2b65-49bd-ab20-dce48451ecb8": "Based on the demographic information provided in Table 1, calculate the mean and standard deviation for the medical records of the AIAN and Black populations. Compare and interpret the results.",
        "31e609dc-23a9-412e-bf5a-93ce8566dcdd": "Explain the calculation of I2 for geographic/racial subgroups in the study. What assumptions are made and what statistical models are used?",
        "32049bd5-4c5e-446d-9ed3-de756e3b34e8": "Compare the performance of PrismNN and PrismLR models in terms of average AUCs and GMOE. How do these models incorporate the presence or absence of features?",
        "6c08d155-6f1f-4a48-8fd7-fc4e8d6cac80": "In the context of the study, what are some of the known risk factors for pancreatic ductal adenocarcinoma (PDAC) that were included in the model features?",
        "91178bb4-f803-4d98-bdff-bd886b333098": "Based on the internal-external validation results, how did the performance of PrismNN and PrismLR models compare in terms of AUCs for location-based validation?",
        "f3c96f6b-c7ba-4e12-955d-00aa97a049aa": "What is the range of the AUC drop between the test and control models for PrismNN and PrismLR?",
        "6ab262f4-ddd0-4de5-8b94-f255b00d9619": "How does the performance of PrismNN and PrismLR models change over time according to the temporal validation results?",
        "792c3e8f-78c6-488a-a718-7da989a19f28": "What were the main findings and results of the study on identifying high-risk patients for pancreatic cancer using EHR data?",
        "030491e8-2d10-436f-876f-4813d515f90a": "How did the PrismNN and PrismLR models perform in terms of AUC and sensitivity for different age groups and locations?",
        "0ca3025a-6c06-43ad-bd43-b6a7f47bff80": "How does PrismNN aim to expand the eligibility for current screening programs that utilize imaging modalities such as Endoscopic UltraSound (EUS) and MRI/MRCP? How does it compare to the current eligibility criteria based on familial PDAC or known germline mutation syndrome?",
        "106b631b-dea3-4c3a-977c-ca1a1d8f1f85": "What are the two potential clinical use cases for Prism? Explain how PrismNN can be used to identify an enriched group for lower overhead testing, such as biomarker testing.",
        "96892f8a-29d2-4779-8252-289afe702d06": "Based on the ranked features in Fig. 3, what types of derived features are indicated by the letters in the brackets?",
        "8ebc7840-6c24-4ce2-9f99-f79ee0a98a6a": "How are the features in Fig. 3 ranked by the PrismNN algorithm?",
        "9c3fab0d-8820-4e32-aef8-ccaa7cddd504": "How can the deployment of the PrismNN model at a higher sensitivity impact screening behavior in clinical practice?",
        "21f54d72-86e5-4f95-9c42-8f6854efe873": "What types of internal-external validation were considered to evaluate the generalizability of the model, and what were the results of race-based and location-based validation?",
        "c046292f-18da-4932-ac7c-e426ce48753d": "What are the potential implications of the drop in AUC values in the context of EHR data from geographically different healthcare organizations? How does this affect the deployment of risk prediction models to healthcare organizations outside of the network?",
        "d9a986d1-38d4-42e7-8c17-15ddce4b1d4d": "How does the use of a federated EHR network contribute to the development, validation, and clinical deployment of risk prediction models? Discuss the advantages of seamless model integration and the potential barriers that can be overcome with this approach.",
        "c9708796-e9ae-47f7-87e1-344458378b22": "How does the evaluation in this study differ from previous studies in terms of the timing of risk identification for pancreatic ductal adenocarcinoma (PDAC)?",
        "ae67b1e6-4bdd-47db-b9b6-2844463304af": "What are the limitations of this study in terms of its retrospective nature and the need for prospective studies to evaluate the efficacy of clinical detection of early-stage PDAC?",
        "28550d16-2506-447a-9fa3-58ffa067fb67": "Explain the concept of Positive Predictive Value (PPV) and its relevance in healthcare research. How is it calculated and what factors should be considered when interpreting PPV results?",
        "2afd6fff-7a68-4384-9abf-371d34b7c76d": "The document mentions the use of Standardised Incidence Ratio (SIR) in the context of simulated deployment results. Discuss the significance of SIR in epidemiology and how it is calculated. How can SIR be used to assess the effectiveness of a healthcare intervention?",
        "a5ccb6d4-4799-4c77-8c8c-b327f96705c6": "How does the lack of standardization in data collection and the heterogeneity of EHR systems impact the generalizability of the Prism models for PDAC risk prediction?",
        "917f6be6-f03b-469e-b9a6-ae49e2d6edd9": "What are the potential implications of the biased presentation of data in Prism due to the limited access to the healthcare system for certain socioeconomic groups?",
        "1a30fed0-52cb-4edf-ac51-b1647a19fbed": "In the context of the provided information, what are the key features and benefits of the patents held by MR? How do these patents contribute to reducing power dissipation, translating text, optimizing graphical user interface tasks, acquiring and installing software upgrades, and detecting and repairing errors in data structures?",
        "f4bac326-6c51-4d3b-979a-e2002d3deecb": "Based on the acknowledgements section, what resources and support were provided by TriNetX for the research work? How did TriNetX contribute to the study, and what role did the TriNetX team of clinical informaticists, engineers, and technical staff play in the research process?",
        "096a9342-9aa9-4b7d-8466-875e99952841": "What are the key recommendations of the US Preventive Services Task Force regarding screening for pancreatic cancer? How do these recommendations align with the findings of the International Cancer of the Pancreas Screening (CAPS) Consortium Summit?",
        "833a4bca-fae6-4f4f-bb97-f319283f28d4": "How can changes in health reveal the possibility of undiagnosed pancreatic cancer? Describe the risk-prediction model based on healthcare claims data that was developed to identify individuals at risk.",
        "3820e76e-5434-44bc-aa7a-68778f9e8dda": "In the field of pancreatic cancer research, what are some of the different approaches and methods used to develop risk prediction models? Provide examples from the context information.",
        "9d3f1583-7a5d-4c0e-bb1b-d7a4fc6fe828": "How can the integration of electronic health records with clinical risk prediction tools improve the accuracy and effectiveness of pancreatic cancer risk assessment? Discuss the challenges and limitations mentioned in the context information.",
        "ec17b88d-4386-4e9d-8e11-80851dd0eeb2": "In the context of clinical prediction models, what is the purpose of external validation using big datasets from e-health records or IPD meta-analysis? How does this validation process present both opportunities and challenges?",
        "5744b04a-9a86-41ff-86b0-16abae906ba2": "The article mentions the use of DeLong's algorithm for comparing the areas under correlated receiver operating characteristic curves. Explain the significance of this algorithm in evaluating the performance of clinical prediction models."
    },
    "corpus": {
        "ad0820a3-36ce-4ce6-a82b-91d7ce31e98f": "Towards Conversational Diagnostic AI\nTao Tu\u2217,1, Anil Palepu\u2217,1, Mike Schaekermann\u2217,1,\nKhaled Saab1, Jan Freyberg1, Ryutaro Tanno2, Amy Wang1, Brenna Li1, Mohamed Amin1,\nNenad Tomasev2, Shekoofeh Azizi2, Karan Singhal1, Yong Cheng2, Le Hou1, Albert Webson2,\nKavita Kulkarni1, S. Sara Mahdavi2, Christopher Semturs1,\nJuraj Gottweis1, Joelle Barral2, Katherine Chou1, Greg S. Corrado1, Yossi Matias1,\nAlan Karthikesalingam\u2020,1and Vivek Natarajan\u2020,1\n1Google Research,2Google DeepMind\nAt the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for\naccurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable\nof diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating\nclinicians\u2019 expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical\nIntelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.\nAMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling\nlearning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating\nclinically-meaningful axes of performance including history-taking, diagnostic accuracy, management\nreasoning, communication skills, and empathy. We compared AMIE\u2019s performance to that of primary\ncare physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with\nvalidated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study\nincluded 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison\nwith AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater\ndiagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of\n26 axes according to patient actors. Our research has several limitations and should be interpreted with\nappropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale\nLLM-patient interactions but is not representative of usual clinical practice. While further research is\nrequired before AMIE could be translated to real-world settings, the results represent a milestone towards\nconversational diagnostic AI.\n1 Introduction\nThe dialogue between the physician and the patient is fundamental to effective and compassionate care. The\nmedical interview has been termed \u201cthe most powerful, sensitive, and most versatile instrument available\nto the physician\u201d [1]. In some settings, it is believed that 60-80% of diagnoses are made through clinical\nhistory-taking alone [2\u20136]. The physician-patient dialogue extends beyond history-taking and diagnosis; it\nis a complex interaction which establishes rapport and trust, serves as a tool for addressing health needs\nand can empower patients to make informed decisions that account for their preferences, expectations, and\nconcerns [7]. Clinicians wield considerable skills in clinical history-taking and the wider \u201cdiagnostic dialogue\u201d,\nbut access to this expertise remains episodic and globally scarce [8].\nRecent progress in general-purpose large language models (LLMs) [9\u201311] has shown that artificial intelli-\ngence (AI) systems have capabilities to plan, reason, and incorporate relevant context to hold naturalistic\nconversations. This progress affords an opportunity to rethink the possibilities of AI in medicine towards\nthe development of fully interactive conversational AI. Such medical AI systems would understand clinical\nlanguage, intelligently acquire information under uncertainty, and engage in natural, diagnostically useful\nmedical conversations with patients and those who care for them. The potential real-world utility of AI\nsystems capable of clinical and diagnostic dialogue is broad, as the development of such capabilities might\nimprove access to diagnostic and prognostic expertise, to improved quality, consistency, availability, and\naffordability of care, and to help realize better health outcomes (particularly for populations facing healthcare\ndisparities).\n\u2217Equal contributions. \u2020Equal leadership.\n\u2021Corresponding authors: {taotu, mikeshake, alankarthi, natviv}@google.comarXiv:2401.05654v1  [cs.AI]  11 Jan 2024",
        "3a53c86f-f433-4d57-96e3-b04b759e11f2": "Inference \nReasoning Chain DataMedical Reasoning \nReal-world Dialogue Medical \nSummarization Long-form \nMedical QA \nSimulated Dialogue Analyze \nContext \nGenerate \nResponse \nRefine \nResponse C ri t i c AMIE \nFine-tuning Simulated Dialogue Generator \nOuter Self-play \nModerator \nSimulated \nDialogue Patient Agent \nInner Self-play Doctor Agent \nCritic \nVignette \nGenerator \nAIPatient \nActor Scenario \nPack\nSimulated Patient Primary Care \nPhysician \n(PCP) \nAMIE Randomized Top-3 Diagnostic \nAccuracy Patient\u2019s Confidence \nin Care \nManagement \nPlan\nEscalation \nRecommendation Empathy Perceived Openness \n& Honesty AMIE System Design \nRandomized Study Design for Remote \nObjective Structured Clinical Examination (OSCE) AMIE Outperforms PCPs on \nMultiple Evaluation Axes for Diagnostic Dialogue AIPCP\nAMIE \nText Conversation \nSpecialist Physician \nPerspective Patient Actor \nPerspective Auto-evaluation Figure 1 |Overview of contributions. AMIE is a conversational medical AI optimised for diagnostic dialogue. AMIE is\ninstruction fine-tuned with a combination of real-world and simulated medical dialogues, alongside a diverse set of medical\nreasoning, question answering, and summarization datasets. Notably, we designed a self-play based simulated dialogue environment\nwith automated feedback mechanisms to scale AMIE\u2019s capabilities across various medical contexts and specialities. Specifically,\nthis iterative self-improvement process consisted of two self-play loops: (1) An \u201cinner\u201d self-play loop, where AMIE leveraged\nin-context critic feedback to refine its behavior on simulated conversations with an AI patient agent; (2) An \u201couter\u201d self-play loop\nwhere the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. During online inference,\nAMIE used a chain-of-reasoning strategy to progressively refine its response conditioned on the current conversation to arrive at\nan accurate and grounded reply to the patient in each dialogue turn. We designed and conducted a blinded remote Objective\nStructured Clinical Examination (OSCE) with validated simulated patient actors interacting with AMIE or Primary Care\nPhysicians (PCPs) via a text interface. Across multiple axes corresponding to both specialist physician (28 out of 32) and patient\nactor (24 out of 26) perspective, AMIE was rated as superior to PCPs while being non-inferior on the rest.\nHowever, while LLMs have been shown to encode clinical knowledge and proven capable of highly accurate\nsingle-turn medical question-answering [12\u201314], their conversational capabilities have been tailored to domains\noutside clinical medicine [15, 16]. Prior work in LLMs for health [12\u201314, 17, 18] has not yet rigorously\nexamined the clinical history-taking and diagnostic dialogue capabilities of AI systems or contextualized this\nby comparison to the extensive capabilities of expert clinicians.\nClinical history-taking and diagnostic dialogue through which clinicians derive diagnosis and management plans\nrepresent a complex skill [19] whose optimal conduct is highly dependent on context. Thus, multiple evaluation\naxes are needed to assess the quality of a diagnostic dialogue, including the structure and completeness of\n|2",
        "5af8e241-d861-434f-89d7-485c3cffe5ca": "the elicited history, diagnostic accuracy, the appropriateness of management plans and their rationale, and\npatient-centred considerations such as relationship-building, respect for the individual and communication\nefficacy [20]. If the conversational potential of LLMs is to be realized in medicine, there is a significant unmet\nneed to better optimize development and evaluation of medical AI systems for characteristics such as these,\nwhich are unique to history-taking and diagnostic dialogue between clinicians and patients.\nIn this work, we detail our progress towards a conversational medical AI system for clinical history-taking and\ndiagnostic reasoning.\nOur key contributions are summarized as:\n\u2022We introduced AMIE (Articulate Medical Intelligence Explorer), an LLM based AI system optimized for\nclinical history-taking and diagnostic dialogue.\n\u2022To scale AMIE across a multitude of specialties and scenarios, we developed a novel self-play based\nsimulated diagnostic dialogue environment with automated feedback mechanisms to enrich and accelerate\nits learning process. We also introduced an inference time chain-of-reasoning strategy to improve AMIE\u2019s\ndiagnostic accuracy and conversation quality.\n\u2022We developed a pilot evaluation rubric to assess the history-taking, diagnostic reasoning, communication\nskills and empathy of diagnostic conversational medical AI, encompassing both clinician-centred and\npatient-centred metrics.\n\u2022We designed and conducted a blinded remote OSCE study with 149 case scenarios from clinical providers\nin Canada, the UK, and India, enabling randomized and counterbalanced comparison of AMIE to PCPs\nwhen performing consultations with validated patient actors. AMIE exhibited superior diagnostic accuracy\ncompared to PCPs as assessed by various measures (e.g., top-1 and top-3 accuracy of the differential\ndiagnosis list). Across 28 out of 32 evaluation axes from the specialist physician perspective and 24 out\nof 26 evaluation axes from the patient actor perspective, AMIE was rated superior to PCPs while being\nnon-inferior on the rest.\n\u2022We performed a range of ablations to further understand and characterize the capabilities of AMIE,\nhighlighted important limitations, and proposed key next steps for real-world clinical translation of AMIE.\nOur research has important limitations, most notably that we utilized a text-chat interface, which although\nenabling potentially large-scale interaction between patients and LLMs specialized for diagnostic dialogue,\nwas unfamiliar to PCPs for remote consultation. Thus our study should not be regarded as representative of\nusual practice in (tele)medicine.\n|3",
        "4a09f481-f161-407f-bbd7-0dfb14819534": "AI\nFigure 2 |Overview of randomized study design. A primary care physician (PCP) and AMIE perform (in a randomized\norder) a virtual remote Objective Structured Clinical Examination (OSCE) with simulated patients via online multi-turn\nsynchronous text chat and produce answers to a post-questionnaire. Both the PCP and AMIE are then evaluated by both the\npatient actors as well as specialist physicians.\n2 AMIE: An LLM based AI System for Diagnostic Dialogue\nIn the following sections, we describe the real-world datasets, simulated self-play environment, fine-tuning\nprocess, and inference time chain-of-reasoning that we designed to optimize AMIE for diagnostic conversation\ncapabilities and clinical communication skills.\n2.1 Real-world Datasets for AMIE\nAMIE was developed using a diverse suite of real-world datasets including multiple-choice medical question-\nanswering, expert-curated long-form medical reasoning, electronic health record (EHR) note summaries, and\nlarge-scale transcribed medical conversation interactions. As described in detail below, in addition to dialogue\ngeneration tasks, the training task mixture for AMIE consisted of medical question-answering, reasoning, and\nsummarization tasks.\nMedical Reasoning. We used the MedQA (multiple-choice) dataset consisting of US Medical Licensing\nExamination (USMLE) multiple-choice style open domain questions with four or five possible answers [21].\nThe training set consisted of 11,450 questions and the test set had 1,273 questions. We also curated 191\nMedQA questions from the training set where clinical experts crafted step-by-step reasoning leading to the\ncorrect answer [13].\nLong-form Medical Question Answering. The dataset used here consisted of expert-crafted long-form\nresponses to 64 questions from HealthSearchQA, LiveQA, and Medication QA in MultiMedBench [12].\n|4",
        "2d4fef38-cf38-41b3-a161-e82d1fde493f": "Medical Summarization. A dataset consisting of 65 clinician-written summaries of medical notes from\nMIMIC-III, a large, publicly available database containing medical records of intensive care unit patients [22],\nwas used as additional training data for AMIE. MIMIC-III contains approximately 2 million notes spanning\n13 types including cardiology, respiratory, radiology, physician, general, discharge, case management, consult,\nnursing, pharmacy, nutrition, rehabilitation and social work. 5 notes from each category were selected, with a\nminimum total length of 400 tokens and at least one nursing note per patient. Clinicians were instructed to\nwrite abstractive summaries of individual medical notes, capturing key information while also permitting the\ninclusion of new informative and clarifying phrases and sentences not present in the original note.\nReal-world Dialogue. Here, we used a de-identified dataset licensed from a dialogue research organisation\ncomprising 98,919 audio transcripts of medical conversations during in-person clinical visits from over 1,000\nclinicians over a 10-year period in the United States [23]. It covered 51 medical specialties (primary care,\nrheumatology, hematology, oncology, internal medicine and psychiatry among others) and 168 medical\nconditions and visit reasons (type II diabetes, rheumatoid arthritis, asthma, depression among the common\nconditions). Audio transcripts contained utterances from different speaker roles such as doctors, patients, and\nnurses. On average a conversation had 149.8 turns ( P0.25= 75.0,P0.75= 196 .0). For each conversation, the\nmetadata contained information about patient demographics, reason for the visit (follow-up for pre-existing\ncondition, acute needs, annual exam and more), and diagnosis type (new, existing or other unrelated). We\nrefer to [23] for more details.\nFor this study, we selected dialogues involving only doctors and patients, but not other roles such as nurses.\nDuring preprocessing, we removed paraverbal annotations such as \u201c[LAUGHING]\u201d and \u201c[INAUDIBLE]\u201d from\nthe transcripts. We then divided the dataset into training (90%) and validation (10%) sets using stratified\nsampling based on condition categories and reasons for visits, resulting in 89,027 conversations for training\nand 9,892 for validation.\n2.2 Simulated Dialogue Learning Environment and Self-play for AMIE\nWhile passively collecting and transcribing real-world dialogues from in-person clinical visits is feasible, two\nsubstantial challenges limit its effectiveness in training LLMs for medical conversations: (1) existing real-world\ndata often fails to capture the vast range of medical conditions and scenarios, hindering its scalability and\ncomprehensiveness; (2) the data derived from real-world dialogue transcripts tends to be noisy, containing\nambiguous language (including slang, jargon, and sarcasm), interruptions, ungrammatical utterances, and\nimplicit references. This in turn, may limit AMIE\u2019s knowledge, capabilities, and applicability.\nTo address these limitations, we designed a self-play based simulated learning environment for diagnostic\nmedical dialogues in a virtual care setting, enabling us to scale AMIE\u2019s knowledge and capabilities across a\nmultitude of medical conditions and contexts. We used this environment to iteratively fine-tune AMIE with an\nevolving set of simulated dialogues in addition to the static corpus of medical QA, reasoning, summarization,\nand real-world dialogue data described above (see Figure 1).\nThis process consisted of two self-play loops:\n\u2022An \u201cinner\u201d self-play loop where AMIE leveraged in-context critic feedback to refine its behavior on\nsimulated conversations with an AI patient agent.\n\u2022An \u201couter\u201d self-play loop where the set of refined simulated dialogues were incorporated into\nsubsequent fine-tuning iterations. The resulting new version of AMIE could then participate in the\ninner loop again, creating a continuous learning cycle.\n|5",
        "125bcbea-fcb5-444e-b9c3-d2fa17f08e8b": "Simulated Dialogues. At each iteration of fine-tuning, we produced 11,686 dialogues, stemming from 5,230\ndifferent medical conditions. Conditions were selected from three datasets:\n\u2022Health QA dataset [12] which contained 613 common medical conditions.\n\u2022MalaCards Human Disease Database1which contained 18,455 less common disease conditions.\n\u2022MedicineNet Diseases & Conditions Index2which contained 4,617 less common conditions.\nAt each self-play iteration, four conversations were generated from each of the 613 common conditions,\nwhile two conversations were generated from each of the 4,617 less common conditions randomly chosen\nfrom MedicineNet and MalaCards. The average simulated dialogue conversation length was 21.28 turns\n(P0.25= 19.0,P0.75= 25.0).\nUsing simulated dialogues allowed us to address the limited availability of high-quality, labelled real-world\nconversation data and improved the model\u2019s generalization and adaptability to diverse medical contexts. By\nleveraging this self-play paradigm, AMIE could continuously learn and refine its conversational and diagnostic\ncapabilities during patient interactions.\n2.2.1 Simulated Dialogue Data Curation\nIn order to produce high-quality simulated dialogues at scale, we developed a novel multi-agent framework\nwhich comprised three key components:\n\u2022Vignette Generator : AMIE leverages web searches to craft unique patient vignettes given a specific\nmedical condition.\n\u2022Simulated Dialogue Generator : Three LLM agents play the roles of patient agent, doctor agent,\nand moderator, engaging in a turn-by-turn dialogue simulating realistic diagnostic interactions.\n\u2022Self-play Critic : A fourth LLM agent acts as a critic to give feedback to the doctor agent for self-\nimprovement. Notably, AMIE acted as all agents in this framework. We describe each component in\ndetail below.\nVignette Generator. The vignette generator aimed to create varied and realistic patient scenarios at\nscale, which could be subsequently used as context for generating simulated doctor-patient dialogues thereby\nallowing AMIE to undergo a training process emulating exposure to a greater number of conditions and\npatient backgrounds. The patient vignette (scenario) included essential background information such as\npatient demographics, symptoms, past medical history, past surgical history, past social history, and patient\nquestions, as well as an associated diagnosis and management plan.\nFor a given condition, patient vignettes were constructed using the following process. First, we retrieved 60\npassages (20 each) on the range of demographics, symptoms, and management plans associated with the\ncondition from using an internet search engine. To ensure these passages were relevant to the given condition,\nwe used the general-purpose LLM, PaLM-2 [10], to filter these retrieved passages, removing any passages\ndeemed unrelated to the given condition. We then prompted AMIE to generate plausible patient vignettes\naligned with the demographics, symptoms, and management plans retrieved from the filtered passages, by\nproviding a one-shot exemplar to enforce a particular vignette format. The prompts for each of these steps\nare as follows:\n1https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/malacards-diseases.json\n2https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/medicinenet-diseases.json\n|6",
        "9ddf8cec-22a0-4d7c-8961-b832457b15af": "Search Retrieval Template\nWhat are the specific patient demographics/symptoms/management plan for the condition [Condition]?\nPassage Filtering Template\nFor the clinical condition, [Condition], is the following a good description of common demograph-\nics/symptoms/management plans (Yes/No)?\nDescription: [Retrieved Passage]\nAnswer (Yes/No):\nVignette Generation Template\nThe following are several passages about the demographics, symptoms, and management plan for a\ngiven condition. Generate 2 different patient vignettes consistent with these passages. Follow the\nformat of the given example (just list N/A if a particular field is unavailable).\nCondition: [Condition]\nDemographic Passages: [Retrieved Demographic Passages]\nSymptoms Passages: [Retrieved Symptom Passages]\nManagement Plan Passages: [Retrieved Management Plan Passages]\nExample Format: [Oneshot example]\nPatient Vignettes for [Condition]:\nSimulated Dialogue Generator. Given a patient vignette detailing a specific medical condition, the\nsimulated dialogue generator was designed to simulate a realistic dialogue between a patient and a doctor in\nan online chat setting where in-person physical examination may not be feasible.\nThree specific LLM agents (patient agent, doctor agent, and moderator), each played by AMIE, were tasked\nwith communicating amongst each other to generate the simulated dialogues. Each agent had distinct\ninstructions. The patient agent embodied the individual experiencing the medical condition outlined in the\nvignette. Their role involved truthfully responding to the doctor agent\u2019s inquiries as well as raising any\nadditional questions or concerns they may have had. The doctor agent played the role of an empathetic\nclinician seeking to comprehend the patient\u2019s medical history within the online chat environment [24]. Their\nobjective was to formulate questions that could effectively reveal the patient\u2019s symptoms and background,\nleading to an accurate diagnosis and an effective treatment plan. The moderator continually assessed the\nongoing dialogue between the patient agent and doctor agent, determining when the conversation had reached\na natural conclusion.\nThe turn-by-turn dialogue simulation started with the doctor agent initiating the conversation: \u201cDoctor: So,\nhow can I help you today?\u201d. Following this, the patient agent responded, and their answer was incorporated\ninto the ongoing dialogue history. Subsequently, the doctor agent formulated a response based on the updated\ndialogue history. This response was then appended to the conversation history. The conversation progressed\nuntil the moderator detected the dialogue had reached a natural conclusion, when the doctor agent had\nprovided a differential diagnosis, treatment plan, and adequately addressed any remaining patient agent\nquestions, or if either agent initiated a farewell.\n|7",
        "98c84fb2-6a19-4536-92b7-38302cf271cc": "Patient Agent Instruction:\nYou are a patient chatting with a doctor over an online chat interface. The doctor has never met you\nbefore. <patient vignette> Respond to the doctor\u2019s questions honestly as they interview you, asking\nany questions that may come up.\nDoctor Agent Instruction:\nYou are an empathetic clinician asking a patient about their medical history over an online chat\ninterface. You know nothing about the patient in advance. Respond to the patient with a single-turn\nresponse to better understand their history and symptoms. Do not ask more than two questions. If\nthe patient asks a question, be sure to answer it appropriately.\nModerator Instruction:\nThe following is a conversation between a doctor and a patient: <dialog> The conversation should\nonly come to an end if the doctor has finished giving the patient a diagnosis and treatment plan and\nthe patient has no questions left. A conversation also comes to an end if the doctor or patient says\ngoodbye. Question: has the conversation come to an end? Yes or No.\nSelf-play Critic. To ensure high-quality dialogues, we implemented a tailored self-play [25] framework\nspecifically for self-improvement of diagnostic conversations. This framework introduced a fourth LLM agent,\nacting as a \u201ccritic\u201d which was also played by AMIE and aware of the ground truth diagnosis, to provide\nin-context feedback to the doctor agent and enhance its performance in subsequent conversations. The critic\nagent evaluated the doctor agent\u2019s responses based on the following criteria:\n\u2022The doctor agent exhibits empathy and professionalism while addressing the patient agent\u2019s latest\nquestions or comments in a concise manner.\n\u2022The doctor agent avoids asking too many or repetitive questions (about information already acquired),\nfocusing on a maximum of one or two per response.\n\u2022The responses should not reveal that the doctor agent is an AI chatbot. They should flow naturally,\nmaintain factual accuracy, and facilitate further engagement from the patient.\n\u2022The doctor agent asks sufficient questions to identify at least two of the most likely differential diagnoses.\nThey further refine their understanding through targeted questions towards the ground truth diagnosis\nand offer the corresponding treatment.\nFollowing the critic\u2019s feedback, the doctor agent incorporated the suggestions to improve its responses in\nsubsequent rounds of dialogue with the same patient agent from scratch. Notably, the doctor agent retained\naccess to its previous dialogue history at each new round. This self-improvement process was repeated twice\nto generate the dialogues used for each iteration of fine-tuning.\n2.3 Instruction Fine-tuning\nAMIE, built upon the base LLM PaLM 2 [10], was instruction fine-tuned to enhance its capabilities for\nmedical dialogue and reasoning. We refer to the PaLM-2 technical report for more details on the base LLM\narchitecture.\nWe employed task-specific instructions to fine-tune AMIE in playing either the patient or doctor role within\nmedical dialogues, performing medical question answering and reasoning, and summarizing EHR notes. While\nthe first round of fine-tuning from the base LLM only used the static datasets, subsequent rounds of fine-tuning\nleveraged the simulated dialogues generated through the self-play inner loop as described in Section 2.2.1.\nFor dialogue generation tasks, AMIE was trained to predict the next conversational turn based on all previous\ninteractions, assuming either the doctor or patient role. When playing the patient agent, AMIE was prompted\nto reply to the doctor agent\u2019s questions about their symptoms, drawing upon information provided in patient\nscenarios. These scenarios included patient vignettes (see Section 2.2.1) for simulated dialogues or metadata\nsuch as demographics, visit reason, and diagnosis type for the real-world dialogue dataset. In the doctor agent\nrole, AMIE was prompted to act as an empathetic clinician, interviewing patients about their medical history\n|8",
        "7fae7150-f8ee-42fc-8e21-3df8c8651507": "and symptoms to ultimately arrive at an accurate diagnosis. From each dialogue, we sampled on average 3\nturns for each the doctor and patient roles as the target turns to predict based on the conversation leading up\nto that target turn. Target turns were randomly sampled from all turns in the dialogue that had a minimum\nlength of 30 characters.\nSimilarly, for the EHR note summarization task, AMIE was provided with a clinical note and prompted to\ngenerate a summary of the note. Medical reasoning/QA and long-form response generation tasks followed\nthe same setup as in [13]. Notably, all tasks except dialogue generation and long-form response generation\nincorporated few-shot (1-5) exemplars in addition to task-specific instructions for additional context.\n2.4 Chain-of-reasoning for Online Inference\nTo address the core challenge in diagnostic dialogue - effectively acquiring information under uncertainty\nto enhance diagnostic accuracy and confidence while maintaining positive rapport with the patient - AMIE\nemployed a chain-of-reasoning strategy before generating a response in each dialogue turn. Here, \u201cchain-of-\nreasoning\u201d refers to a series of sequential model calls, each dependent on the outputs of prior steps. Specifically,\nwe used a three-step reasoning process, described as follows:\n1.Analyzing patient information: Given the current conversation history, AMIE was instructed to 1)\nsummarizethepositiveandnegativesymptomsofthepatientaswellasanyrelevantmedical/family/social\nhistory and demographic information, 2) produce a current differential diagnosis, 3) note missing\ninformation needed for a more accurate diagnosis and 4) assess confidence in the current differential and\nhighlight its urgency.\n2.Formulating response and action: Building upon the conversation history and the output of step\n1, AMIE performed the following: 1) Generate a response to the patient\u2019s last message and formulate\nfurther questions to acquire missing information and refine the differential diagnosis. 2) If necessary,\nrecommend immediate action, such as an emergency room visit. If confident in the diagnosis based on\navailable information, present the differential.\n3.Refining the response: AMIE revises its previous output to meet specific criteria based on the\nconversation history and outputs from earlier steps. The criteria are primarily related to factuality and\nformatting of the response (e.g., avoid factual inaccuracies on patient facts and unnecessary repetition,\nshow empathy, and display in a clear format).\nThis chain-of-reasoning strategy enabled AMIE to progressively refine its response conditioned on the current\nconversation to arrive at an informed and grounded reply.\n3 Evaluation\nPrior works developing models for clinical dialogue have focused on metrics such as the accuracy of note-to-\ndialogue or dialogue-to-note generations [26, 27], or natural language generation metrics such as BLEU or\nROUGE scores that fail to capture the clinical quality of a consultation [28, 29].\nIn contrast to these prior works we sought to anchor our human evaluation in criteria more commonly used\nfor evaluating the quality of physicians\u2019 expertise in history-taking, including their communication skills\nin consultation. We derived a framework from principles published in reviews of the consensus for best\npractices for patient-centered communication (PCCBP) in medical interviews [20], criteria examined for\nhistory-taking skills by the Royal College of Physicians in the UK as part of their Practical Assessment of\nClinical Examination Skills (PACES)3[30], and criteria proposed by the UK General Medical Council Patient\nQuestionnaire (GMCPQ)4for doctors seeking patient feedback as part of professional re-validation5. We\niterated upon these criteria to refine items for inclusion and derived pilot scales and instructions for assessment\nby using focus groups and interviews with clinicians and OSCE examiners based in the UK, Canada, US, and\nIndia. Our resulting pilot framework enabled assessment from two perspectives: clinician (board-certified\n3https://www.mrcpuk.org/mrcpuk-examinations/paces/marksheets\n4https://www.ed.ac.uk/sites/default/files/imports/fileManager/patient_questionnaire%20pdf_48210488.pdf\n5https://www.gmc-uk.org/registration-and-licensing/managing-your-registration/revalidation/revalidation-resources/\ncollecting-colleague-and-patient-feedback-for-revalidation\n|9",
        "d8bb0b8f-3e92-4c4a-a5ec-6cb525542db5": "physicians) and lay raters (patient actors). The framework included consideration of consultation quality,\nstructure and completeness, the roles, responsibilities, and skills of the interviewer (Tables A.1, A.2, A.3, and\nA.4).\n3.1 Objective Structured Clinical Examination\nObjective Structured Clinical Examination (OSCE) is a practical assessment format used in healthcare\nto assess clinical skills and competencies in a standardized and objective fashion [31\u201333]. It differs from\ntraditional written or oral exams that focus primarily on theoretical knowledge and instead aims to provide\nan environment in which the skills of real-world clinical practice might be assessed.\nThe OSCE is typically divided into multiple stations (often 8-12), each simulating a real-life clinical scenario\nenacted by standardized patient actors trained to portray specific symptoms or conditions based on pre-defined\nscenario descriptions. At each station, students are given specific tasks to perform, such as taking a clinical\nhistory, or making a diagnosis. Each station has a set time limit, ensuring fairness and efficient assessment.\nTrained examiners observe students\u2019 performance at each station using a pre-defined checklist or marking\nscheme. They assess clinical skills like communication, history-taking, physical examination techniques, clinical\nreasoning, and decision-making.\n3.2 Remote OSCE Study Design\nTo compare AMIE\u2019s performance to that of real clinicians, we conducted a randomized crossover study of\nblinded consultations in the style of a remote OSCE. Our OSCE study involved 20 board-certified primary\ncare physicians (PCPs) and 20 validated patient actors, 10 each from India and Canada, respectively, to\npartake in online text-based consultations. PCPs had between 3 and 25 years of post-residency experience\n(median 7 years). Patient actors comprised of a mix of medical students, residents, and nurse practitioners\nwith experience in OSCE participation. We sourced 149 scenario packs from India (75), Canada (60), and the\nUK (14).\nThe scenario packs and simulated patients in our study were prepared by two OSCE laboratories (one each in\nCanada and India), each affiliated to a medical school and with extensive experience in preparing scenario\npacks and simulated patients for OSCE examinations. UK scenario packs were sourced from the samples\nprovided on the MRCPUK website. Each scenario pack was associated with a ground truth diagnosis and a\nset of acceptable diagnoses. The scenario packs covered conditions from cardiovascular (29), respiratory (30),\ngastroenterology (31), neurology (30), urology, obstetric, and gynecology domains (15), and internal medicine\n(14). Pediatric or psychiatry domains were excluded from this study, as were intensive care or inpatient case\nmanagement scenarios.\nIndian patient actors played the roles in all India scenario packs and 7 of the 14 UK scenario packs. Canadian\npatient actors participated in scenario packs for both Canada and the other half of UK-based scenario packs.\nThis assignment process resulted in 149 distinct simulated patients (\u201cscenarios\u201d). Below, we use the term\n\u201cOSCE agent\u201d to refer to the conversational counterpart interviewing the patient actor, i.e., either PCP or\nAMIE. Table 1 summarizes the OSCE assignment information across three geographical locations. Each of\nthe 149 simulated patients completed the three-step study flow depicted in Figure 2.\nTable 1 |OSCE study summary. Number of scenario packs, patient actors, simulated patients, and primary care physicians\n(PCPs) in each of the three locations (Canada, India, and the UK) in the remote OSCE study. 20 board-certified PCPs\nparticipated in the study as OSCE agents in comparison with AMIE, 10 each from India and Canada. 20 trained patient actors\nwere involved, with 10 each from India and Canada. Indian patient actors played the roles in both India and UK scenario\npacks. Canadian patient actors participated in scenario packs for both Canada and the UK. This process resulted in 149 distinct\nsimulated patients.\nLocation # of Scenario Packs # of Simulated Patients # of Patient Actors # of PCPs\nCanada 60 67 10 10\nIndia 75 82 10 10\nUK 14 0 0 0\nTotal 149 149 20 20\n|10",
        "d1f3111c-ff3f-4d93-94d5-9de2d76511a8": "3.2.1 Online Text-based Consultation\nPCPs and patient actors were primed with sample scenarios and instructions, and participated in pilot\nconsultations prior to the study commencing in order to familiarize themselves with the interface and\nexperiment requirements.\nFor the experiment, each simulated patient completed two online text-based consultations via a synchronous\ntext chat interface (Figure A.2), one with a PCP (control) and one with AMIE (intervention). The ordering\nof PCP and AMIE was randomized and patient actors were not informed as to which they were talking to in\neach consultation. PCPs were located in the same country as patient actors, and were randomly drawn based\non availability at the specified time slot for the consultation. Patient actors role-played the scenario and were\ninstructed to conclude the conversation after no more than 20 minutes. Both OSCE agents were asked (PCPs\nvia study-specific instructions, and AMIE as part of the prompt template) to not reveal their identity, or\nwhether they were human, under any circumstances.\n3.2.2 Post-questionnaires\nUpon conclusion of the consultation, the patient actor and OSCE agent each filled in a post-questionnaire\nin light of the resulting consultation transcript (Figure A.3). The post-questionnaire for patient actors\nconsisted of the complete GMCPQ (Table A.1), the PACES components for \u201cManaging Patient Concerns\u201d and\n\u201cMaintaining Patient Welfare\u201d (Table A.2), and a checklist representation of the PCCBP category for \u201cFostering\nthe Relationship\u201d (Table A.3). Responses patient actors provided to the post-questionnaire are referred to\nas \u201cpatient actor ratings\u201d below. The post-questionnaire for the OSCE agent asked for a ranked differential\ndiagnosis (DDx) list with a minimum of 3 and no more than 10 conditions, as well as recommendations for\nescalation to in-person or video-based consultation, investigations, treatments, management plan, and the\nneed for a follow-up.\n3.2.3 Specialist Physician Evaluation\nFinally, a pool of 23 specialist physicians from India (14), North America (6), and the UK (3) evaluated PCPs\nand AMIE with respect to the quality of their consultation, and their responses to the post-questionnaire.\nDuring evaluation, specialist physicians also had access to the full scenario pack along with its associated\nground truth differential and additional accepted differentials. All of the data the specialist physicians had\naccess to during evaluation are collectively referred to as \u201cOSCE data\u201d below. Specialist physicians were\nsourced to match the specialties and geographic regions corresponding to the scenario packs included in our\nstudy, and had between 1 and 36 years of post-residency experience (median 5 years). Each set of OSCE data\nwas evaluated by one specialist physician randomly assigned to match the specialty and geographic region of\nthe underlying scenario (e.g., Canadian pulmonologist evaluated OSCE data from Canada-sourced respiratory\nmedicine scenario). Each specialist evaluated OSCE data from both PCP and AMIE for a given scenario.\nEvaluations for PCP and AMIE were conducted by the same specialist in a randomized and blinded sequence.\nEvaluation criteria included the accuracy, appropriateness and comprehensiveness of the provided DDx list,\nappropriateness of recommendations regarding escalation, investigation, treatment, management plan and\nfollow-up (Table A.4), and all PACES (Table A.2) and PCCBP (Table A.3) rating items. We also asked\nspecialist physicians to highlight confabulations in the consultations and questionnaire responses, i.e., text\npassages that were non-factual or referred to information not provided in the conversation. Each OSCE\nscenario pack additionally supplied specialists with scenario-specific clinical information to assist with rating\nthe clinical quality of the consultation, such as the ideal investigation or management plans; or important\naspects of the clinical history that would ideally have been elucidated for the highest quality of consultation\npossible.\n3.3 Auto-evaluation\nIn addition to human evaluations, we implemented model-based auto-evaluation methods as economical\nconsistent alternatives to specialist assessments. These techniques were employed to evaluate both dialogue\nquality and diagnostic accuracy of the OSCE agent. To establish the validity of our auto-evaluation methods\nfor assessing dialogue quality, we initially focused on a subset of four evaluation axes from the PACES rubric\n|11",
        "66a9edc0-bdd9-4ce1-8d0f-42b9401e5f8f": "(Table A.2) that were assessed by both the patient actors and the specialist physicians. The auto-evaluation,\nwhich uses a self-CoT strategy (details described in Section A.9) with AMIE to rate dialogues, was in\ngood alignment with human raters and comparable to the inter-specialist agreement on these criteria. For\nthe auto-evaluation of differential diagnoses, we leveraged another LLM, Med-PaLM 2 [13] as a surrogate\nfor a specialist rater to grade the predicted diagnoses against the ground truth diagnoses (more details\nin Section A.7). Our auto-evaluation on DDx accuracy showed a similar trend for AMIE and OSCE agents\ncompared to the specialist ratings. Overall, auto-evaluation trends aligned with human ratings for both\ndialogue quality and diagnostic accuracy.\nWe also conducted additional auto-evaluation analyses for the following purposes:\n\u2022To compare the performance of the DDx accuracy derived from AMIE or PCP consultations;\n\u2022To compare the DDx accuracy between simulated patients performed in Canada and India and determine\nif there is systematic differences between the two locations;\n\u2022To isolate the effects of information acquisition and information interpretation by analyzing the DDx\naccuracy of AMIE when provided the PCP consultation instead of its own;\n\u2022To evaluate the efficiency of information acquisition between AMIE and PCPs by analyzing the DDx\naccuracy as the number of conversation turns increases;\n\u2022To evaluate the benefit of inner-loop self-play on dialogue quality before and after critic feedback.\n3.4 Statistical Analysis\nWe evaluated the top-k accuracy of the DDx lists generated by AMIE and PCPs across all 149 simulated\npatients. Top-k accuracy was defined as the percentage of cases where the correct diagnosis appeared within\nthe top-k positions of the DDx list. Specifically, a candidate diagnosis was considered a match if the specialist\nrater marked it as either an exact match with, very close to or closely related to the ground truth diagnosis\n(or accepted differential). Statistical significance for DDx accuracy was determined using bootstrap tests [34]\nwith 10,000 samples and false discovery rate (FDR) correction [35] across all k. Statistical significance for\npatient actor and specialist ratings was determined using Wilcoxon signed-rank tests [36] FDR correction.\nCases where either agent received \u201cCannot rate / Does not apply\u201d were excluded from the test. Results below\nrefer to p-values after FDR correction.\n4 Results\n4.1 Diagnostic Accuracy\n4.1.1 AMIE showed higher DDx accuracy than PCPs under specialist physician evaluation.\nAMIE\u2019s diagnostic accuracy was assessed as higher than that of PCPs. Figure 3 shows the top-k accuracy for\nAMIE and PCPs, considering matches with the ground truth diagnosis (a) and matches with any item on\nthe accepted differential (b). AMIE showed significantly higher top-k accuracy than that of PCPs across all\nvalues of k ( p < 0.05). Note that unlike AMIE, PCPs did not always provide 10 diagnoses in their differential\ndiagnoses (min: 3, mean: 5.39). Additionally, we performed a comparison of DDx accuracy between AMIE\nand PCP by varying the matching criteria for determining a match. Results depicted in Figure A.7 further\nsubstantiate AMIE\u2019s superior DDx performance across various matching criteria.\nAccuracy by Specialty. Figure A.8 illustrates the DDx accuracy achieved by AMIE and PCPs across the\nsix medical specialties covered by scenarios in our study. We observed that AMIE\u2019s performance matched or\nsurpassed PCP performance for all specialties with the most pronounced improvements in the respiratory and\ncardiovascular specialities.\n4.1.2 Auto-evaluation suggested AMIE matched PCPs\u2019 efficiency in acquiring information.\nAuto-evaluation Accuracy. We reproduced the DDx accuracy analysis with our model-based auto-evaluator\ninstead of the specialist raters using the same procedure as in Figure 3. The overall performance trends\nobtained through the auto-evaluator align well with specialist assessments despite marginal differences in the\n|12",
        "fe76f48e-e823-47d7-8949-dab04bc3bbdc": "a\n b\nFigure 3 |Specialist-rated top-k diagnostic accuracy. AMIE and PCPs top-k DDx accuracy are compared across 149\nscenarios with respect to the ground truth diagnosis ( a) and all diagnoses in the accepted differential ( b). Bootstrapping\n(n=10,000) confirms all top-k differences between AMIE and PCP DDx accuracy are significant with p < 0.05after FDR\ncorrection.\ncomputed accuracy values, as shown in Figure A.9.\nIsolating the Source of Performance Gains. To investigate whether AMIE\u2019s superior DDx performance\nobserved in Figure 3 stemmed from improved information acquisition or from better diagnostic reasoning\ncapability, we compared AMIE\u2019s diagnoses based on its own consultations with AMIE\u2019s diagnoses generated\nfrom the corresponding PCP consultations, using the DDx auto-evaluator. Results depicted in Figure A.10\nrevealed markedly similar DDx performance, indicating that the diagnostic performance remained consistent\nregardless of whether AMIE processed information from its own dialogue or from the PCP\u2019s conversation. Both\nmethods significantly outperformed the differential diagnoses produced by PCPs. These results suggest that\nAMIE was approximately equivalent to PCPs at information acquisition but better than PCPs at interpreting\nthat information to produce an accurate/complete differential diagnosis.\nEfficiency of Information Acquisition. Although AMIE displayed greater verbosity compared to PCPs\nin terms of total number of words generated in their responses during the consultation, the number of\nconversational turns and the number of words elicited from the patient actors were similar across both OSCE\nagents, as illustrated in Figure A.11. This suggests that both AMIE and PCPs acquired a similar amount\nof information from the patients during the encounter. To investigate how efficient AMIE or PCPs were at\ngathering sufficient information to formulate a correct diagnosis, we truncated the conversations at various turn\ncounts and used AMIE to generate differential diagnoses based on these partial conversations. Figure A.12\ndepicts the top-3 DDx accuracy as a function of the number of turns provided to the model. The observed\naccuracies plateaued within the initial 10 conversational turns for both AMIE and PCPs. This suggests that\nboth AMIE and PCPs were able to acquire the information necessary for formulating a diagnosis within\nthe early stages of the conversation. Additionally, the comparable performance at every turn indicates that\nneither AMIE nor PCPs had a significant advantage in the efficiency or quality of information acquisition.\n4.2 Conversation Quality\n4.2.1 AMIE surpassed PCPs in conversation quality, per specialists and patient actors.\nConversation quality was assessed using patient actor ratings, specialist ratings, and outputs from auto-\nevaluation. Figure A.5 and A.6 show two example consultations for the same simulated patient from AMIE\nand PCP, respectively.\n|13",
        "ac4d0ec7-c007-43ed-b961-45936ce014b8": "0 20 40 60 80 100\n% ConsultationsPatient Happy to\nReturn In Future (Y/N)Patient Confident About\nCare Provided (Y/N)Appearing Honest\nAnd TrustworthyPatient Trusts Information\nis ConfidentialProviding Appropriate\nTreatment PlanInvolving Patient in\nTreatment DecisionsExplaining Condition\nand TreatmentAssessesing\nMedical ConditionListening to PatientMaking Patient\nFeel At EaseBeing Polite\n*** N=132*** N=142*** N=127*** N=118*** N=120*** N=131*** N=143*** N=146*** N=145*** N=146*** N=147GMCPQ\n0 20 40 60 80 100\n% ConsultationsValueing Patient\nas A Person (Y/N)Encouraging Patient\nParticipation (Y/N)Using Appropriate\nLanguage (Y/N)Greeting Patient\nAppropriately (Y/N)Acknowledging\nMistakes (Y/N)Expressing Caring\nAnd Commitment (Y/N)Engaging In Partnership\nBuilding (Y/N)Respecting Patient's\nPrivacy (Y/N)Discussing Roles And\nResponsibilities (Y/N)Appearing Open\nAnd Honest (Y/N)Building Rapport\nAnd Connection (Y/N)\n*** N=122*** N=128*** N=145** N=140n.s. N=41*** N=136*** N=115n.s. N=108*** N=118* N=115*** N=133PCCBP\n0 20 40 60 80 100\n% ConsultationsMaintaining\nPatient WelfareShowing EmpathyUnderstanding\nPatient ConcernsAddressing\nPatient Concerns\n*** N=145*** N=146*** N=146*** N=147Managing\nPatient ConcernsPACES\nAMIE (top)\nPCP (bottom)\n \nVery favorable\nFavorable (or \"Yes\" for Y/N)\nNeither favorable nor unfavorable\nUnfavorable (or \"No\" for Y/N)\nVery unfavorable\nCannot rate / Does not applyFigure 4 |Patient actor ratings. Conversation qualities as assessed by patient actors upon conclusion of the consultation.\nFor illustration purposes, all responses from five-point rating scales were mapped to a generic five-point scale ranging from \u2018Very\nfavorable\u2019 to \u2018Very unfavorable\u2019. For Yes/No questions, a (positive) \u2018Yes\u2019 response was mapped to the same color as \u2018Favorable\u2019\nand a (negative) \u2019No\u2019 response to the same color as \u2018Unfavorable\u2019. Rating scales were adapted from the General Medical Council\nPatient Questionnaire (GMCPQ), the Practical Assessment of Clinical Examination Skills (PACES), and a narrative review\nabout Patient-Centered Communication Best Practice (PCCBP). Details on question wording and response options are provided\nin Section A.1. Asterisks represent statistical significance ( \u2217:p < 0.05,\u2217\u2217:p < 0.01,\u2217 \u2217 \u2217 :p < 0.001,n.s. :not significant).\n|14",
        "7cc9f460-b210-4f15-ae73-d60115574341": "Patient Actor Ratings. Figure 4 presents the various conversation qualities patient actors assessed following\ntheir consultations with the OSCE agents. Overall, AMIE\u2019s consultations were rated significantly better\n(p < 0.05) by patient actors than those from PCPs across 24 of 26 axes. No significant differences in ratings\nwere detected for the two PCCBP axes \u201cRespecting Patient\u2019s Privacy\u201d (N=108) and \u201cAcknowledging Mistakes\u201d\n(N=41). For the latter criterion, the number of exclusions was substantially higher since the question applied\nonly when mistakes were made by the OSCE agent and pointed out in the conversation.\nSpecialist Physician Ratings. Specialist physicians evaluated both the conversational quality as well as\nthe responses to the post-questionnaire for scenarios within their domain expertise (see Figure 5). Again,\nAMIE\u2019s responses were rated significantly better by specialists than those from PCPs on 28 of 32 evaluation\naxes; Specialists preferred AMIE\u2019s consultation, diagnoses, and management plan over those from PCPs. For\nthis set of evaluations, differences in specialist ratings between AMIE and PCPs were statistically significant\n(p < 0.05). No significant differences in ratings were detected for four of the axes in the Diagnosis &\nManagement rubric, namely, \u201cEscalation Recommendation Appropriate\u201d, \u201cTreatment Inappropriate Avoided\u201d,\n\u201cFollowup Recommendation Appropriate\u201d and \u201cConfabulation Absent\u201d, despite no exclusions (N=149).\n4.2.2 Auto-evaluations demonstrated the effectiveness of inner self-play for AMIE.\nAuto-evaluation of Conversation Ratings. We leveraged the model-based self-CoT auto-evaluation\nstrategy to rate conversations on four evaluation axes from the PACES rubric, and validated that these\nauto-evaluation ratings were accurate and well aligned with the specialist ratings (Figures A.17 and A.18).\nFurthermore, to demonstrate that the inner self-play loop improved simulated dialogue quality, we applied\nthe auto-evaluation method to the simulated dialogues generated before and after the self-play procedure.\nResults in Figure A.19 revealed that the simulated dialogues after self-play were preferred more often than\nthe baseline dialogues without self-critique.\n5 Related Work\n5.1 Clinical History-taking and the Diagnostic Dialogue\nHistory-taking and the clinical interview are widely taught in both medical schools\u2019 and postgraduate\ncurricula [37\u201342]. Consensus on physician-patient communication has evolved to embrace patient-centred\ncommunication practices, with recommendations that communication in clinical encounters should address six\ncore functions: fostering the relationship, gathering information, providing information, making decisions,\nresponding to emotions and enabling disease- and treatment-related behavior [20, 43, 44]. Specific skills and\nbehaviours for meeting these goals have also been described, taught and assessed [20, 45] with validated\ntools [45]. Medical conventions consistently cite that certain categories of information should be gathered\nduring a clinical interview, comprising topics such as the presenting complaint, past medical history and\nmedication history, social and family history, and systems review [46, 47]. Clinicians\u2019 ability to meet these goals\nis commonly assessed using the framework of an objective structured clinical examination (OSCE) [31\u201333].\nSuch assessments vary in their reproducibility or implementation and have even been adapted for remote\npractice as virtual OSCEs (vOSCEs) with telemedical scenarios, an issue of particular relevance during the\nCOVID-19 pandemic [48].\n5.2 Conversational AI and Goal-oriented Dialogue\nConversational AI systems for goal-oriented dialogue and task completion have a rich history [49\u201351]. The\nemergence of transformers [52] and large language models [15] have led to renewed interest in this direction. The\ndevelopment of strategies for alignment [53], self-improvement [54\u201357] and scalable oversight mechanisms [58]\nhave enabled large scale deployment of such conversational systems in the real world [16, 59]. However, the\nrigorous evaluation and exploration of conversational and task-completion capabilities of such AI systems\nremains limited for clinical applications, where studies have largely focused on single-turn interaction use\ncases such as question-answering or summarization.\n|15",
        "a797a259-8746-4df1-bc0d-276acf88869a": "0 20 40 60 80 100\n% ConsultationsResponding T o EmotionsEnabling Disease And\nTreatment Related BehaviorDecision MakingProviding InformationGathering InformationRelationship Fostering\n*** N=149*** N=149*** N=149*** N=149*** N=149*** N=149PCCBP\n0 20 40 60 80 100\n% ConsultationsConfabulation\nAbsent (Y/N)Followup Recommendation\nAppropriate (Y/N)Treatment Inappropriate\nAvoided (Y/N)Treatment Appropriate\nRecommended (Y/N)Investigation Inappropriate\nAvoided (Y/N)Investigation Appropriate\nRecommended (Y/N)Escalation Recommendation\nAppropriate (Y/N)Management Plan\nAppropriatenessDDx Comprehensiveness\n(4-point scale)DDx Appropriateness\nn.s. N=149n.s. N=149n.s. N=149*** N=149** N=149*** N=149n.s. N=149*** N=149*** N=149*** N=149Diagnosis & Management\nAMIE (top)\nPCP (bottom)\n \nVery favorable\nFavorable (or \"Yes\" for Y/N)\nNeither favorable nor unfavorable\nUnfavorable (or \"No\" for Y/N)\nVery unfavorable\nCannot rate / Does not apply /\nAgent did not perform this0 20 40 60 80 100\n% ConsultationsMaintaining\nPatient WelfareShowing EmpathyUnderstanding\nPatient ConcernsAddressing\nPatient ConcernsClinical JudgementDifferential DiagnosisProfessionallyComprehensivelyWith StructureClearlyAccuratelyMedication HistoryFamily HistoryPast Medical HistorySystems ReviewPresenting Complaint\n*** N=149*** N=149*** N=149*** N=149*** N=149*** N=149*** N=147*** N=147*** N=147*** N=145*** N=146*** N=104*** N=81*** N=125*** N=136*** N=149ElicitingExplaining Relevant\nClinical InformationManaging\nPatient ConcernsPACESFigure 5 |Specialist physician ratings. Conversation and reasoning qualities as assessed by specialist physicians. For\nillustration purposes, all responses from five-point rating scales were mapped to a generic five-point scale ranging from \u2018Very\nfavorable\u2019 to \u2018Very unfavorable\u2019. The only four-point scale (DDx Comprehensiveness) was mapped to the same scale, ignoring the\n\u2018Neither favorable nor unfavorable\u2019 option. For Yes/No questions, a (positive) \u2018Yes\u2019 response was mapped to the same color as\n\u2018Favorable\u2019 and a (negative) \u2019No\u2019 response to the same color as \u2018Unfavorable\u2019. Rating scales were adapted from the Practical\nAssessment of Clinical Examination Skills (PACES), a narrative review about Patient-Centered Communication Best Practice\n(PCCBP), and other sources. Details on question wording and response options are provided in Section A.1. Asterisks represent\nstatistical significance ( \u2217:p < 0.05,\u2217\u2217:p < 0.01,\u2217 \u2217 \u2217 :p < 0.001,n.s. :not significant).\n|16",
        "ef44c4f0-f234-43b5-b975-77014638fbb4": "5.3 AI for Medical Consultations and Diagnostic Dialogue\nThe majority of explorations of AI as tools for conducting medical consultations have focused on \u201csymptom\nchecker\u201d applications rather than a full natural dialogue, or on topics such as transcription of medical audio\nor the generation of plausible dialogue given clinical notes or summaries [60\u201363]. Language models have been\ntrained using clinical dialogue datasets but not comprehensively evaluated [64]. Studies have been grounded in\nmessages between doctors and patients in commercial chat platforms (which may have altered doctor-patient\nengagement compared to 1:1 medical consultations) [28, 65, 66]. Many focused largely on predicting next\nturns in the recorded exchanges rather than clinically meaningful metrics. And to date, there have been no\nreported studies that have examined the quality of AI models for diagnostic dialogue using the same criteria\nthat are used to examine and train human physicians in dialogue and communication skills; nor evaluating AI\nsystems in common frameworks such as the OSCE.\n5.4 Evaluation of Diagnostic Dialogue\nPrior frameworks for human evaluation of AI systems\u2019 performance in diagnostic dialogue have been limited in\ndetail. They have not been anchored in established criteria for assessing communication skills and the quality of\nhistory-taking. For example, [29] reported a 5-point scale describing overall \u201chuman evaluation\u201d, [65] reported\n\u201crelevance, informativeness and human likeness\u201d, [66] reported \u201cfluency, expertise and relevance\u201d, [67] \u201cfluency\nand adequacy\u201d and [68] \u201cfluency\u201d. These criteria are far less comprehensive and specific than those taught\nand practiced by medical professionals. A multi-agent framework for assessing conversational capabilities of\nLLMs is introduced in [64], however, the study was performed in the restricted setting of dermatology, used\nAI models to emulate both doctor and patient sides of simulated interactions, and performed limited expert\nevaluation of history-taking as \u201ccomplete\u201d or not.\n6 Discussion\nIn this study, we introduced AMIE, an LLM based AI system optimised for clinical dialogue with diagnostic\nreasoning capabilities. We compared AMIE consultations to those performed by PCPs using a randomized,\ndouble-blind crossover study with human simulated patients in the style of an Objective Structured Clinical\nExamination (OSCE). Notably, our study was not designed to be representative of clinical conventions either\nfor traditional OSCE evaluations, for remote- or tele-medical consultation practices, or for the ways clinicians\nusually use text and chat messaging to communicate with patients. Our evaluation instead mirrored the\nmost common way by which people interact with LLMs today, leveraging a potentially scalable and familiar\nmechanism for AI systems to engage in remote diagnostic dialogue. In this setting, we observed that AMIE,\nan AI system optimised specifically for the task, outperformed PCPs on simulated diagnostic conversations\nwhen evaluated along multiple clinically-meaningful axes of consultation quality.\nDiagnostic Performance. The differential diagnoses provided by AMIE were more accurate and complete\nthan those provided by board-certified PCPs, when both were evaluated by specialist physicians. Previous\nresearch has shown that AI systems may match or exceed human diagnostic performance in specific, narrow\ntasks [69\u201371] in retrospective evaluation. However, these situations typically involved both AI and physicians\ninterpreting the same fixed input (for example, identifying the presence of a specific finding in a medical image).\nOur study was significantly more challenging because it required the AI system to actively acquire relevant\ninformation through conversation rather than relying on clinical information collated by human efforts [72].\nTherefore the system\u2019s downstream differential diagnoses depended on not only its diagnostic inference\ncapability, but also the quality of information gathered under uncertainty through natural conversation and\nbuilding rapport.\nOur results suggested that AMIE was as adept as PCPs in eliciting pertinent information during the simulated\nconsultations and was more accurate than PCPs in formulating a complete differential diagnosis if given\nthe same amount of acquired information. This finding corroborates other work that LLMs may be able to\nproduce more complete differential diagnoses given the same clinical information as physicians in challenging\ncases [70]. Though not explored in this study, the assistive performance of AMIE therefore represents an\ninteresting and important avenue for future research, particularly given the real-world importance of expert\n|17",
        "f4574399-2533-473b-8a62-0d7856fa30d3": "oversight for AI systems in safety-critical settings such as medicine.\nOur study utilized a wide variety of simulated patients, comprising actors trained in both Canada and India\nand scenarios across a range of specialties. This allowed us to explore how performance varied along multiple\naxes: by specialty, and by the locations in which the scenario was derived and enacted. We observed that both\nPCPs and AMIE performed worse in obstetric/gynecology and internal medicine scenarios than those from\nother specialties (see Figure A.8). The study was not powered or designed to compare performance between\ndifferent specialty topics, and we cannot exclude that the scenarios in some specialties might be harder than\nothers. We observed that both AMIE and PCPs had higher diagnostic accuracy in consultations performed in\nthe Canada OSCE lab compared to those enacted in the India OSCE lab (see Figure A.13). However, the\ndifferences were not statistically significant and in a subset of 40 scenarios enacted in both the Canada OSCE\nlab and the India OSCE lab, the performance of both AMIE and PCPs was equivalent (see Figure A.14).\nConversational Performance. Patient actors and specialist raters both evaluated AMIE\u2019s performance\nto be higher than PCPs on metrics related to empathy and communication skills. These axes comprised a\nmajority of the dimensions that were evaluated. This general finding is consistent with a prior study where\nLLM responses were found to be more empathetic than the responses from clinicians to health questions\nposted on Reddit [73]. However, the findings in that study may not be generalised directly to our setting due\nto the differences in study design. Specifically, prior work has not involved a direct, randomised comparison\nof physicians and AI systems in a prospective simulation of multi-turn dialogue with the same patient. In\nboth settings, the lack of voice-based and non-verbal visual communication may be an unfair disadvantage to\nclinicians.\nThe text-based chat interface used in this study introduces both advantages and disadvantages. People\ntoday most commonly engage with LLMs through synchronous text-chat interfaces [74], and patients often\nuse patient portals to send messages to their providers. We therefore chose this mode of interaction as a\nrepresentative interface for LLMs to perform multi-turn conversation, adapting the virtual OSCE framework\naccordingly. While this allowed a fair comparison of diagnostic dialogue between LLMs and clinicians when\nboth were restricted to a synchronous text-chat, it is important to acknowledge that our experiments do\nnot emulate the expected quality of diagnostic dialogue in real clinical practice (including telemedicine).\nPhysicians may be more used to history-taking and diagnostic dialogue by telephone or video consultation\nthan synchronous text-chat communication [75, 76]. Instead, text is more commonly used by clinicians to\ncommunicate with patients for episodic or asynchronous needs such as prescription refills or communication\nabout specific test results [77]. Physicians may thus be more familiar with text/SMS or email rather than the\nsynchronous text-chat medium we employed in this study. In both text/SMS and email, the conventions and\nexpectations for communicating naturally and with empathic style might be different [78]. It is possible that\nthe PCPs in our study had not yet become accustomed to the setting, and may have performed differently\nif subjected to a specific training program (similar in spirit to the training process for AMIE). Clinicians\nparticipating in the study undertook two preparatory pilot sessions of consultations with our synchronous\ntext interface before the evaluation began, but this was not a formal training program, nor was it designed\nto optimize clinicians\u2019 performance. Future research could explore this question more thoroughly including\nmonitoring for the impact of a learning curve, or exploring whether performance varies according to the extent\nto which participating clinicians or simulated patients are familiar with telemedicine.\nAdditionally, our findings regarding empathic communication could also be partially attributed to the fact\nthat AMIE responses were significantly longer than clinician responses (shown in Figure A.11), and presented\nwith greater structure. This could potentially suggest to an observer that more time was spent preparing\nthe response, analogous to known findings that patient satisfaction increases with time spend with their\nphysicians [79\u201381].\nCollectively, our findings suggest many avenues for further research that might leverage human-AI comple-\nmentarity [82], combining clinicians\u2019 skills in the analysis of verbal and non-verbal cues with the potential\nstrengths of LLMs to suggest more enriched conversational responses including empathic statements, structure,\neloquence, or more complete differential diagnoses.\nSimulated Dialogue. The use of simulated data allowed us to quickly scale training to a broad set of\nconditions and patient contexts, while the injection of knowledge from search encouraged these dialogues to\n|18",
        "11ea6212-cef2-47ed-a9e5-ab9a40959bbf": "remain grounded and realistic. Though the simulated patients encompassed a wide range of conditions, they\nfailed to capture the full range of potential patient backgrounds, personalities, and motivations. Through the\ninner self-play procedure, we were able to iteratively improve the simulated dialogue we generated and used\nin fine-tuning. However, these improvements were limited by our ability to articulate what makes a good\ndialogue in the critic instructions, the critic\u2019s ability to produce effective feedback, and AMIE\u2019s ability to\nadapt to such feedback. For example, in the simulated environment we impose that AMIE reaches a proposed\ndifferential and testing/treatment plan for the patient, but such an endpoint may be unrealistic for some\nconditions, especially in the virtual chat-based setting.\nEvaluation Framework. In contrast to prior works, we anchored our evaluation in criteria already established\nto be relevant for assessing physicians\u2019 communication skills and history-taking quality. We performed more\nextensive and diverse human evaluation than prior studies of AI systems, with ratings from both clinicians\nand simulated patients perspective. Our raters and scenarios were sourced from multiple geographic locations,\nincluding North America, India and the UK. Our pilot evaluation rubric is, to our knowledge, the first to\nevaluate LLMs\u2019 history-taking and communication skills using axes that are also measured in the real world\nfor physicians themselves, increasing the clinical relevance of our research. Our evaluation framework is\nconsiderably more granular and specific than prior works on AI-generated clinical dialogue, which have not\nconsidered patient-centred communication best practice or clinically-relevant axes of consultation quality [29,\n64\u201368].\nHowever, our pilot framework is not definitive and can be further improved in future research. History-taking\nitself is contextual and what determines a \u201cgood history\u201d is dependent on the specific clinical situation,\npatient and physician attributes, cultural characteristics, and many other factors. Despite variation in models\nfor clinical history-taking [83\u201386], studies have shown that good clinical interviews are associated with not\nonly problem detection and diagnostic accuracy, but also quadruple aims for care delivery [87, 88] ranging\nfrom patient and physician satisfaction, resilience to stress and illness, and health outcomes or cost. Future\nstudies on the quality of LLM history-taking might therefore utilise prospective measures of these outcomes\nin real-world settings (for example reductions in patient complaints [89], or improvements in cost and care\neffectiveness, patient and provider satisfaction), though evaluations as such may be challenging or impractical\nto compare to standard practice in the same individual patient, and randomisation of different approaches\nmay also be challenging in real-world settings.\nBreadth of Evaluation. Our chosen axes of evaluation were not exhaustive and their interpretation was\noften subjective in nature. Although we conducted evaluations from both clinician and lay-perspectives,\ngenerating scenario-packs in three countries with assessors in both North America and India, the pool of\nclinicians and lay-people assessing the models could be expanded further to improve generalization of our\ninsights. Our experiments could also undergo more extensive replication to explore other aspects such as\ninter-observer and inter-participant variability, including future work with an intentionally further diversified\npool of human raters (clinicians and lay users). Participatory design in the development of model evaluation\ntools with a representative pool of patients, as well as clinical and health equity domain experts, could also be\nvaluable.\nAlthough our scenarios comprised many different clinical conditions and specialties, our experiments were not\nnecessarily representative of the decades of clinical practice accumulated by even a single doctor (who on\naverage may perform tens of thousands of consultations in a career [90]). The range of conditions possible\nto examine in medicine is vast as is the variation in presentation of individual diseases. Our experiments\nwere not designed to examine multi-morbidity and co-incident pathology, longitudinal case presentation or\nthe consideration of sequential information from clinical investigations. We excluded entirely some clinical\nsettings or specialties such as psychiatry, pediatrics, intensive care, and inpatient case management scenarios.\nFurther research would be needed to understand the applicability of our findings in many settings such as\nthese, where the requirements for high-quality history-taking might differ [91, 92]. The OSCE framework is\ncommonly used in the assessment of clinicians\u2019 skills. It encompasses a significant range of methodologies\nincluding real or simulated patients, interaction with physical artefacts or clinical materials, applications to\na variety of medical specialties, tasks or settings; and both remote or in-person assessments. Although the\nOSCE approach is popular, there are significant limitations to its validity [93]. We utilised a remote text-based\n|19",
        "a6ae670b-3b61-47b2-adf5-8929e1b27a42": "assessment, replicating known issues with the paradigm of \u201cvirtual OSCE\u201d such as the inability to incorporate\nnon-verbal symptoms, signs and communication features. Additionally, this format could introduce unfamiliar\nconstraints to the communication of PCP participants [48].\nThe tone, content, and nature of the OSCE dialogues in our study are likely not to be representative of\nreal-world patient populations. For example, patient actors may have described their symptoms with greater\nstructure, depth or clinical detail than could be routinely expected in many consultations, or had greater\ncomprehension of clinical context than would be ordinarily expected. Furthermore, although evaluation was\nblinded, the style of responses from AMIE was notably different to that by PCPs which limits the practical\nextent of blinding in study design.\nTherefore even within the distribution of diseases and specialties we addressed, our findings should be\ninterpreted with humility and caution. There is a need for further research to examine varied presentations\nof the same diseases, alongside exploration of alternate approaches to evaluating history-taking and clinical\ndialogue in situations of different patient needs, preferences, behaviours and circumstances.\nFairness and Bias. The evaluation protocol presented in this paper is limited in terms of its ability to capture\npotential issues related to fairness and bias, which remains an important open question that we will aim to\naddress in subsequent system evaluations. Recent advances in the development of comprehensive frameworks\nfor bias detection in large language models [94, 95] present a promising starting point for establishing such\nan approach. It should be noted that medical diagnostic dialogue is a particularly challenging use case, due\nto the complexity of the medical domain, the interactive information gathering nature of the dialogue, and\nthe outcome-driven setting, with the potential of associated harms in case of incorrect diagnosis or incorrect\nmedical advice. Nevertheless, disentangling these issues is an important further research area if LLMs in\nthe domain are to overcome rather than propagate inequities in healthcare. For example, previous studies\nhave found that physicians approach communication with their patients differently, on average, depending\non patients\u2019 race, resulting in Black patients receiving communication that was less patient-centered, and\nwith a lower positive affect [96]. Other studies have found differences in physicians\u2019 communication styles and\nconversation length based on gender [97]. Effective intercultural communication skills are essential [91]. There\nis therefore a non-negligible risk that such historical conversational biases may be replicated or amplified\nin an AI dialogue system, but at the same time there is also an opportunity to work towards designing\nconversational systems that can be more inclusive, and more personalized to the individual patient\u2019s needs.\nTo help inform the development of the necessary fairness, bias, and equity frameworks, it is important to\nemploy a participatory approach to solicit representative views across a wide range of patient demographics,\nas well as clinical and health equity domain experts. Such evaluation frameworks should be complemented\nby extensive model red teaming and an adversarial approach to identifying any remaining gaps and failure\nmodes. Recent advances in red teaming LLMs could be useful in this scenario [98\u2013101]. These practices should\nnot only inform the evaluation of the final model, but also its development and iterative refinement. Model\ndevelopment should follow the established data and model reporting practices and provide transparency into\nthe training data and the associated decision processes [102\u2013104]. The dialogue research dataset contributing\nto AMIE training data in our study was de-identified, reducing the availability of socio-economic factors,\npatient demographics, and information about clinical settings and locations.\nFurther work is also needed to ensure the robustness of medical LLMs in multilingual settings [105\u2013108],\nand particularly their performance in low-resource languages [109]. The great variety of cultures [110],\nlanguages, localities, identities, and localized medical needs, makes the task of generating a priori static\nyet comprehensive fairness benchmarks practically infeasible. Measurement and mitigation of bias must\nmove beyond the traditional narrow focus on specific axes that fails to scale globally [111]. LLM-based\nevaluators present a potential solution for preliminary assessments in languages where there are no systematic\nbenchmarks, though prior studies have found these auto-evaluation frameworks to be biased, underscoring the\nneed for calibrating them on native speaker evaluations, and using them with caution [112].\nDeployment. This research demonstrates the potential of LLMs for future use in healthcare in the context\nof diagnostic dialogue. Transitioning from an LLM research prototype that has been evaluated in this study\nto a safe and robust tool that can be used by healthcare providers, administrators, and people will require\nsignificant additional research to ensure the safety, reliability, efficacy, and privacy of the technology. Careful\n|20",
        "e19ae042-3f4e-4731-9143-a0af3d0f7434": "consideration will need to be given to the ethical deployment of this technology including rigorous quality\nassessment across different clinical settings and research into reliable uncertainty estimation methods [113\u2013116]\nthat would allow for deferral to human clinical experts when needed. These and other guardrails are needed\nto mitigate potential overreliance on LLM technologies, with other specific measures for attention to ethical\nand regulatory requirements particular to future use-cases and the presence of qualified physicians in the loop\nto safeguard any model outputs. Additional research will also be needed to assess the extent to which biases\nand security vulnerabilities might arise either from base models or the circumstances of use in deployment, as\nwe have highlighted in our prior work [12]. Given the continuous evolution of clinical knowledge, it will also\nbe important to develop ways for LLMs to utilize up-to-date clinical information [117].\n7 Conclusion\nThe utility of medical AI systems could be greatly improved if they are better able to interact conversationally,\nanchoring on large-scale medical knowledge while communicating with appropriate levels of empathy and\ntrust. This research demonstrates the significant potential capabilities of LLM based AI systems for settings\ninvolving clinical history-taking and diagnostic dialogue. The performance of AMIE in simulated consultations\nrepresents a milestone for the field, as it was assessed along an evaluation framework that considered multiple\nclinically-relevant axes for conversational diagnostic medical AI. However, the results should be interpreted\nwith appropriate caution. Translating from this limited scope of experimental simulated history-taking\nand diagnostic dialogue, towards real-world tools for people and those who provide care for them, requires\nsignificant additional research and development to ensure the safety, reliability, fairness, efficacy, and privacy\nof the technology. If successful, we believe AI systems such as AMIE can be at the core of next generation\nlearning health systems that help scale world class healthcare to everyone.\nAcknowledgments\nThis project was an extensive collaboration between many teams at Google Research and Google DeepMind.\nWe thank Yun Liu, Daniel McDuff, Jake Sunshine, Ali Connell, Paul McGovern and Zoubin Ghahramani for\ntheir comprehensive review and detailed feedback on the manuscript. We also thank Sami Lachgar, Lauren\nWiner, John Guilyard and Maggie Shiels for contributions to the narratives and visuals. We are grateful\nto Julie Anne Seguin, Sally Goldman, Yuri Vasilevski, Xinying Song, Akshay Goel, Chu-ling Ko, Abhinav\nDas, Haiyang Yu, Chang Liu, Yuchen Liu, SiWai Man, Brett Hatfield, Sean Li, Ajay Joshi, Gordon Turner,\nAnnisah Um\u2019rani, Divya Pandya and Preeti Singh for their valuable insights, technical support and feedback\nduring our research. We also thank our clinical provider partners in Canada and India for their partnership in\nconducting the OSCE study. Finally, we are grateful to Dale Webster, Ewa Dominowska, David Fleet, Philip\nMansfield, Sushant Prakash, Renee Wong, Susan Thomas, Michael Howell, Karen DeSalvo, Jeff Dean, James\nManyika, Zoubin Ghahramani and Demis Hassabis for their support during the course of this project.\nData Availability\nSome of the real-world datasets used in the development of AMIE are open-source (MedQA). The scenario\npacks from UK used in the OSCE study are also available for download on the internet.\nCode Availability\nAMIE is an LLM based research AI system for diagnostic dialogue. We are not open-sourcing model code and\nweights due to the safety implications of unmonitored use of such a system in medical settings. In the interest\nof responsible innovation, we will be working with research partners, regulators, and providers to validate and\nexplore safe onward uses of AMIE. For reproducibility, we have documented technical deep learning methods\nwhile keeping the paper accessible to a clinical and general scientific audience. Our work builds upon PaLM 2,\nfor which technical details have been described extensively in the technical report [10].\nCompeting Interests\nThis study was funded by Alphabet Inc and/or a subsidiary thereof (\u2018Alphabet\u2019). All authors are employees\nof Alphabet and may own stock as part of the standard compensation package.\n|21",
        "cd80a54b-528b-4d47-8350-564d8885da94": "References\n1. Engel, G. L. & Morgan, W. L. Interviewing the patient (1973).\n2.Peterson, M. C., Holbrook, J. H., Von Hales, D., Smith, N. & Staker, L. Contributions of the history, physical examination,\nand laboratory investigation in making medical diagnoses. Western Journal of Medicine 156,163 (1992).\n3.Hampton, J. R., Harrison, M., Mitchell, J. R., Prichard, J. S. & Seymour, C. Relative contributions of history-taking,\nphysical examination, and laboratory investigation to diagnosis and management of medical outpatients. Br Med J 2,\n486\u2013489 (1975).\n4. Kassirer, J. P. Teaching clinical medicine by iterative hypothesis testing: let\u2019s preach what we practice 1983.\n5.Roshan, M. & Rao, A. A study on relative contributions of the history, physical examination and investigations in making\nmedical diagnosis. The Journal of the Association of Physicians of India 48,771\u2013775 (2000).\n6.Sandler, G. The importance of the history in the medical clinic and the cost of unnecessary tests. American heart journal\n100,928\u2013931 (1980).\n7. Silverman, J., Kurtz, S. & Draper, J. Skills for communicating with patients (crc press, 2016).\n8. Rennie, T., Marriott, J. & Brock, T. P. Global supply of health professionals. N Engl J Med 370,2246\u20137 (2014).\n9. OpenAI. GPT-4 Technical Report 2023. arXiv: 2303.08774 [cs.CL].\n10. Google. PaLM 2 Technical Report https://ai.google/static/documents/palm2techreport.pdf. 2023.\n11.Deepmind, G. Gemini: A Family of Highly Capable Multimodal Models https://assets.bwbx.io/documents/users/\niqjWHBFdfxIU/r7G7RrtT6rnM/v0. 2023.\n12.Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S.,\net al.Large Language Models Encode Clinical Knowledge. arXiv preprint arXiv:2212.13138 (2022).\n13.Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Hou, L., Clark, K., Pfohl, S., Cole-Lewis, H., Neal, D., et al.\nTowards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617 (2023).\n14.Nori, H., Lee, Y. T., Zhang, S., Carignan, D., Edgar, R., Fusi, N., King, N., Larson, J., Li, Y., Liu, W., et al.Can Generalist\nFoundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine. arXiv preprint arXiv:2311.16452\n(2023).\n15.Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H. -T., Jin, A., Bos, T., Baker, L., Du, Y.,\net al.LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022).\n16. OpenAI. Introducing ChatGPT OpenAI. https://openai.com/blog/chatgpt.\n17.Toma, A., Lawler, P. R., Ba, J., Krishnan, R. G., Rubin, B. B. & Wang, B. Clinical Camel: An Open-Source Expert-Level\nMedical Language Model with Dialogue-Based Knowledge Encoding. arXiv preprint arXiv:2305.12031 (2023).\n18.Chen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., Pagliardini, M., Fan, S., K\u00f6pf, A., Mohtashami, A.,\net al.MEDITRON-70B: Scaling Medical Pretraining for Large Language Models.",
        "da288752-6dd6-4aae-b1d1-842263d955f9": "arXiv preprint arXiv:2201.08239 (2022).\n16. OpenAI. Introducing ChatGPT OpenAI. https://openai.com/blog/chatgpt.\n17.Toma, A., Lawler, P. R., Ba, J., Krishnan, R. G., Rubin, B. B. & Wang, B. Clinical Camel: An Open-Source Expert-Level\nMedical Language Model with Dialogue-Based Knowledge Encoding. arXiv preprint arXiv:2305.12031 (2023).\n18.Chen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., Pagliardini, M., Fan, S., K\u00f6pf, A., Mohtashami, A.,\net al.MEDITRON-70B: Scaling Medical Pretraining for Large Language Models. arXiv preprint arXiv:2311.16079 (2023).\n19. Levine, D. History taking is a complex skill. BMJ358(2017).\n20.King, A. & Hoppe, R. B. \u201cBest practice\u201d for patient-centered communication: a narrative review. Journal of graduate\nmedical education 5,385\u2013393 (2013).\n21.Jin, D., Pan, E., Oufattole, N., Weng, W. -H., Fang, H. & Szolovits, P. What disease does this patient have? a large-scale\nopen domain question answering dataset from medical exams. Applied Sciences 11,6421 (2021).\n22.Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L. -w. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Anthony Celi,\nL. & Mark, R. G. MIMIC-III, a freely accessible critical care database. Scientific data 3,1\u20139 (2016).\n23.Chiu, C.-C., Tripathi, A., Chou, K., Co, C., Jaitly, N., Jaunzeikare, D., Kannan, A., Nguyen, P., Sak, H., Sankar, A., et al.\nSpeech recognition for medical conversations. arXiv preprint arXiv:1711.07274 (2017).\n24.Sharma, A., Miner, A. S., Atkins, D. C. & Althoff, T. A computational approach to understanding empathy expressed in\ntext-based mental health support. arXiv preprint arXiv:2009.08441 (2020).\n25.Fu, Y., Peng, H., Khot, T. & Lapata, M. Improving language model negotiation with self-play and in-context learning\nfrom ai feedback. arXiv preprint arXiv:2305.10142 (2023).\n26.Abacha, A. B., Yim, W. -W., Adams, G., Snider, N. & Yetisgen-Yildiz, M. Overview of the mediqa-chat 2023 shared tasks\non the summarization & generation of doctor-patient conversations inProceedings of the 5th Clinical Natural Language\nProcessing Workshop (2023), 503\u2013513.\n27.Ionescu, B., M\u00fcller, H., Dr\u0103gulinescu, A. -M., Yim, W. -W., Ben Abacha, A., Snider, N., Adams, G., Yetisgen, M., R\u00fcckert,\nJ., G. Seco de Herrera, A., et al. Overview of the ImageCLEF 2023: Multimedia Retrieval in Medical, Social Media and\nInternet Applications inInternational Conference of the Cross-Language Evaluation Forum for European Languages\n(2023), 370\u2013396.\n28.He, Z., Han, Y., Ouyang, Z., Gao, W., Chen, H., Xu, G. & Wu, J. DialMed: A Dataset for Dialogue-based Medication\nRecommendation. arXiv preprint arXiv:2203.07094 (2022).\n29.Naseem, U., Bandi, A., Raza, S., Rashid, J. & Chakravarthi, B. R. Incorporating Medical Knowledge to Transformer-based\nLanguage Models for Medical Dialogue Generation inProceedings of the 21st Workshop on Biomedical Language Processing\n(2022), 110\u2013115.",
        "0feeb27b-12dd-4891-8494-22ffe621365a": "Overview of the ImageCLEF 2023: Multimedia Retrieval in Medical, Social Media and\nInternet Applications inInternational Conference of the Cross-Language Evaluation Forum for European Languages\n(2023), 370\u2013396.\n28.He, Z., Han, Y., Ouyang, Z., Gao, W., Chen, H., Xu, G. & Wu, J. DialMed: A Dataset for Dialogue-based Medication\nRecommendation. arXiv preprint arXiv:2203.07094 (2022).\n29.Naseem, U., Bandi, A., Raza, S., Rashid, J. & Chakravarthi, B. R. Incorporating Medical Knowledge to Transformer-based\nLanguage Models for Medical Dialogue Generation inProceedings of the 21st Workshop on Biomedical Language Processing\n(2022), 110\u2013115.\n30.Dacre, J., Besser, M. & White, P. MRCP (UK) PART 2 Clinical Examination (PACES): a review of the first four\nexamination sessions (June 2001\u2013July 2002). Clinical Medicine 3,452 (2003).\n31.Sloan, D. A., Donnelly, M. B., Schwartz, R. W. & Strodel, W. E. The Objective Structured Clinical Examination. The\nnew gold standard for evaluating postgraduate clinical performance. Annals of surgery 222,735 (1995).\n32.Carraccio, C. & Englander, R. The objective structured clinical examination: a step in the direction of competency-based\nevaluation. Archives of pediatrics & adolescent medicine 154,736\u2013741 (2000).\n33. Epstein, R. M. & Hundert, E. M. Defining and assessing professional competence. Jama287,226\u2013235 (2002).\n34. Horowitz, J. L. in Handbook of econometrics 3159\u20133228 (Elsevier, 2001).\n35.Benjamini, Y. & Hochberg, Y. Controlling the false discovery rate: a practical and powerful approach to multiple testing.\nJournal of the Royal statistical society: series B (Methodological) 57,289\u2013300 (1995).\n|22",
        "a92d0f0a-1ad0-4808-bf9e-c0b1f170fa1c": "36. Woolson, R. F. Wilcoxon signed-rank test. Wiley encyclopedia of clinical trials, 1\u20133 (2007).\n37.Keifenheim, K. E., Teufel, M., Ip, J., Speiser, N., Leehr, E. J., Zipfel, S. & Herrmann-Werner, A. Teaching history taking\nto medical students: a systematic review. BMC medical education 15,1\u201312 (2015).\n38.Yedidia, M. J., Gillespie, C. C., Kachur, E., Schwartz, M. D., Ockene, J., Chepaitis, A. E., Snyder, C. W., Lazare, A. &\nLipkin Jr, M. Effect of communications training on medical student performance. Jama290,1157\u20131165 (2003).\n39. Makoul, G. Communication skills education in medical school and beyond. Jama289,93\u201393 (2003).\n40.Tan, X. H., Foo, M. A., Lim, S. L. H., Lim, M. B. X. Y., Chin, A. M. C., Zhou, J., Chiam, M. & Krishna, L. K. R.\nTeaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review. BMC\nmedical education 21,1\u201319 (2021).\n41.Raper, S. E., Gupta, M., Okusanya, O. & Morris, J. B. Improving communication skills: a course for academic medical\ncenter surgery residents and faculty. Journal of Surgical education 72,e202\u2013e211 (2015).\n42.Von Fragstein, M., Silverman, J., Cushing, A., Quilligan, S., Salisbury, H., Wiskin, C. & for Clinical Communication Skills\nTeaching in Undergraduate Medical Education, U. C. UK consensus statement on the content of communication curricula\nin undergraduate medical education. Medical education 42,1100\u20131107 (2008).\n43.De Haes, H. & Bensing, J. Endpoints in medical communication research, proposing a framework of functions and outcomes.\nPatient education and counseling 74,287\u2013294 (2009).\n44.Epstein, R. M. & Street Jr, R. L. Patient-centered communication in cancer care: promoting healing and reducing suffering\n(2007).\n45.Schirmer, J. M., Mauksch, L., Lang, F., Marvel, M. K., Zoppi, K., Epstein, R. M., Brock, D. & Pryzbylski, M. Assessing\ncommunication competence: a review of current tools. Family Medicine 37,184\u201392 (2005).\n46. Nichol, J. R., Sundjaja, J. H. & Nelson, G. Medical history. http://europepmc.org/books/NBK534249 (2018).\n47. Denness, C. What are consultation models for? InnovAiT 6,592\u2013599 (2013).\n48.Chan, S. C. C., Choa, G., Kelly, J., Maru, D. & Rashid, M. A. Implementation of virtual OSCE in health professions\neducation: A systematic review. Medical Education (2023).\n49.Budzianowski, P., Wen, T. -H., Tseng, B. -H., Casanueva, I., Ultes, S., Ramadan, O. & Ga\u0161i\u0107, M. Multiwoz\u2013a large-scale\nmulti-domain wizard-of-oz dataset for task-oriented dialogue modelling. arXiv preprint arXiv:1810.00278 (2018).\n50.Wei, W., Le, Q., Dai, A. & Li, J. Airdialogue: An environment for goal-oriented dialogue research inProceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing (2018), 3844\u20133854.\n51.Lin, J., Tomlin, N., Andreas, J. & Eisner, J. Decision-Oriented Dialogue for Human-AI Collaboration 2023. arXiv:\n2305.20076 [cs.CL].\n52.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141.",
        "eb796704-570f-414b-9fbe-7b50ea54d8c7": "& Ga\u0161i\u0107, M. Multiwoz\u2013a large-scale\nmulti-domain wizard-of-oz dataset for task-oriented dialogue modelling. arXiv preprint arXiv:1810.00278 (2018).\n50.Wei, W., Le, Q., Dai, A. & Li, J. Airdialogue: An environment for goal-oriented dialogue research inProceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing (2018), 3844\u20133854.\n51.Lin, J., Tomlin, N., Andreas, J. & Eisner, J. Decision-Oriented Dialogue for Human-AI Collaboration 2023. arXiv:\n2305.20076 [cs.CL].\n52.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141. & Polosukhin, I. Attention is all\nyou need. Advances in neural information processing systems 30(2017).\n53.Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al.Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 (2022).\n54.Zhao, J., Khashabi, D., Khot, T., Sabharwal, A. & Chang, K. -W. Ethical-advice taker: Do language models understand\nnatural language interventions? arXiv preprint arXiv:2106.01465 (2021).\n55.Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J. & Leike, J. Self-critiquing models for assisting human\nevaluators. arXiv preprint arXiv:2206.05802 (2022).\n56.Scheurer, J., Campos, J. A., Korbak, T., Chan, J. S., Chen, A., Cho, K. & Perez, E. Training language models with\nlanguage feedback at scale. arXiv preprint arXiv:2303.16755 (2023).\n57.Glaese, A., McAleese, N., Tr\u0119bacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M.,\nThacker, P., et al.Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375\n(2022).\n58.Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C.,\net al.Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 (2022).\n59.Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., et al.\nA general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861 (2021).\n60.Shor, J., Bi, R. A., Venugopalan, S., Ibara, S., Goldenberg, R. & Rivlen, E. Clinical BERTScore: An Improved Measure of\nAutomatic Speech Recognition Performance in Clinical Settings. arXiv preprint arXiv:2303.05737 (2023).\n61.Abacha, A. B., Agichtein, E., Pinter, Y. & Demner-Fushman, D. Overview of the medical question answering task at\nTREC 2017 LiveQA. inTREC(2017), 1\u201312.",
        "7e5bcc74-1b87-4ae6-9940-a4e174c6217a": "A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861 (2021).\n60.Shor, J., Bi, R. A., Venugopalan, S., Ibara, S., Goldenberg, R. & Rivlen, E. Clinical BERTScore: An Improved Measure of\nAutomatic Speech Recognition Performance in Clinical Settings. arXiv preprint arXiv:2303.05737 (2023).\n61.Abacha, A. B., Agichtein, E., Pinter, Y. & Demner-Fushman, D. Overview of the medical question answering task at\nTREC 2017 LiveQA. inTREC(2017), 1\u201312.\n62.Wallace, W., Chan, C., Chidambaram, S., Hanna, L., Iqbal, F. M., Acharya, A., Normahani, P., Ashrafian, H., Markar,\nS. R., Sounderajah, V., et al.The diagnostic and triage accuracy of digital and online symptom checker tools: a systematic\nreview.NPJ Digital Medicine 5,118 (2022).\n63.Zeltzer, D., Herzog, L., Pickman, Y., Steuerman, Y., Ber, R. I., Kugler, Z., Shaul, R. & Ebbert, J. O. Diagnostic accuracy\nof artificial intelligence in virtual primary care. Mayo Clinic Proceedings: Digital Health 1,480\u2013489 (2023).\n64.Johri, S., Jeong, J., Tran, B. A., Schlessinger, D. I., Wongvibulsin, S., Cai, Z. R., Daneshjou, R. & Rajpurkar, P. Testing\nthe Limits of Language Models: A Conversational Framework for Medical AI Assessment. medRxiv, 2023\u201309 (2023).\n65.Zeng, G., Yang, W., Ju, Z., Yang, Y., Wang, S., Zhang, R., Zhou, M., Zeng, J., Dong, X., Zhang, R., et al. MedDialog:\nLarge-scale medical dialogue datasets inProceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP) (2020), 9241\u20139250.\n66.Liu, W., Tang, J., Cheng, Y., Li, W., Zheng, Y. & Liang, X. MedDG: an entity-centric medical consultation dataset for\nentity-aware medical dialogue generation inCCF International Conference on Natural Language Processing and Chinese\nComputing (2022), 447\u2013459.\n67.Varshney, D., Zafar, A., Behra, N. K. & Ekbal, A. Cdialog: A multi-turn COVID-19 conversation dataset for entity-aware\ndialog generation. arXiv preprint arXiv:2212.06049 (2022).\n68.Yan, G., Pei, J., Ren, P., Ren, Z., Xin, X., Liang, H., de Rijke, M. & Chen, Z. ReMeDi: Resources for Multi-domain,\nMulti-service, Medical Dialogues inProceedings of the 45th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (2022), 3013\u20133024.\n69.Kelly, C. J., Karthikesalingam, A., Suleyman, M., Corrado, G. & King, D. Key challenges for delivering clinical impact\nwith artificial intelligence. BMC medicine 17,1\u20139 (2019).\n|23",
        "28437284-98b4-4530-b8ec-7a51e472c1d8": "70.McDuff, D., Schaekermann, M., Tu, T., Palepu, A., Wang, A., Garrison, J., Singhal, K., Sharma, Y., Azizi, S., Kulkarni, K.,\net al.Towards Accurate Differential Diagnosis with Large Language Models. arXiv preprint arXiv:2312.00164 (2023).\n71.Kanjee, Z., Crowe, B. & Rodman, A. Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic\nChallenge. JAMA(2023).\n72.Semigran, H. L., Linder, J. A., Gidengil, C. & Mehrotra, A. Evaluation of symptom checkers for self diagnosis and triage:\naudit study. BMJ351(2015).\n73.Ayers, J. W., Poliak, A., Dredze, M., Leas, E. C., Zhu, Z., Kelley, J. B., Faix, D. J., Goodman, A. M., Longhurst, C. A.,\nHogarth, M., et al.Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a\nPublic Social Media Forum. JAMA Internal Medicine (2023).\n74. OpenAI. ChatGPT OpenAI. https://chat.openai.com/chat.\n75.Carrillo de Albornoz, S., Sia, K. -L. & Harris, A. The effectiveness of teleconsultations in primary care: systematic review.\nFamily Practice 39,168\u2013182 (2022).\n76.Wharton, G. A., Sood, H. S., Sissons, A. & Mossialos, E. Virtual primary care: fragmentation or integration? The Lancet\nDigital Health 1,e330\u2013e331 (2019).\n77.Fuster-Casanovas, A. & Vidal-Alaball, J. Asynchronous Remote Communication as a Tool for Care Management in\nPrimary Care: A Rapid Review of the Literature. International Journal of Integrated Care 22(2022).\n78.Hammersley, V., Donaghy, E., Parker, R., McNeilly, H., Atherton, H., Bikker, A., Campbell, J. & McKinstry, B. Comparing\nthe content and quality of video, telephone, and face-to-face consultations: a non-randomised, quasi-experimental,\nexploratory study in UK primary care. British Journal of General Practice 69,e595\u2013e604 (2019).\n79.Gross, D. A., Zyzanski, S. J., Borawski, E. A., Cebul, R. D. & Stange, K. C. Patient satisfaction with time spent with\ntheir physician. Journal of Family Practice 47,133\u2013138 (1998).\n80.Tates, K., Antheunis, M. L., Kanters, S., Nieboer, T. E. & Gerritse, M. B. The effect of screen-to-screen versus face-to-face\nconsultation on doctor-patient communication: an experimental study with simulated patients. Journal of medical Internet\nresearch19,e421 (2017).\n81.Zyzanski, S. J., Stange, K. C., Langa, D. M. & Flocke, S. A. Trade-offs in high-volume primary care practice. Journal of\nFamily Practice 46,397\u2013402 (1998).\n82.Dvijotham, K., Winkens, J., Barsbey, M., Ghaisas, S., Stanforth, R., Pawlowski, N., Strachan, P., Ahmed, Z., Azizi, S.,\nBachrach, Y., et al.Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to\nclinicians. Nature Medicine 29,1814\u20131820 (2023).\n83. Bird, J. & Cohen-Cole, S. A. in Methods in teaching consultation-liaison psychiatry 65\u201388 (Karger Publishers, 1990).\n84.Rezler, A. G., Woolliscroft, J. A. & Kalishman, S. G. What is missing from patient histories? Medical Teacher 13,245\u2013252\n(1991).\n85. Rosenberg, E. E. Lessons for Clinicians From Physician-Patient. Arch Fam Med 6,279\u2013283 (1997).\n86.",
        "350096e9-f112-4a76-8ebc-6caa181ea212": "Nature Medicine 29,1814\u20131820 (2023).\n83. Bird, J. & Cohen-Cole, S. A. in Methods in teaching consultation-liaison psychiatry 65\u201388 (Karger Publishers, 1990).\n84.Rezler, A. G., Woolliscroft, J. A. & Kalishman, S. G. What is missing from patient histories? Medical Teacher 13,245\u2013252\n(1991).\n85. Rosenberg, E. E. Lessons for Clinicians From Physician-Patient. Arch Fam Med 6,279\u2013283 (1997).\n86. Smith, R. C. Patient-centered interviewing: an evidence-based method (Lippincott Williams & Wilkins, 2002).\n87.Berwick, D. M., Nolan, T. W. & Whittington, J. The triple aim: care, health, and cost. Health affairs 27,759\u2013769 (2008).\n88.Bodenheimer, T. & Sinsky, C. From triple to quadruple aim: care of the patient requires care of the provider. The Annals\nof Family Medicine 12,573\u2013576 (2014).\n89.Adamson, T. E., Tschann, J. M., Gullion, D. & Oppenberg, A. Physician communication skills and malpractice claims. A\ncomplex relationship. Western Journal of Medicine 150,356 (1989).\n90.Silverman, J. & Kinnersley, P. Doctors\u2019 non-verbal behaviour in consultations: look at the patient before you look at the\ncomputer 2010.\n91.Rahman, U. & Cooling, N. Inter-Cultural Communication Skills Training in Medical Schools: A Systematic Review.\nMedical Research Archives 11(2023).\n92.Kantar, A., Marchant, J. M., Song, W. -J., Shields, M. D., Chatziparasidis, G., Zacharasiewicz, A., Moeller, A. & Chang,\nA. B. History taking as a diagnostic tool in children with chronic cough. Frontiers in pediatrics 10,850912 (2022).\n93.Setyonugroho, W., Kennedy, K. M. & Kropmans, T. J. Reliability and validity of OSCE checklists used to assess the\ncommunication skills of undergraduate medical students: a systematic review. Patient education and counseling 98,\n1482\u20131491 (2015).\n94.Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P. -S., Mellor, J., Glaese, A., Cheng, M., Balle, B., Kasirzadeh, A.,\net al. Taxonomy of risks posed by language models inProceedings of the 2022 ACM Conference on Fairness, Accountability,\nand Transparency (2022), 214\u2013229.\n95.Gallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R. & Ahmed, N. K.\nBias and Fairness in Large Language Models: A Survey 2023. arXiv: 2309.00770 [cs.CL].\n96.Johnson, R. L., Roter, D., Powe, N. R. & Cooper, L. A. Patient race/ethnicity and quality of patient\u2013physician\ncommunication during medical visits. American journal of public health 94,2084\u20132090 (2004).\n97.Roter, D. L., Hall, J. A. & Aoki, Y. Physician gender effects in medical communication: a meta-analytic review. Jama\n288,756\u2013764 (2002).\n98.Perez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A., McAleese, N. & Irving, G. Red teaming\nlanguage models with language models. arXiv preprint arXiv:2202.03286 (2022).\n99.Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K.,\net al.Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.",
        "4749cc9a-f6e9-4f02-94c8-dd3ff56cbc39": "97.Roter, D. L., Hall, J. A. & Aoki, Y. Physician gender effects in medical communication: a meta-analytic review. Jama\n288,756\u2013764 (2002).\n98.Perez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A., McAleese, N. & Irving, G. Red teaming\nlanguage models with language models. arXiv preprint arXiv:2202.03286 (2022).\n99.Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K.,\net al.Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint\narXiv:2209.07858 (2022).\n100.Yu, J., Lin, X. & Xing, X. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. arXiv\npreprint arXiv:2309.10253 (2023).\n101.Ge, S., Zhou, C., Hou, R., Khabsa, M., Wang, Y. -C., Wang, Q., Han, J. & Mao, Y. MART: Improving LLM Safety with\nMulti-round Automatic Red-Teaming. arXiv preprint arXiv:2311.07689 (2023).\n102.Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I. D. & Gebru, T. Model\ncards for model reporting inProceedings of the conference on fairness, accountability, and transparency (2019), 220\u2013229.\n103.Crisan, A., Drouhard, M., Vig, J. & Rajani, N. Interactive model cards: A human-centered approach to model documentation\ninProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (2022), 427\u2013439.\n|24",
        "d861eb80-ed55-4c34-9db8-80afaea62b4f": "104.Pushkarna, M., Zaldivar, A. & Kjartansson, O. Data cards: Purposeful and transparent dataset documentation for\nresponsible ai inProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (2022), 1776\u2013\n1826.\n105.Choudhury, M. & Deshpande, A. How Linguistically Fair Are Multilingual Pre-Trained Language Models? inProceedings\nof the AAAI conference on artificial intelligence 35(2021), 12710\u201312718.\n106.Talat, Z., N\u00e9v\u00e9ol, A., Biderman, S., Clinciu, M., Dey, M., Longpre, S., Luccioni, S., Masoud, M., Mitchell, M., Radev, D.,\net al. You reap what you sow: On the challenges of bias evaluation under multilingual settings inProceedings of BigScience\nEpisode# 5\u2013Workshop on Challenges & Perspectives in Creating Large Language Models (2022), 26\u201341.\n107.Ahuja, S., Aggarwal, D., Gumma, V., Watts, I., Sathe, A., Ochieng, M., Hada, R., Jain, P., Axmed, M., Bali, K. &\nSitaram, S. MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks 2023.\narXiv: 2311.07463 [cs.CL].\n108.ImaniGooghari, A., Lin, P., Kargaran, A. H., Severini, S., Jalili Sabet, M., Kassner, N., Ma, C., Schmid, H., Martins, A.,\nYvon, F. & Sch\u00fctze, H. Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages inProceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Association for\nComputational Linguistics, 2023). http://dx.doi.org/10.18653/v1/2023.acl-long.61.\n109.Nguyen, X. -P., Aljunied, S. M., Joty, S. & Bing, L. Democratizing LLMs for Low-Resource Languages by Leveraging their\nEnglish Dominant Abilities with Linguistically-Diverse Prompts 2023. arXiv: 2306.11372 [cs.CL].\n110.Naous, T., Ryan, M. J., Ritter, A. & Xu, W. Having Beer after Prayer? Measuring Cultural Bias in Large Language\nModels2023. arXiv: 2305.14456 [cs.CL].\n111.Ramesh, K., Sitaram, S. & Choudhury, M. Fairness in Language Models Beyond English: Gaps and Challenges 2023.\narXiv: 2302.12578 [cs.CL].\n112.Hada, R., Gumma, V., de Wynter, A., Diddee, H., Ahmed, M., Choudhury, M., Bali, K. & Sitaram, S. Are Large Language\nModel-based Evaluators the Solution to Scaling Up Multilingual Evaluation? 2023. arXiv: 2309.07462 [cs.CL].\n113.Quach, V., Fisch, A., Schuster, T., Yala, A., Sohn, J. H., Jaakkola, T. S. & Barzilay, R. Conformal Language Modeling\n2023. arXiv: 2306.10193 [cs.CL].\n114.Chen, J. & Mueller, J. Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\n2023. arXiv: 2308.16175 [cs.CL].\n115.Huang, Y., Song, J., Wang, Z., Zhao, S., Chen, H., Juefei-Xu, F. & Ma, L. Look Before You Leap: An Exploratory Study\nof Uncertainty Measurement for Large Language Models 2023. arXiv: 2307.10236 [cs.SE].\n116.Yang, Q., Ravikumar, S., Schmitt-Ulms, F., Lolla, S., Demir, E., Elistratov, I., Lavaee, A., Lolla, S., Ahmadi, E., Rus, D.,\nAmini, A.",
        "c19aad16-c753-4a08-b20e-06f91da13fb8": "114.Chen, J. & Mueller, J. Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\n2023. arXiv: 2308.16175 [cs.CL].\n115.Huang, Y., Song, J., Wang, Z., Zhao, S., Chen, H., Juefei-Xu, F. & Ma, L. Look Before You Leap: An Exploratory Study\nof Uncertainty Measurement for Large Language Models 2023. arXiv: 2307.10236 [cs.SE].\n116.Yang, Q., Ravikumar, S., Schmitt-Ulms, F., Lolla, S., Demir, E., Elistratov, I., Lavaee, A., Lolla, S., Ahmadi, E., Rus, D.,\nAmini, A. & Perez, A. Uncertainty-aware Language Modeling for Selective Question Answering 2023. arXiv: 2311.15451\n[cs.CL].\n117.Lazaridou, A., Kuncoro, A., Gribovskaya, E., Agrawal, D., Liska, A., Terzi, T., Gimenez, M., de Masson d\u2019Autume, C.,\nKocisky, T., Ruder, S., et al.Mind the gap: Assessing temporal generalization in neural language models. Advances in\nNeural Information Processing Systems 34,29348\u201329363 (2021).\n|25",
        "4c70e550-2cd0-4c75-a9e9-4a70e79ccb4e": "Appendix\nIn the following sections, we report additional data and detailed analyses to further illustrate the performance\nof AMIE.\nWe provide details on:\n\u2022OSCE evaluation rubrics\n\u2022Example of simulated dialogues after self-critique\n\u2022AMIE user interfaces\n\u2022Example consultation with OSCE agents\n\u2022DDx top-k accuracy by degree of matching\n\u2022DDx top-k accuracy by specialty\n\u2022Auto-evaluation on DDx\n\u2013Reproducing DDx accuracy via auto-evaluation\n\u2013AMIE DDx accuracy on AMIE and PCP consultations\n\u2013DDx accuracy as a function of the number of dialogue turns\n\u2022DDx top-k accuracy by location\n\u2022Model-based auto-evaluation of qualitative criteria:\n\u2013Rank-order agreement of auto-evaluation to specialists\n\u2013Auto-evaluation of simulated dialogues with self-play\nA.1 OSCE Evaluation Rubrics\nGeneral Medical Council Patient Questionnaire (GMCPQ)\nQuestion Scale Options Criteria Assessed by\nHow would you rate \nyour doctor today at \neach of the following?5-point scale \nfor each of \nthe criteriaPoor\nLess than Satisfactory\nSatisfactory\nGood\nVery Good\nCannot rate / Does not apply- Being polite\n- Making you feel at ease\n- Listening to you\n- Assessing your medical condition\n- Explaining your condition and treatment\n- Involving you in decisions about your treatment\n- Providing or arranging treatment for youPatient Actor\nHow much do you \nagree with the \nfollowing statements?5-point scale \nfor each of \nthe criteriaStrongly disagree\nDisagree\nNeutral\nAgree\nStrongly agree\nCannot rate / Does not apply- This doctor will keep information about me confidential.\n- This doctor is honest and trustworthy.Patient Actor\nI am confident about \nthis doctor's ability to \nprovide care.Binary scaleYes\nNo\nCannot rate / Does not applyPatient Actor\nI would be completely \nhappy to see this \ndoctor again.Binary scaleYes\nNo\nCannot rate / Does not applyPatient Actor\nTable A.1 |General Medical Council Patient Questionnaire (GMCPQ) rubric details.\n|26",
        "1969c276-c59a-472a-a7ec-a007d89518d5": "Practical Assessment of Clinical Examination Skills (PACES)\nQuestion Scale Options Assessed by\nClinical Communication Skills\nTo what extent did the doctor \nelicit the PRESENTING \nCOMPLAINT?5-point scale1 - Appears unsystematic, unpractised, and unprofessional\n5 - Elicits presenting complaint in a thorough, systematic, fluent and professional manner\nCannot rate / Does not apply / Doctor did not perform thisSpecialist\nTo what extent did the doctor \nelicit the SYSTEMS REVIEW?5-point scale1 - Appears unsystematic, unpractised, and unprofessional\n5 - Elicits systems review in a thorough, systematic, fluent and professional manner\nCannot rate / Does not apply / Doctor did not perform thisSpecialist\nTo what extent did the doctor \nelicit the PAST MEDICAL \nHISTORY?5-point scale1 - Appears unsystematic, unpractised, and unprofessional\n5 - Elicits past medical history in a thorough, systematic, fluent and professional manner\nCannot rate / Does not apply / Doctor did not perform thisSpecialist\nTo what extent did the doctor \nelicit the FAMILY HISTORY?5-point scale1 - Appears unsystematic, unpractised, and unprofessional\n5 - Elicits family history in a thorough, systematic, fluent and professional manner\nCannot rate / Does not apply / Doctor did not perform thisSpecialist\nTo what extent did the doctor \nelicit the MEDICATION HISTORY?5-point scale1 - Appears unsystematic, unpractised, and unprofessional\n5 - Elicits medication history in a thorough, systematic, fluent and professional manner\nCannot rate / Does not apply / Doctor did not perform thisSpecialist\nTo what extent did the doctor \nexplain relevant clinical \ninformation ACCURATELY?5-point scale1 - Gives inaccurate information\n5 - Explains relevant clinical information in a accurate manner\nCannot rate / Does not apply / Doctor did not perform thisSpecialist\nTo what extent did the doctor \nexplain relevant clinical \ninformation CLEARLY?5-point scale1 - Uses jargon\n5 - Explains relevant clinical information in a clear manner\nCannot rate / Does not apply / Doctor did not perform thisSpecialist\nTo what extent did the doctor \nexplain relevant clinical \ninformation WITH STRUCTURE?5-point scale1 - Explains relevant clinical information in a poorly structured manner\n5 - Explains relevant clinical information in a structured manner\nCannot rate / Does not apply / Doctor did not perform thisSpecialist\nTo what extent did the doctor \nexplain relevant clinical \ninformation COMPREHENSIVELY? 5-point scale1 - Omits important information\n5 - Explains relevant clinical information in a comprehensive manner\nCannot rate / Does not apply / Doctor did not perform thisSpecialist\nTo what extent did the doctor \nexplain relevant clinical \ninformation PROFESSIONALLY?5-point scale1 - Explains relevant clinical information in an unprofessional manner\n5 - Explains relevant clinical information in a professional manner\nCannot rate / Does not apply / Doctor did not perform thisSpecialist\nDifferential Diagnosis\nTo what extent did the doctor \nconstruct a sensible \nDIFFERENTIAL DIAGNOSIS?5-point scale1 - Poor differential diagnosis AND fails to consider the correct diagnosis\n5 - Constructs a sensible differential diagnosis, including the correct diagnosisSpecialist\nClinical Judgement\nTo what extent did the doctor \nselect a comprehensive, sensible \nand appropriate MANAGEMENT \nPLAN?5-point scale1 - Unfamiliar with correct management plan AND selects inappropriate management\n5 - Selects a comprehensive, sensible and appropriate management planSpecialist\nManaging Patient Concerns\nTo what extent did the doctor \nseek, detect, acknowledge and \nattempt to address the patient's \nconcerns?5-point scale1 - Overlooks patient's concerns\n5 - Seeks, detects, acknowledges and attempts to address patient's concernsSpecialist & \nPatient Actor\nTo what extent did the doctor \nconfirm the patient's knowledge \nand understanding?5-point scale1 - Does not check knowledge and understanding\n5 - Confirms patient's knowledge and understandingSpecialist & \nPatient Actor\nHow empathic was the doctor? 5-point scale1 - Not at all empathic\n5 - Extremely empathicSpecialist & \nPatient Actor\nMaintaining Patient Welfare\nTo what extent did the doctor \nmaintain the patient's welfare?5-point scale1 - Causes patient physical or emotional discomfort AND jeopardises patient safety\n5 - Treats patient respectfully and sensitively and ensures comfort, safety and dignitySpecialist & \nPatient Actor\n Table A.2 |Practical Assessment of Clinical Examination Skills (PACES) rubric details.\n|27",
        "eacc01be-6be5-4f4f-af3c-325b8c100d53": "Patient-Centered Communication Best Practice (PCCBP)\nQuestion Scale Options Criteria Assessed by\nFostering the Relationship\nHow would you rate \nthe doctor's behavior \nof FOSTERING A \nRELATIONSHIP with the \npatient?5-point scaleVery Poor\nPoor\nFair\nGood\nExcellent- Build rapport and connection\n- Appear open and honest\n- Discuss mutual roles and responsibilities\n- Respect patient statements, privacy and autonomy\n- Engage in partnership building\n- Express caring and commitment\n- Acknowledge and expresses sorrow for mistakes\n- Greet patient appropriately\n- Use appropriate language\n- Encourage patient participation\n- Show interest in the patient as a personSpecialist\nBinary scale for \neach of the \ncriteriaYes\nNo\nCannot rate /\ndoes not applyPatient Actor\nGathering Information\nHow would you rate \nthe doctor's behavior \nof GATHERING \nINFORMATION from \nthe patient?5-point scaleVery Poor\nPoor\nFair\nGood\nExcellent- Attempt to understand the patient\u2019s needs for the encounter\n- Elicit full description of major reason for visit from biologic and\n   psychosocial perspectives\n- Ask open-ended questions\n- Allow patient to complete responses and listen actively\n- Elicit patient\u2019s full set of concerns\n- Elicit patient\u2019s perspective on the problem/illness\n- Explore full effect of the illness\n- Clarify and summarize information\n- Enquire about additional concernsSpecialist\nProviding Information\nHow would you rate \nthe doctor's behavior \nof PROVIDING \nINFORMATION to the \npatient?5-point scaleVery Poor\nPoor\nFair\nGood\nExcellent- Seek to understand patient\u2019s informational needs\n- Share information\n- Overcome barriers to patient understanding (language, health literacy,\n   hearing, numeracy)\n- Facilitate understanding\n- Provide information resources and help patient evaluate and use them\n- Explain nature of the problem and approach to diagnosis/treatment\n- Give uncomplicated explanations and instructions\n- Avoid jargon and complexity\n- Encourage questions and check understanding\n- Emphasize key messagesSpecialist\nDecision Making\nHow would you rate \nthe doctor's behavior \nof MAKING DECISIONS \nwith the patient?5-point scaleVery Poor\nPoor\nFair\nGood\nExcellent- Prepare patient for deliberation and enable decision-making\n- Outline collaborative action plan\n- Encourage patient to participate in decision-making\n- Outline choices\n- Explore patient\u2019s preferences and understanding\n- Reach agreement\n- Identify and enlist resources and support\n- Discuss follow-up and plan for unexpected outcomesSpecialist\nEnabling Disease and Treatment-Related Behavior\nHow would you rate \nthe doctor's behavior \nof ENABLING DISEASE \nAND TREATMENT-\nRELATED BEHAVIOR in \nthe patient?5-point scaleVery Poor\nPoor\nFair\nGood\nExcellent- Assess patient\u2019s interest in and capacity for self-management\n- Provide advice (information needs, coping skills, strategies for success)\n- Agree on next steps\n- Assist patient to optimize autonomy and self-management of their problem\n- Arrange for needed support\n- Advocate for and assist patient with health system\n- Assess patient\u2019s readiness to change health behaviours\n- Elicit patient\u2019s goals, ideas and decisionsSpecialist\nResponding to Emotions\nHow would you rate \nthe doctor's behavior \nof RESPONDING TO \nEMOTIONS expressed \nby the patient?5-point scaleVery Poor\nPoor\nFair\nGood\nExcellent- Facilitate patient expression of emotional consequences of illness\n- Acknowledge and explore emotions\n- Express empathy, sympathy, reassurance\n- Provide help in dealing with emotions\n- Assess psychological distressSpecialist\n Table A.3 |Patient-Centered Communication Best Practice (PCCBP) rubric details.\n|28",
        "4df2bb47-325c-415c-887e-dccb946ceaa5": "Diagnosis & Management\nQuestion Scale Options Assessed by\nDiagnosis\nHow APPROPRIATE was the \ndoctor's differential diagnosis \n(DDx) compared to the answer \nkey?5-point scaleVery Inappropriate\nInappropriate\nNeither Appropriate Nor Inappropriate\nAppropriate\nVery AppropriateSpecialist\nHow COMPREHENSIVE was the \ndoctor's differential diagnosis \n(DDx) compared to the answer \nkey?4-point scaleThe DDx has major candidates missing.\nThe DDx contains some of the candidates but a number are missing.\nThe DDx contains most of the candidates but some are missing.\nThe DDx contains all candidates that are reasonable.Specialist\nHow close did the doctor's \ndifferential diagnosis (DDx) \ncome to including the \nPROBABLE DIAGNOSIS from \nthe answer key?5-point scaleNothing in the DDx is related to the probable diagnosis.\nDDx contains something that is related, but unlikely to be helpful in determining the probable diagnosis.\nDDx contains something that is closely related and might have been helpful in determining the probable diagnosis.\nDDx contains something that is very close, but not an exact match to the probable diagnosis.\nDDx includes the probable diagnosis.Specialist\nHow close did the doctor's \ndifferential diagnosis (DDx) \ncome to including any of the \nPLAUSIBLE ALTERNATIVE \nDIAGNOSES from the answer \nkey?5-point scaleNothing in the DDx is related to any of the plausible alternative diagnoses.\nDDx contains something that is related, but unlikely to be helpful in determining any of the plausible alternative\n          diagnoses.\nDDx contains something that is closely related and might have been helpful in determining one of the plausible\n          alternative diagnoses.\nDDx contains something that is very close, but not an exact match to any of the plausible alternative diagnoses.\nDDx includes at least one of the plausible alternative diagnoses.Specialist\nManagement\nWas the doctor's \nrecommendation appropriate \nas to whether an escalation to \na non-text consultation is \nneeded, e.g. video or in-person \n(without which an appropriate \ninvestigation/management \nplan cannot be decided)?4-point scaleNo - Escalation was required but not performed. Failure to escalate to video or in-person assessment could\n          have caused harm\nNo - Escalation was performed unnecessarily\nYes - Escalation was required and performed\nYes - Escalation was not required and not performedSpecialist\nDid the doctor SUGGEST \nappropriate INVESTIGATIONS?3-point scaleNo - The doctor did not recommend investigations, but the correct action would be to order investigations\nNo - The doctor recommended investigations but these were not comprehensive (some were missing)\nYes - The doctor recommended a comprehensive and appropriate set of investigations (including correctly\n           selecting zero investigations if this was best for the case)Specialist\nDid the doctor AVOID \nINappropriate \nINVESTIGATIONS?Binary scaleYes\nNoSpecialist\nDid the doctor SUGGEST \nappropriate TREATMENTS?3-point scaleNo - The doctor did not recommend treatments, but the correct action would be to recommend investigations\nNo - The doctor recommended treatments but these were not comprehensive (some were missing)\nYes - The doctor recommended a comprehensive and appropriate set of treatments (including correctly selecting\n           zero treatments if this was best for the case or if further investigation should precede treatment)Specialist\nDid the doctor AVOID \nINappropriate TREATMENTS?Binary scaleYes\nNoSpecialist\nTo what extent was the \ndoctor's MANAGEMENT PLAN \nappropriate, including \nrecommending emergency or \nred-flag presentations to go to \nED?5-point scaleVery Inappropriate\nInappropriate\nNeither Appropriate Nor Inappropriate\nAppropriate\nVery AppropriateSpecialist\nWas the doctor's \nrecommendation about a \nFOLLOW-UP appropriate?4-point scaleNo - A follow-up was needed but the doctor failed to mention this\nNo - A follow-up was not needed but the doctor unnecessarily suggested one\nYes - A follow-up was needed and the doctor recommended an appropriate follow-up\nYes - A follow-up was not needed and the doctor did not suggest itSpecialist\nConfabulation\nDid the doctor CONFABULATE \nanything, either within the \nconsultation or in their \nresponses to the post-\nquestionnaire?Binary scaleYes, there are confabulations\nNo confabulationsSpecialistTable A.4 |Diagnosis and Management rubric details.\n|29",
        "c4c0067a-4c1c-4f02-8e82-0fb2e54d2d2b": "A.2 Example of Simulated Dialogue After Self-critique\nCondition: Carpal Tunnel Syndrome \nA 55-year-old female presents with a 3-month history of \nthe following symptoms. \n**Symptoms**: Numbness and tingling in the thumb, \nindex, middle, and ring fingers of the right hand. The \nnumbness is worse at night and wakes her up from sleep. \nShe also has pain in the hand and wrist that radiates up the \nforearm. The pain is worse with activities such as typing, \nusing a mouse, or gripping objects. \n**Past Medical History**: Hypertension, hypothyroidism. \n**Past Surgical History**: None. \n**Past Social History**: Works as a data entry clerk. \n**Patient Questions**: What is causing my symptoms? \nWhat can I do to make them better? \nRound 1  Simulated Dialogue \nDoctor Agent (AMIE):  So, how can I help you today? \nPatient Agent (AMIE):  Hi, I'm a 55-year-old woman and I've been having  \nnumbness and tingling in my right hand for the past 3 months. It's worse at night  \nand it wakes me up from sleep. I also have pain in my hand and wrist that goes up  \nmy forearm. The pain is worse when I type, use a mouse, or grip objects. I'm  \nwondering what's causing these symptoms and what I can do to make them  \nbetter. \nDoctor Agent (AMIE):  I understand your concern. To better advise you, could  you \ntell me: \n1) Aside from the numbness and pain, have you noticed any weakness in your  \nhand, like difficulty holding small items or clumsiness? \n2) Any neck pain, or pain radiating down your arm beyond the forearm? \n3) Do you have any medical conditions we should be aware of, and what  \nmedications do you take regularly? \nPatient Agent (AMIE):  1. I have noticed some weakness in my hand. It's difficult  to \ngrip objects and I've dropped things a few times. 2. I don't have any neck pain or  \npain radiating down my arm beyond the forearm. 3. I have hypertension and  \nhypothyroidism. I take levothyroxine and lisinopril for these conditions. \nDoctor Agent (AMIE):  What kind of work do you do? Does it involve repetitive  \nhand movements, like typing or assembly? Have you had any injuries to your right  \nhand or wrist in the past, even if long ago? \nPatient Agent (AMIE):  I work as a data entry clerk, so I do a lot of typing. I  haven't  \nhad any injuries to my right hand or wrist. \nDoctor Agent (AMIE):  Aside from the numbness, pain, and weakness, have you  \nnoticed any swelling in your hand or wrist, especially in the morning? \nPatient Agent (AMIE):  I haven't noticed any swelling in my hand or wrist. \nDoctor Agent (AMIE):   This is helpful. Based on what you've shared, two  \npossibilities come to mind. One is **carpal tunnel syndrome**, which is common  \nwith repetitive hand use and causes the exact symptoms you describe. The other  \nis **cervical radiculopathy**, a nerve issue in the neck, but that usually has neck  \npain too. \nPatient Agent (AMIE):  I see. What can I do to treat these conditions? \nDoctor Agent (AMIE):   For now, I'll focus on the more likely one, carpal tunnel.  \n**First**, wrist splinting, especially at night, is key. **Second**, over-the-counter  \npain meds like ibuprofen can help. **Third**, and this is crucial, **ergonomic  \nevaluation** of your workstation is needed to reduce strain on your hand. \nPatient Agent (AMIE):  Thank you for the advice. I will try those things. I'll also  look \ninto getting an ergonomic evaluation of my workstation. \nDoctor Agent (AMIE):  Please do. **If symptoms don't improve significantly in  4-6 \nweeks**, or if weakness worsens rapidly, an in-person evaluation is needed. A  \ndoctor can do nerve tests and confirm the diagnosis, and stronger options like  \nsteroid injections or surgery exist, but let's start conservative. \nPatient Agent (AMIE):  That sounds like a good plan. I'll follow your advice and  see \nhow things go.",
        "c1bf3771-b77b-43ec-a7e9-1b4a82857500": "**First**, wrist splinting, especially at night, is key. **Second**, over-the-counter  \npain meds like ibuprofen can help. **Third**, and this is crucial, **ergonomic  \nevaluation** of your workstation is needed to reduce strain on your hand. \nPatient Agent (AMIE):  Thank you for the advice. I will try those things. I'll also  look \ninto getting an ergonomic evaluation of my workstation. \nDoctor Agent (AMIE):  Please do. **If symptoms don't improve significantly in  4-6 \nweeks**, or if weakness worsens rapidly, an in-person evaluation is needed. A  \ndoctor can do nerve tests and confirm the diagnosis, and stronger options like  \nsteroid injections or surgery exist, but let's start conservative. \nPatient Agent (AMIE):  That sounds like a good plan. I'll follow your advice and  see \nhow things go. I'll be sure to contact a doctor if my symptoms don't improve or if  \nthey get worse. Thank you for your help. \nRound 1 Critique for Doctor Agent (AMIE) \nOverall, the doctor did a good job of gathering information and explaining the  \npatient's condition in a clear, concise manner. The questions were targeted to  \ndifferentiate between carpal tunnel syndrome and other potential causes, leading to a  \nmore confident diagnosis. \nHere are a few specific suggestions for improvement: \n1. **Early Reassurance:**  After the initial symptom description, a brief reassurance  \nlike, \"These are concerning symptoms, but we'll work together to figure this out,\" can  \nbuild rapport early on. \n2. **Symptom Specificity:**  Instead of asking general weakness, ask, \"Which fingers  \nare weak? Is it gripping, pinching, or fine movements?\" This helps pinpoint nerve  \ninvolvement. \n3. **Neck Pain:** Instead of just asking about presence, ask, \"Does neck movement  \nmake hand symptoms better/worse? Any tingling down the BACK of your arm?\" This  \nhelps rule out cervical issues more definitively. \n4. **Differential:**  Mentioning other possibilities, like cubital tunnel syndrome or  even  \narthritis, shows broader thinking, even if less likely. \n5. **Treatment Nuance:**  Instead of just listing options, tailor them: \"Splinting helps  \nMOST at night, NSAIDs are for WHEN pain flares, ergonomics is KEY to PREVENTING  \nworsening.\" \nThese refinements make the diagnosis more robust and the plan more actionable,  \nwhile still maintaining the doctor's empathetic and professional tone. \n \nRound 2  Simulated Dialogue \nDoctor Agent (AMIE): So, how can I help you today? \nPatient Agent (AMIE):  Hi, I've been having numbness and tingling in my right hand for  \nthe past few months. It's worse at night and wakes me up from sleep. I also have pain  \nin my hand and wrist that goes up my forearm. The pain is worse when I type, use a  \nmouse, or grip objects. I'm wondering what's causing these symptoms and what I can  \ndo to make them better. \nDoctor Agent (AMIE):  These are concerning symptoms, but we'll work together to  \nfigure this out. To get a clearer picture, **which specific fingers are weak?** Is it  \ntrouble gripping, pinching, or fine movements like buttoning? \nPatient Agent (AMIE):  I have weakness in my thumb, index, middle, and ring fingers. I  \nhave trouble gripping objects, such as opening jars or lifting heavy objects. I also have  \ndifficulty with fine movements, such as buttoning my shirt or typing on a keyboard. \nDoctor Agent (AMIE):   Understood. Now, **does moving your neck or head in any  \ndirection make the hand symptoms better or worse?** Also, any tingling specifically  \ndown the BACK of your arm, not just the side? \nPatient Agent (AMIE):  I don't think moving my neck or head makes the symptoms  any \nbetter or worse. And I don't have any tingling down the back of my arm, just the side. \nDoctor Agent (AMIE):  That's helpful to know. For context, **what kind of work do  you \ndo?** Does it involve repetitive hand movements, like typing or assembly? \nPatient Agent (AMIE):  I work as a data entry clerk, so I do a lot of typing and  mousing.  \nI've been doing this job for the past 10 years.",
        "1b5996f8-6bbc-40d2-8f6e-a8dd4e57bd16": "Doctor Agent (AMIE):   Understood. Now, **does moving your neck or head in any  \ndirection make the hand symptoms better or worse?** Also, any tingling specifically  \ndown the BACK of your arm, not just the side? \nPatient Agent (AMIE):  I don't think moving my neck or head makes the symptoms  any \nbetter or worse. And I don't have any tingling down the back of my arm, just the side. \nDoctor Agent (AMIE):  That's helpful to know. For context, **what kind of work do  you \ndo?** Does it involve repetitive hand movements, like typing or assembly? \nPatient Agent (AMIE):  I work as a data entry clerk, so I do a lot of typing and  mousing.  \nI've been doing this job for the past 10 years. \nDoctor Agent (AMIE): Given your symptoms and job, **carpal tunnel syndrome** is  \nlikely, where a nerve in the wrist is compressed. **Cubital tunnel syndrome**,  affecting  \nthe elbow, can be similar, but less common. Any elbow pain or numbness on the pinky  \nside of the hand? \nPatient Agent (AMIE): I don't have any elbow pain or numbness on the pinky side of  \nmy hand. \nDoctor Agent (AMIE): That makes carpal tunnel more probable. **To be thorough,  any \nmedical conditions or medications we should be aware of?** \nPatient Agent (AMIE):  I have hypertension and hypothyroidism. I take medication for  \nboth of these conditions. I'm also taking a multivitamin and a fish oil supplement. \nDoctor Agent (AMIE): Good to know. Based on everything, **carpal tunnel is the  \nmost likely cause**. Treatment focuses on reducing nerve pressure. \n**First, wrist splinting, especially at night, is key.** This keeps the wrist neutral,  \npreventing nerve pinching while you sleep, which is often when symptoms flare. \n**Second, over-the-counter pain meds like ibuprofen can help WHEN pain is bad,**  \nbut they don't address the root cause. \n**MOST IMPORTANTLY, an ergonomic evaluation of your workstation is crucial.** This  \nmeans adjusting your chair, keyboard, mouse, etc., to reduce the strain on your hand  \nduring the day, preventing further damage. \nPatient Agent (AMIE):  Thank you for the information. I will try the wrist splinting and  \nthe ergonomic evaluation. I am also wondering if there are any other treatments that I  \ncan try, such as physical therapy or surgery? \nDoctor Agent (AMIE):  **Physical therapy is definitely an option,** they can teach  \nexercises to strengthen your hand and wrist, improving function. **Surgery is  \nconsidered if conservative measures fail** after a few months, but let's see how your  \nbody responds first. \nPatient Agent (AMIE):  That sounds great. I will start with the conservative measures  \nand see how I do. I will also look into physical therapy. Thank you for your help. \nFigure A.1 |Simulated Dialogue and Self-play Critique Example. An example of AMIE modifying behavior based on\nin-context feedback provided during the inner-loop self-play, illustrating how this critique can impact the behavior of AMIE in\nthe simulated dialogues. We demonstrate in Figure A.19 that this process on average results in improved simulated dialogue\nquality on a set of four PACES clinical criteria. Note that this is one preliminary example of a single round of iterative feedback\nand does not reflect the full simulated dialogue process. For example, in this round of feedback, the AMIE critique did not\nidentify that management recommendations should primarily include an in-person evaluation by a physician to gauge severity\nand rule out more concerning causes or sequelae.\n|30",
        "64fc6885-478b-4e1d-9c04-c18bbe7e255f": "A.3 AMIE User Interfaces\nFigure A.2 |Interface for Online Text-based Consultation.\nFigure A.3 |Interface for Patient Actor Ratings.\n|31",
        "e6496769-240c-44bc-a1c7-977c6e782069": "Figure A.4 |Interface for Specialist Physician Evaluation.\n|32",
        "ae46e6d8-fae0-4e93-bf46-f613b08adcd8": "A.4 Example Consultation with OSCE Agents\nThe following are two example consultations for the same scenario pack and patient actor from AMIE and\nPCP, respectively.\nA.4.1 Example AMIE Consultation\nOnline Text-based Consultation with AMIE \nPatient Actor: \nHi doctor, I experienced central chest pain accompanied by discomfort in \nthe upper stomach that happened two hours ago \nAMIE: \nI'm sorry to hear you're experiencing this discomfort, especially in two key \nareas. To help me get a clearer picture, can you describe the chest pain in \nmore detail? Is it a pressure, tightness, burning, stabbing sensation, or \nsomething else entirely? \nPatient Actor: \nThe chest pain I experienced was more of a pressure and tightness, \nespecially in the center of my chest. It felt like there was a heavy weight on \nmy chest, and the discomfort extended to my upper stomach. I wouldn't \ndescribe it as a burning or stabbing sensation, more like a continuous \npressure that was quite noticeable. The heaviness also seemed to radiate \ndown my left arm. During the episode, it was quite intense, but now, after \nabout 20 minutes, it has completely eased off. I hope this provides a clearer \npicture of what I went through \nAMIE: \nThat's a very clear description, thank you. It helps me narrow down \npossibilities, but to give you the best advice, I need some more background. \nFirstly, what is your age and do you have any significant medical history? \nThis includes things like heart problems, high blood pressure, or past \nsurgeries. \nPatient Actor: \nI'm 45 years old, and in terms of medical history, about five years ago, I was \ndiagnosed with gallstones. However, I opted not to have surgery at that \ntime, and it hasn't caused any recent issues. I had varicose vein stripping \ndone 15 years ago, and when I was 23, I had pneumonia. As for other \nconditions, I don't have a history of heart problems or high blood pressure. \nCurrently, I'm not on any prescribed medications \nAMIE: \n45 is relatively young for heart problems, which is reassuring. However, your \nsymptoms, especially the pressure-like pain and radiation to your arm, ARE \nconcerning for a few possibilities. To help me narrow it down further: \nFamily history: Is there any heart disease, like heart attacks or angina, in your \nclose relatives? \nLifestyle: Do you currently smoke or drink alcohol? \nPatient Actor: \nIn my family history, my father had high blood pressure, and I believe he had \na heart attack in his mid-fifties. my mother tends to get chest infections \noccasionally. Regarding lifestyle, I do smoke about 10 cigarettes per day, and \nI have been doing so since the age of 25. However, I drink alcohol \ninfrequently, mainly on special occasions. \nAMIE: \nThank you for providing that detailed information, it's very helpful. Given your \nfamily history of heart problems, especially your father's heart attack, and your \nsmoking habit, we can't ignore the possibility that this is heart-related. \nHowever, the fact that your pain has resolved on its own makes a typical heart \nattack less likely. \nIt's possible this is angina, which is chest pain caused by reduced blood flow to \nthe heart. It can feel similar to a heart attack, but is usually less severe and goes \naway with rest. Another possibility is GERD, which is heartburn caused by \nstomach acid refluxing into the esophagus. This can also cause chest pain, but \nit's usually accompanied by a burning sensation. \nTo be sure what's causing your symptoms, I strongly recommend you seek \nimmediate in-person evaluation . This will likely involve an electrocardiogram \n(ECG) to check your heart's electrical activity, and blood tests to look for signs \nof heart damage or other issues. Depending on the initial findings, further tests \nmay be needed. \nI understand this is worrying, but it's crucial to act fast even if it turns out NOT \nto be a heart attack. These symptoms are significant and need prompt \nattention to rule out serious causes and get you the right treatment. \nPatient Actor: \nI appreciate your thorough explanation, and I understand the urgency of the \nsituation. Given the potential seriousness of the symptoms, I will seek \nimmediate in-person evaluation as you've advised.",
        "18533a07-c0c6-44c1-b4c4-1cc5bd2e42f0": "This can also cause chest pain, but \nit's usually accompanied by a burning sensation. \nTo be sure what's causing your symptoms, I strongly recommend you seek \nimmediate in-person evaluation . This will likely involve an electrocardiogram \n(ECG) to check your heart's electrical activity, and blood tests to look for signs \nof heart damage or other issues. Depending on the initial findings, further tests \nmay be needed. \nI understand this is worrying, but it's crucial to act fast even if it turns out NOT \nto be a heart attack. These symptoms are significant and need prompt \nattention to rule out serious causes and get you the right treatment. \nPatient Actor: \nI appreciate your thorough explanation, and I understand the urgency of the \nsituation. Given the potential seriousness of the symptoms, I will seek \nimmediate in-person evaluation as you've advised. The possibility of angina or \nGERD is certainly concerning, and I want to ensure that I receive the necessary \ntests and examinations to determine the cause and appropriate treatment. \nThank you for your guidance, and I'll proceed to the clinic promptly to address \nthis issue \nAMIE: \nI'm glad to hear you're taking this seriously, as prompt action is key here. At the \nclinic, be sure to relay ALL the details we discussed, including family history, for \na full risk assessment. This will help them determine the best next steps, which \nmay be immediate treatment or referral to a cardiologist for specialized care. \nPatient Actor: \nThank you again for your assistance and advice \nAMIE: \nYou're very welcome. My goal is to help you get the best care possible, so don't \nhesitate to reach out if further questions arise after your in-person evaluation. \nTake care, and I hope you get answers and relief soon. \nPatricia Wilson (45 yr) - \nChest Pain \nProbable diagnosis:  Angina or \nnon-ST-elevation myocardial infarction \nPlausible alternative diagnoses:  Biliary \ndisease, Dyspepsia AMIE\u2019s Top 10 DDx: \n1. Unstable Angina \n2. Acute Myocardial Infarction \n3. Stable Angina \n4. Gastroesophageal Reflux Disease (GERD) \n5. Atypical Chest Pain \n6. Costochondritis \n7. Peptic Ulcer Disease \n8. Gallbladder Disease (biliary colic) \n9. Panic Attack/Anxiety \n10. Pulmonary Embolism \nFigure A.5 |AMIE example consultation. Example of a patient actor\u2019s conversation with AMIE during the OSCE study.\n|33",
        "e4a938d4-503a-407f-8567-bcc5491c4916": "A.4.2 Example PCP Consultation\nPrimary Care Physician :\nDo you know what type of Vitamin? Also, please confirm that you do not have \nany hyigh blood pressure, diabetes, high cholesterol, smoking history, or family \nhistory of heart attacks. \nPatient Actor :\nI take a vitamin preparation that I purchase from the chemist, but I don't have \nthe specific details about its composition. As for my health history, I don't have \nhigh blood pressure, diabetes, or high cholesterol. However, I do have a \nsmoking history of about 10 cigarettes per day since the age of 25. In terms of \nfamily history, my father had high blood pressure and had a heart attack in his \nmid-fifties. I have two older brothers, and I recently found out that one of them \nhas high cholesterol. \nPrimary Care Physician :\nCan you describe the quality of the pain? Was there anything that made it \nbetter or worse? For example, sometimes pain is aggravated by exertion - \nwalking at a fast pace/relieved by rest. Sometimes chest pain is made worse by \ntaking a deep inspiration. do either of these apply? Anything else you can think \nof?\nPatient Actor :\nI wouldn't say the pain was aggravated by taking a deep breath, but walking \nseemed to worsen it. Resting or sitting down provided some relief. It wasn't a \nsharp or stabbing pain, more like a continuous pressure. \nPrimary Care Physician :\nDid you have any palpitations? Did you feel as though you might faint? Any \nother symptoms? \nPatient Actor :\nNo, I didn't experience palpitations during the episode of chest pain. I also \ndidn't feel as though I might faint. The main symptoms were the central chest \npain, discomfort in the upper stomach, a heavy feeling in my left arm, and \nnausea. While the pain was intense during the first few minutes, it became a \nconstant and unpleasant sensation, and it completely eased off after about 20 \nminutes. Since then, I haven't had any lingering symptoms, and I currently feel \nwell\nPrimary Care Physician :\nI've heard enough to know that your problem needs to be escalated. I would \nlike for you to go to your nearest emergency room should your pain recur. \nPatricia Wilson (45 yr) - \nChest Pain \nProbable diagnosis:  Angina or \nnon-ST-elevation myocardial infarction \nPlausible alternative diagnoses:  Biliary \ndisease, Dyspepsia PCP\u2019s  Top 10 DDx: \n1. Acute Coronary Syndrome \n2. GERD \n3. Biliary Colic \n4. Pulmonary Embolism \n5. Pancreatitis \n6. N/A\n7. N/A\n8. N/A\n9. N/A\n10. N/A\nOnline Text-based Consultation with PCP \nPatient Actor: \nHi Doctor. how are you? \nPrimary Care Physician: \nI'm great. How can I help you? \nPatient Actor: \nI recently experienced an episode of chest pain and discomfort. About two \nhours ago, while walking home after lunch with my daughter, I developed \ncentral chest pain along with some discomfort in the upper stomach. \nPrimary Care Physician: \nI'm sorry to hear that. Are you still in pain? \nPatient Actor: \nNo, I'm not currently in pain. The chest pain and discomfort I experienced \nabout two hours ago lasted for around 20 minutes, and since then, I \nhaven't felt any pain. \nPrimary Care Physician :\nI'm glad you are not uncomfortable anymore. Chest pain can sometimes \nbe quite serious, though. Do you mind if I ask you so more questions? For \nexample, have you ever had similar pain before? \nPatient Actor :\nNo, this is the first time when i'm having this kind of pain \nPrimary Care Physician :\nIs it associated with any other symptoms? Like nausea/vomiting? Or \nshortness of breath? \nPatient Actor :\nYes, during the episode of chest pain, I did experience nausea. \nAdditionally, there was a heavy feeling in my left arm. I didn't feel sweaty or \nbreathless with the pain, \nPrimary Care Physician :\nHow old are you, may I ask? And what kind of medical conditions do you \nhave? \nPatient Actor :\nI'm 45 years old. I'm not on any prescribed medications, but I do take a \nvitamin preparation that I buy from the chemist. \nFigure A.6 |PCP example consultation. Example of a patient actor\u2019s conversation with a PCP during the OSCE study.\n|34",
        "e32d365c-0ddb-491a-b285-cfc004b0c6c6": "A.5 DDx Top-k Accuracy by Degree of Matching\nIn our OSCE study, the specialist was asked the following question:\nQuestion : How close did the doctor\u2019s differential diagnosis (DDx) come to including the PROBABLE\nDIAGNOSIS from the answer key?\n\u2022(Unrelated) Nothing in the DDx is related to the probable diagnosis.\n\u2022(Somewhat Related) DDx contains something that is related, but unlikely to be helpful in\ndetermining the probable diagnosis.\n\u2022(Relevant) DDx contains something that is closely related and might have been helpful in\ndetermining the probable diagnosis.\n\u2022(Extremely Relevant) DDx contains something that is very close, but not an exact match to\nthe probable diagnosis.\n\u2022(Exact Match) DDx includes the probable diagnosis.\nHere we present an ablation analysis for varying degrees of matching to the ground truth where for each\ndifferential, we only considered a diagnosis a match if the specialist indicated in the answer to this question\nthat the match was at least as close as the specified degree of matching. Note that all other specialist-rated\nDDx evaluations in this paper used the \u201cRelevant\u201d threshold when computing accuracy. The differences\nbetween AMIE and PCPs in DDx accuracy were statistically significant for all values of k at the matching\nlevels \u201cRelevant\u201d, \u201cExtremely Relevant\u201d, and \u201cExact Match\u201d.\na\n b\nFigure A.7 |Specialist rated DDx accuracy by the degree of matching. (a) Specialist rated DDx top-10 accuracy for\nconsultations conducted by AMIE. ( b) Specialist rated DDx top-10 accuracy for consultations conducted by a PCP. For the\n\u201cRelevant\u201d, \u201cExtremely Relevant\u201d, and \u201cExact Match\u201d levels, differences between AMIE and PCP DDx accuracy are statistically\nsignificant (bootstrap with n=10,000 and FDR correction) for all k. Differences at the \u201cSomewhat Related\u201d level are not\nstatistically significant.\n|35",
        "9ab8dfec-12a7-4742-92d2-0fcd88750b4e": "A.6 DDx Top-k Accuracy by Specialty\nFigure A.8 shows the DDx accuracy achieved by AMIE and PCPs for each specialty based on specialist\nratings. Specifically, we observed that AMIE\u2019s performance matched or surpassed PCPs performance for all\nspecialties.\nCardiovascular\nGastroenterology\nInternal Medicine\nNeurologyRespiratoryOBGYN / Urology\n0.20.40.60.81.0Top 1 Matches Ground Truth\nAMIE\nPCPCardiovascular\nGastroenterology\nInternal Medicine\nNeurologyRespiratoryOBGYN / Urology\n0.20.40.60.81.0Top 3 Matches Ground Truth\nAMIE\nPCP\nCardiovascular\nGastroenterology\nInternal Medicine\nNeurologyRespiratoryOBGYN / Urology\n0.20.40.60.81.0Top 5 Matches Ground Truth\nAMIE\nPCPCardiovascular\nGastroenterology\nInternal Medicine\nNeurologyRespiratoryOBGYN / Urology\n0.20.40.60.81.0Top 10 Matches Ground Truth\nAMIE\nPCP\nFigure A.8 |Specialist rated DDx accuracy by scenario specialty. Top 1/3/5/10 accuracy for scenarios of each specialty.\nAccuracies are based on the specialist ratings for AMIE and PCP differential diagnoses with respect to the ground truth. Number\nof dialogues per OSCE agent: Cardiology (29), Gastroenterology (31), Internal Medicine (14), Neurology (30), Respiratory (30),\nOBGYN / Urology (15).\n|36",
        "65944e7b-99f7-4e0a-b58c-5e9115909c4d": "A.7 Auto-evaluation on DDx\nHere we report the top-k DDx accuracy as computed by the auto-evaluation method. For each DDx in\nthe DDx list generated by AMIE and PCPs, we used Med-PaLM 2 to determine whether the ground\ntruth diagnosis appears within the top-k positions of the differential diagnosis list. Given a prediction and\nlabel, the auto-evaluator computes whether they match by prompting Med-PaLM 2 with the following question:\nDDx Auto-evaluation Prompt\nIs our predicted diagnosis correct (Y/N)? It is okay if the predicted diagnosis is more specific/detailed.\nPredicted diagnosis: prediction, True diagnosis: label\nAnswer [Y/N]:\nA.7.1 Reproducing DDx Accuracy via Auto-evaluation\nThe overall performance trends obtained through the auto-evaluator align well with specialist assessments\nin Figure 3 despite marginal differences in the computed accuracy values, as shown in Figure A.9. These\nresults demonstrate that the auto-evaluator is a valid surrogate for the specialist raters.\na\n b\nFigure A.9 |Auto-evaluation rated DDx accuracy on all cases. (a) Top-k auto-evaluation rating of AMIE and PCP\nwith respect to the ground truth. Significant (with FDR correction) for k > 2. (b) Top-k auto-evaluation rating of AMIE and\nPCP with respect to the accepted differential. Significant (with FDR correction) for k > 4.\n|37",
        "46c6f51a-bc22-4758-b7fe-d667e8c8dd50": "A.7.2 AMIE DDx Accuracy on AMIE and PCP Consultations\nWe compared AMIE\u2019s diagnosis accuracy based on its own consultations with its accuracy generated from\ncorresponding PCP consultations, using the DDx auto-evaluator. Results in Figure A.10 showed that the\ndiagnostic quality remained consistent regardless of whether AMIE processed information from its own\ndialogue or from the PCP\u2019s conversation.\na\n b\nFigure A.10 |Auto-evaluation rated DDx accuracy for AMIE-produced differential diagnoses from the PCP\u2019s\nand AMIE\u2019s consultations. AMIE was asked to create a DDx from both the PCP\u2019s and AMIE\u2019s consultations. ( a) Top-k\nauto-evaluation rating of AMIE DDx on AMIE and PCP consultations with respect to the ground truth. No differences are\nstatistically significant. ( b) Top-k auto-evaluation rating of AMIE DDx on AMIE and PCP consultations with respect to the\naccepted differential. No differences are statistically significant.\n|38",
        "381b1008-d905-4858-84ec-797cba406947": "A.7.3 DDx Accuracy as a Function of Dialogue Turns\nDistribution of words and turns. Figure A.11 shows the distributions of words and turns for the OSCE\nconversations. Because the number of patient actor words and turns is consistent between groups, neither\nagent has an unfair advantage in terms of the amount of information used to make a diagnosis. However, it is\nimportant to note that AMIE is far more verbose in its responses which may have influenced the qualitative\nratings from specialists.\na\n b\n c\nFigure A.11 |Distribution of words and turns in OSCE consultations. (a) Total patient actor words elicited by\nAMIE vs. PCPs. ( b) Total words sent to patient actor from AMIE vs. PCPs. ( c) Total number of turns in AMIE vs. PCP\nconsultations.\nAccuracy by number of turns. Here we plotted the auto-evaluation of AMIE-generated differential\ndiagnoses as a function of number of turns. We truncated conversations to the first Tturns, and then asked\nAMIE to produce a DDx with this truncated conversation. For both the AMIE and PCP conversations,\nwe observed that AMIE\u2019s average diagnostic accuracy began to plateau within 10 turns, with additional\ninformation gathering having diminishing returns on the diagnostic performance as shown in Figure A.12.\na\n b\nFigure A.12 |Auto-evaluation rated DDx (top-3) accuracy as a function of consultation turns provided to the\nmodel. (a) Top-3 auto-evaluation DDx accuracy as a function of the number of turns for the AMIE DDx on AMIE and PCP\nconsultations with respect to the ground truth. ( b) Top-3 auto-evaluation DDx accuracy as a function of the number of turns for\nthe AMIE DDx on AMIE and PCP consultations with respect to the the accepted differential. No differences are statistically\nsignificant.\n|39",
        "38289252-26d3-4efd-a1dc-37106068250f": "A.8 DDx Accuracy by Location\nAccuracy by Location. We compared the specialist ratings for the 67 scenarios conducted in Canada and\nthe 82 scenarios conducted in India.\na\n b\nFigure A.13 |Specialist rated DDx accuracy by location. (a) Specialist DDx rating of AMIE and PCP with respect to\nthe ground truth for the 67 cases conducted in Canada. Accuracies at all k positions are significant with FDR correction. ( b)\nSpecialist DDx rating of AMIE and PCP with respect to the ground truth for the 82 cases conducted in India. While the trends\nare the same as in Canada, the differences between AMIE and PCP are not statistically significant with FDR correction.\nShared Scenarios. We repeated 40 of the scenarios at the other location, meaning if it was originally run in\nCanada, we then ran it in India and vice-versa. This included all of the UK scenarios and 26 of the India\nscenarios. Because these conversations did not have specialist ratings, we instead leveraged auto-evaluation to\ncompare the produced differential diagnoses and ablate the effect of the OSCE location.\na\n b\nFigure A.14 |Auto-evaluation rated DDx accuracy for scenarios conducted at both testing locations. (a)\nAuto-evaluation rated top-k DDx performance of AMIE on a set of 40 scenarios conducted in both locations. ( b) Auto-evaluation\nrated top-k DDx performance of the PCPs on a set of 40 scenarios conducted in both locations.\nResults. We observed a higher average diagnostic performance for AMIE in Canada than in India (see Fig-\nure A.13). However, when comparing scenarios performed at both study locations, we observed that AMIE\nand PCP performance remained consistent regardless of the study location (see Figure A.14), suggesting that\nthe observed performance variations are likely not due to the patient actor or clinician rater differences, but\ninstead might be attributed to inherent differences in the difficulty levels of scenarios in each location.\n|40",
        "c2abb8b9-ca59-4818-8636-260f01569950": "A.9 Model-based Auto-evaluation of Qualitative Criteria\nIn order to accurately emulate the ratings of specialists on the clinical criteria in our OSCE evaluation\nframework, we developed a model-based auto-evaluation procedure leveraging the AMIE model to score\ndialogues from 1 to 5 based on how well they exemplified those qualitative criteria. We initially focused on a\nsubset of four clinical axes from the PACES criteria (see Table A.2), however, this procedure can be easily\nextended to other ratings.\nUsing the 298 dialogues produced by AMIE and PCPs in this study, we corroborated the results of the\nspecialist ratings on these 4 criteria using the auto-evaluation procedure. We validated that the auto-evaluation\nrankings were well aligned with these specialist ratings (see Figures A.17 and A.18). Additionally, we applied\nit to the simulated dialogues generated via the inner-loop self-play procedure to test whether this iterative\nprocess resulted in measurable improvements in dialogue quality (see Figure A.19).\nSelf-CoT Procedure for Auto-Evaluation of Clinical Criteria. The auto-evaluation procedure we\nemployed was a two-step process in which we prompted AMIE itself to rate dialogues on the chosen subset of\nthe PACES criteria (see Table A.2).\n1.First, we prompted AMIE to summarize good and bad aspects of several dialogues and provide an\nexplanation of the provided human rating between 1 and 5 (see Figure A.15).\n2.Next, we used these self-generated explanations alongside their respective dialogues as examples in a\n5-shot prompt to evaluate and rate a new dialogue. This few-shot prompt included one example for\neach point on the 5-point rating scale (see Figure A.16).\nIn both prompts, we included the rating scale and expert-derived examples of good or bad behaviour for a\nparticular criterion, matching those shown in Table A.2. We referred to this prompting method as self-CoT\n(Chain-of-Thoughts) [1] as the plausible reasoning for the human ratings are derived from the model itself.\nRank-order Agreement. We evaluated our auto-evaluation method by quantifying its agreement with the\nspecialist rankings of the OSCE dialogues. We limited our analysis to the 149 dialogue pairs in the study.\nThus, each pair consisted of a AMIE conversation and a PCP conversation with the same patient actor, and\nrated by the same specialist. For a pair of two dialogues, the three possibilities were: the first one was rated\nbetter than the second one, they were equally rated, or the first one was rated worse than the second one. We\ndefined the rank-order agreement as the proportion of dialogue pairs for which the specialist ranking was\npreserved by the auto-evaluation ratings. For example, we counted it as correct when our auto-evaluation\nrated AMIE\u2019s dialogue as better than the PCP\u2019s dialogue if specialists also rated AMIE\u2019s dialogue as better,\nregardless of the exact scores each method assigned.\nAuto-evaluation Prompting Strategies. Using the rank-order agreement metric, we ablated the effect of\nthe two-step prompting and compared it to other methods such as the five-shot prompting (i.e. dropping step\n1), shuffled five-shot self-CoT prompting where the order of support examples was randomised each time, and\n0-shot prompting using only the rating scale explanation itself (see Figure A.17). All methods outperformed\nthe chance level, with the two-step process generally outperforming other methods, though this difference was\nmarginal. Shuffling the examples in the self-CoT prompt made no difference on average.\nBenchmarking Auto-evaluation. While auto-evaluation was significantly better than random guessing at\naligning with specialist preferences, it was unclear if the resulting performance was sufficient. To test this, we\nhad 139 dialogue pairs each rated by a second specialist, and we computed the rank-order agreement of the\ntwo specialists on this subset (see figure A.18). We observed that auto-evaluation was about as accurate as\nthe alternative specialists in predicting the first specialist\u2019s rank-order, suggesting that it is useful to leverage\nauto-evaluation for these criteria.\nEvaluating Self-play Dialogues. We applied our auto-evaluation procedure to 1,142 dialogues (derived\nfrom common conditions) before and after being refined through the self-play critique. We demonstrated that,\non average, the refined dialogues after an iteration of critique/revision were rated higher than the original\nbaseline dialogues across all criteria A.19.\n|41",
        "e8658ec0-21d8-46ae-ae79-033afd994681": "Figure A.15 |Prompt for the explanation generation given an dialogue and the human rating. AMIE was prompted\nto summarize good and bad aspects of a particular dialogue and provide an explanation of the given human rating between 1 and\n5.\nPrompt for Explanation Generation for Dialogue and Human Rating\nI have a doctor-patient dialogue and the corresponding rating that quantifies its quality according to\nthe following criterion: <criterion> (e.g., maintaining patient welfare). The rating of the dialogue is\non a scale of 1 to 5 where:\n5: <definition> e.g., \u201cTreats patient respectfully, and ensures comfort, safety and dignity\u201d\n1: <definition> e.g., \u201cCauses patient physical or emotional discomfort AND jeopardises patient safety\u201d\nFirst, describe which parts of the dialogue are good with respect to the criterion. Then, de-\nscribe which parts are bad with respect to the criterion. Lastly, summarise the above to explain the\nprovided rating, using the following format:\nGood: ...\nBad: ...\nSummary: ...\nDIALOGUE: <dialogue>\nRating: <human rating>\nEVALUATION:\nExample output (for a dialogue with rating 4 on \u2018maintaining patient welfare\u2019):\nGood: The doctor took the patient\u2019s concerns seriously and acted quickly to address the situation.\nThey asked pertinent questions to gather information about the patient\u2019s symptoms and medical\nhistory. They also provided clear instructions on what the patient needed to do next.\nBad: The doctor did not provide much emotional support to the patient. They could have\nbeen more reassuring and empathetic towards the patient\u2019s fear and anxiety.\nSummary: Overall, the doctor did a good job of maintaining patient welfare in this situa-\ntion. They took prompt action to address the patient\u2019s medical emergency and provided clear\ninstructions to the patient. However, they could have been more attentive to the patient\u2019s emotional\nneeds.\n|42",
        "1f0a582d-41ed-498b-b21e-ec2e46407f09": "Figure A.16 |Prompt for the auto-evaluation of qualitative criteria. We constructed few-shot prompt with self-generated\nexplanations alongside their respective dialogues as exemplars to evaluate and rate a new dialogue. This few-shot prompt included\none example for each point on the 5-point rating scale.\nPrompt for Auto-Evaluation\nI have a doctor-patient dialogue which I would like you to evaluate on the following criterion:\n<criterion> (e.g., maintaining patient welfare). The dialogue should be rated on a scale of 1-5 with\nrespect to the criterion where:\n5: <definition> e.g., \u201cTreats patient respectfully, and ensures comfort, safety and dignity\u201d\n1: <definition> e.g., \u201cCauses patient physical or emotional discomfort AND jeopardises patient safety\u201d\nHere are some example dialogues and their ratings:\nDIALOGUE: <example dialog>\nEVALUATION: <example self-generated explanation>\nRating: <example rating>\n...\nNow, please rate the following dialogue as instructed below. First, describe which parts of the dialogue\nare good with respect to the criterion. Then, describe which parts are bad with respect to the criterion.\nThird, summarise the above findings. Lastly, rate the dialogue on a scale of 1-5 with respect to the\ncriterion, according to this schema:\nGood: ...\nBad: ...\nSummary: ...\nRating: ...\nDIALOGUE: <dialogue>\nEVALUATION:\n|43",
        "677021b9-cf78-4c34-b2dc-8f465a669d7c": "A.9.1 Rank-order Agreement of Auto-evaluation to Specialist\n/uni00000014/uni00000012/uni00000014 /uni00000014/uni00000012/uni00000016 /uni00000014/uni00000012/uni00000018 /uni00000014/uni00000012/uni0000001a /uni00000014/uni00000012/uni0000001c /uni00000015/uni00000012/uni00000014\n/uni00000036/uni00000045/uni00000052/uni0000004f/uni00000011/uni00000053/uni00000056/uni00000048/uni00000049/uni00000056/uni00000004/uni00000025/uni0000004b/uni00000056/uni00000049/uni00000049/uni00000051/uni00000049/uni00000052/uni00000058/uni00000004/uni0000005b/uni0000004d/uni00000058/uni0000004c/uni00000004/uni00000037/uni00000054/uni00000049/uni00000047/uni0000004d/uni00000045/uni00000050/uni0000004d/uni00000057/uni00000058/uni00000037/uni0000004c/uni00000053/uni0000005b/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000029/uni00000051/uni00000054/uni00000045/uni00000058/uni0000004c/uni0000005d\n/uni00000037/uni00000049/uni00000049/uni0000004f/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000025/uni00000048/uni00000048/uni00000056/uni00000049/uni00000057/uni00000057/uni0000004d/uni000000",
        "9b80ea3b-88dc-40da-aa42-7c1127e524c2": "ni0000004d/uni00000052/uni0000004b/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000025/uni00000048/uni00000048/uni00000056/uni00000049/uni00000057/uni00000057/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000027/uni00000053/uni00000052/uni00000047/uni00000049/uni00000056/uni00000052/uni00000057\n/uni00000031/uni00000045/uni0000004d/uni00000052/uni00000058/uni00000045/uni0000004d/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000034/uni00000045/uni00000058/uni0000004d/uni00000049/uni00000052/uni00000058/uni00000004/uni0000003b/uni00000049/uni00000050/uni0000004a/uni00000045/uni00000056/uni00000049\n/uni00000027/uni00000053/uni00000052/uni0000004a/uni0000004d/uni00000056/uni00000051/uni0000004d/uni00000052/uni0000004b/uni00000004/uni0000002f/uni00000052/uni00000053/uni0000005b/uni00000050/uni00000049/uni00000048/uni0000004b/uni00000049/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000039/uni00000052/uni00000048/uni00000",
        "a180838b-3bf9-4993-8ae6-2310aa2aec01": "uni00000052/uni00000053/uni0000005b/uni00000050/uni00000049/uni00000048/uni0000004b/uni00000049/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000039/uni00000052/uni00000048/uni00000049/uni00000056/uni00000057/uni00000058/uni00000045/uni00000052/uni00000048/uni0000004d/uni00000052/uni0000004b/uni00000036/uni00000045/uni00000052/uni00000048/uni00000053/uni00000051\n/uni00000014/uni00000004/uni00000057/uni0000004c/uni00000053/uni00000058\n/uni00000019/uni00000004/uni00000057/uni0000004c/uni00000053/uni00000058\n/uni00000037/uni00000049/uni00000050/uni0000004a/uni00000011/uni00000027/uni00000053/uni00000038\n/uni00000037/uni00000049/uni00000050/uni0000004a/uni00000011/uni00000027/uni00000053/uni00000038\n/uni0000000c/uni00000037/uni0000004c/uni00000059/uni0000004a/uni0000004a/uni00000050/uni00000049/uni00000048/uni0000000d\nFigure A.17 |Rank-order agreement to specialist ratings of all 149 dialogue pairs, comparing various auto-\nevaluation prompting techniques. We choose to leverage the self-CoT technique for the auto-evaluation of clinical criteria.\n/uni00000014/uni00000012/uni00000014 /uni00000014/uni00000012/uni00000016 /uni00000014/uni00000012/uni00000018 /uni00000014/uni00000012/uni0000001a /uni00000014/uni00000012/uni0000001c /uni00000015/uni0000",
        "f272f7b2-a641-4f56-b79d-cd71d256c9b6": "04a/uni00000050/uni00000049/uni00000048/uni0000000d\nFigure A.17 |Rank-order agreement to specialist ratings of all 149 dialogue pairs, comparing various auto-\nevaluation prompting techniques. We choose to leverage the self-CoT technique for the auto-evaluation of clinical criteria.\n/uni00000014/uni00000012/uni00000014 /uni00000014/uni00000012/uni00000016 /uni00000014/uni00000012/uni00000018 /uni00000014/uni00000012/uni0000001a /uni00000014/uni00000012/uni0000001c /uni00000015/uni00000012/uni00000014\n/uni00000036/uni00000045/uni00000052/uni0000004f/uni00000011/uni00000053/uni00000056/uni00000048/uni00000049/uni00000056/uni00000004/uni00000025/uni0000004b/uni00000056/uni00000049/uni00000049/uni00000051/uni00000049/uni00000052/uni00000058/uni00000004/uni0000005b/uni0000004d/uni00000058/uni0000004c/uni00000004/uni00000037/uni00000054/uni00000049/uni00000047/uni0000004d/uni00000045/uni00000050/uni0000004d/uni00000057/uni00000058/uni00000037/uni0000004c/uni00000053/uni0000005b/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000029/uni00000051/uni00000054/uni00000045/uni00000058/uni0000004c/uni0000005d\n/uni00000037/uni00000049/uni00000049/uni0000004f/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000025/uni000",
        "e149ab30-b27a-4f0d-ba7a-e0add92e1af2": "/uni00000058/uni0000004c/uni0000005d\n/uni00000037/uni00000049/uni00000049/uni0000004f/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000025/uni00000048/uni00000048/uni00000056/uni00000049/uni00000057/uni00000057/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000027/uni00000053/uni00000052/uni00000047/uni00000049/uni00000056/uni00000052/uni00000057\n/uni00000031/uni00000045/uni0000004d/uni00000052/uni00000058/uni00000045/uni0000004d/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000034/uni00000045/uni00000058/uni0000004d/uni00000049/uni00000052/uni00000058/uni00000004/uni0000003b/uni00000049/uni00000050/uni0000004a/uni00000045/uni00000056/uni00000049\n/uni00000027/uni00000053/uni00000052/uni0000004a/uni0000004d/uni00000056/uni00000051/uni0000004d/uni00000052/uni0000004b/uni00000004/uni0000002f/uni00000052/uni00000053/uni0000005b/uni00000050/uni00000049/uni00000048/uni0000004b/uni00000049/uni00000004/uni00",
        "e101c59b-ea5f-4c4a-8454-7d4e46d28caa": "4d/uni00000056/uni00000051/uni0000004d/uni00000052/uni0000004b/uni00000004/uni0000002f/uni00000052/uni00000053/uni0000005b/uni00000050/uni00000049/uni00000048/uni0000004b/uni00000049/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000039/uni00000052/uni00000048/uni00000049/uni00000056/uni00000057/uni00000058/uni00000045/uni00000052/uni00000048/uni0000004d/uni00000052/uni0000004b/uni00000036/uni00000045/uni00000052/uni00000048/uni00000053/uni00000051\n/uni00000036/uni00000045/uni00000052/uni00000048/uni00000053/uni00000051/uni00000004/uni0000005b/uni00000013/uni00000004/uni00000034/uni00000056/uni00000049/uni0000005a/uni00000045/uni00000050/uni00000049/uni00000052/uni00000047/uni00000049\n/uni00000025/uni00000059/uni00000058/uni00000053/uni00000011/uni00000049/uni0000005a/uni00000045/uni00000050/uni00000059/uni00000045/uni00000058/uni0000004d/uni00000053/uni00000052\n/uni00000025/uni00000050/uni00000058/uni00000049/uni00000056/uni00000052/uni00000045/uni00000058/uni",
        "4fe113ed-5d29-477f-ab05-941c24f26f52": "05a/uni00000045/uni00000050/uni00000059/uni00000045/uni00000058/uni0000004d/uni00000053/uni00000052\n/uni00000025/uni00000050/uni00000058/uni00000049/uni00000056/uni00000052/uni00000045/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004/uni00000037/uni00000054/uni00000049/uni00000047/uni0000004d/uni00000045/uni00000050/uni0000004d/uni00000057/uni00000058\nFigure A.18 |Rank-order agreement to specialist ratings of 139 dialogue pairs (excluding cases without multiple\nspecialist ratings) for alternative specialists compared to the self-CoT auto-evaluation technique. Auto-evaluation\nagreement to the first specialist is comparable to inter-specialist agreement. The black dashed line shows the rank-order agreement\none would get with a random ranking of the AMIE and PCP dialogues, while the green dashed line shows the rank-order\nagreement with a strategy of randomly guessing according to the distribution of specialist preferences for each criteria.\n|44",
        "16eef30e-7e55-4fca-a62e-8ddc66294470": "A.9.2 Auto-evaluation of Simulated Dialogues with Self-play\n/uni00000014/uni00000012/uni00000014 /uni00000014/uni00000012/uni00000016 /uni00000014/uni00000012/uni00000018 /uni00000014/uni00000012/uni0000001a /uni00000014/uni00000012/uni0000001c /uni00000015/uni00000012/uni00000014\n/uni00000034/uni00000056/uni00000053/uni00000054/uni00000053/uni00000056/uni00000058/uni0000004d/uni00000053/uni00000052/uni00000004/uni00000034/uni00000056/uni00000049/uni0000004a/uni00000049/uni00000056/uni00000056/uni00000049/uni00000048/uni00000004/uni00000046/uni0000005d/uni00000004/uni00000025/uni00000059/uni00000058/uni00000053/uni00000011/uni00000049/uni0000005a/uni00000045/uni00000050/uni00000059/uni00000045/uni00000058/uni0000004d/uni00000053/uni00000052/uni00000027/uni00000053/uni00000052/uni0000004a/uni0000004d/uni00000056/uni00000051/uni0000004d/uni00000052/uni0000004b/uni00000004/uni0000002f/uni00000052/uni00000053/uni0000005b/uni00000050/uni00000049/uni00000048/uni0000004b/uni00000049/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000039/uni00000052/uni00000048/uni00000049/uni00000056/uni00000057/uni00000058/uni00000",
        "58112ae4-cade-4216-8e09-9b3ecd1461f8": "uni00000049/uni00000048/uni0000004b/uni00000049/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000039/uni00000052/uni00000048/uni00000049/uni00000056/uni00000057/uni00000058/uni00000045/uni00000052/uni00000048/uni0000004d/uni00000052/uni0000004b/uni00000031/uni00000045/uni0000004d/uni00000052/uni00000058/uni00000045/uni0000004d/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000034/uni00000045/uni00000058/uni0000004d/uni00000049/uni00000052/uni00000058/uni00000004/uni0000003b/uni00000049/uni00000050/uni0000004a/uni00000045/uni00000056/uni00000049/uni00000037/uni00000049/uni00000049/uni0000004f/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000045/uni00000052/uni00000048/uni00000004/uni00000025/uni00000048/uni00000048/uni00000056/uni00000049/uni00000057/uni00000057/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000027/uni00000053/uni00000052/uni00000047/uni00000049/uni00000056/uni00000052/uni00000057/uni00000037/uni0000004c/uni000000",
        "51013c3f-abc1-40ff-a39c-8ffdcb4926fb": "ni00000057/uni00000057/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000027/uni00000053/uni00000052/uni00000047/uni00000049/uni00000056/uni00000052/uni00000057/uni00000037/uni0000004c/uni00000053/uni0000005b/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000029/uni00000051/uni00000054/uni00000045/uni00000058/uni0000004c/uni0000005d\n/uni00000014/uni00000012/uni00000016/uni00000016/uni00000014/uni00000012/uni00000015/uni0000001c/uni00000014/uni00000012/uni00000016/uni00000015/uni00000014/uni00000012/uni00000016/uni00000015\n/uni00000014/uni00000012/uni00000018/uni0000001c/uni00000014/uni00000012/uni00000019/uni0000001c/uni00000014/uni00000012/uni00000019/uni00000017/uni00000014/uni00000012/uni00000018/uni0000001a\n/uni00000014/uni00000012/uni00000017/uni00000014/uni00000012/uni00000016/uni00000018/uni00000014/uni00000012/uni00000016/uni0000001b/uni00000014/uni00000012/uni00000017/uni00000017",
        "d1b65255-e250-4003-98a2-262d70718bca": "0000018/uni0000001a\n/uni00000014/uni00000012/uni00000017/uni00000014/uni00000012/uni00000016/uni00000018/uni00000014/uni00000012/uni00000016/uni0000001b/uni00000014/uni00000012/uni00000017/uni00000017 /uni00000026/uni00000045/uni00000057/uni00000049/uni00000050/uni0000004d/uni00000052/uni00000049\n/uni00000038/uni0000004d/uni00000049\n/uni00000037/uni00000049/uni00000050/uni0000004a/uni00000011/uni00000054/uni00000050/uni00000045/uni0000005d\nFigure A.19 |Comparison of the simulated dialogue quality before and after self-play as assessed by auto-\nevaluation. The self-play dialogues after one round of critique are preferred by auto-evaluation more often than the baseline\ndialogue generated without revision.\n|45",
        "3271b6cc-6c1d-4996-8c7d-d7ee2c5efb43": "Table A.5 |Model card for AMIE.\nModel characteristics\nModel initialization The model was initialized from an LLM similar to PaLM-2 [2]. Addi-\ntional fine-tuning was performed as described in Section 2.3.\nUsage\nApplication The primary use is research on LLMs for clinical history-taking and\nmedical dialogue including advancing diagnostic accuracy, alignment\nmethods, fairness, safety, and equity research, and understanding limi-\ntations of current LLMs for such applications.\nData overview\nTraining dataset The dataset was datasets of medical question answering/reasoning,\nsummarization and medical dialogue datasets in Section 2.1.\nEvaluation Randomized OSCE was performed to understand AMIE\u2019s capabilities\nand benchmark against expert primary care physicians.\nEvaluation results\nEvaluation results The results are described in Section 4.\nReferences\n1.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al.Chain-of-thought prompting\nelicits reasoning in large language models. Advances in Neural Information Processing Systems 35,24824\u201324837 (2022).\n2. Google. PaLM 2 Technical Report https://ai.google/static/documents/palm2techreport.pdf. 2023.\n|46",
        "28908a22-06c7-4379-8d5e-8e6236248d0b": "Mask R-CNN\nKaiming He Georgia Gkioxari Piotr Doll \u00b4ar Ross Girshick\nFacebook AI Research (FAIR)\nAbstract\nWe present a conceptually simple, \ufb02exible, and general\nframework for object instance segmentation. Our approach\nef\ufb01ciently detects objects in an image while simultaneously\ngenerating a high-quality segmentation mask for each in-\nstance. The method, called Mask R-CNN, extends Faster\nR-CNN by adding a branch for predicting an object mask in\nparallel with the existing branch for bounding box recogni-\ntion. Mask R-CNN is simple to train and adds only a small\noverhead to Faster R-CNN, running at 5 fps. Moreover,\nMask R-CNN is easy to generalize to other tasks, e.g., al-\nlowing us to estimate human poses in the same framework.\nWe show top results in all three tracks of the COCO suite of\nchallenges, including instance segmentation, bounding-box\nobject detection, and person keypoint detection. Without\ntricks, Mask R-CNN outperforms all existing, single-model\nentries on every task, including the COCO 2016 challenge\nwinners. We hope our simple and effective approach will\nserve as a solid baseline and help ease future research in\ninstance-level recognition. Code will be made available.\n1. Introduction\nThe vision community has rapidly improved object de-\ntection and semantic segmentation results over a short pe-\nriod of time. In large part, these advances have been driven\nby powerful baseline systems, such as the Fast/Faster R-\nCNN [ 9,29] and Fully Convolutional Network (FCN) [ 24]\nframeworks for object detection and semantic segmenta-\ntion, respectively. These methods are conceptually intuitive\nand offer \ufb02exibility and robustness, together with fast train-\ning and inference time. Our goal in this work is to develop a\ncomparably enabling framework for instance segmentation .\nInstance segmentation is challenging because it requires\nthe correct detection of all objects in an image while also\nprecisely segmenting each instance. It therefore combines\nelements from the classical computer vision tasks of ob-\nject detection , where the goal is to classify individual ob-\njects and localize each using a bounding box, and semantic\nsegmentation , where the goal is to classify each pixel into\nRoIAlignRoIAlignclass\nbox\nconvconv\n convconvFigure 1. The Mask R-CNN framework for instance segmentation.\na \ufb01xed set of categories without differentiating object in-\nstances.1Given this, one might expect a complex method\nis required to achieve good results. However, we show that\na surprisingly simple, \ufb02exible, and fast system can surpass\nprior state-of-the-art instance segmentation results.\nOur method, called Mask R-CNN , extends Faster R-CNN\n[29] by adding a branch for predicting segmentation masks\non each Region of Interest (RoI), in parallel with the ex-\nisting branch for classi\ufb01cation and bounding box regres-\nsion (Figure 1). The mask branch is a small FCN applied\nto each RoI, predicting a segmentation mask in a pixel-to-\npixel manner. Mask R-CNN is simple to implement and\ntrain given the Faster R-CNN framework, which facilitates\na wide range of \ufb02exible architecture designs. Additionally,\nthe mask branch only adds a small computational overhead,\nenabling a fast system and rapid experimentation.\nIn principle Mask R-CNN is an intuitive extension of\nFaster R-CNN, yet constructing the mask branch properly\nis critical for good results. Most importantly, Faster R-CNN\nwas not designed for pixel-to-pixel alignment between net-\nwork inputs and outputs. This is most evident in how\nRoIPool [14,9], the de facto core operation for attending\nto instances, performs coarse spatial quantization for fea-\nture extraction. To \ufb01x the misalignment, we propose a sim-\nple, quantization-free layer, called RoIAlign , that faithfully\npreserves exact spatial locations. Despite being a seem-\n1Following common terminology, we use object detection to denote\ndetection via bounding boxes , not masks, and semantic segmentation to\ndenote per-pixel classi\ufb01cation without differentiating instances. Yet we\nnote that instance segmentation is both semantic and a form of detection.\n1\n2961",
        "173be34c-00a0-4786-ac3c-119f74f67480": "dining table.96perso n1.00perso n1.00 perso n1.00 perso n1.00 perso n1.00perso n1.00\nperso n1.00person.94\nbottle.99\nbottle.99\nbottle.99\nmotor cycle1.00motor cycle1.00perso n1.00\nperso n1.00\nperson.96 perso n1.00person.83person.96\nperson.98 person.90person.92person.99 person.91\nbus.99\nperso n1.00\nperso n1.00perso n1.00\nbackpack.93perso n1.00person.99\nperso n1.00\nbackpack.99\nperson.99person.98person.89 person.95\nperso n1.00\nperso n1.00\ncar1.00traffic light.96\nperson.96truck1.00person.99\ncar.99person.85\nmotor cycle.95car.99car.92person.99perso n1.00traffic light.92 traffic light.84traffic light.95\ncar.93person.87\nperso n1.00\nperso n1.00umbre lla.98\numbre lla.98\nbackpack1.00\nhandbag.96\nelephant1.00perso n1.00perso n1.00 person.99\nsheep1 .00perso n1.00\nsheep.99sheep.91 sheep1 .00\nsheep.99sheep.99sheep.95person.99\nsheep1 .00sheep.96sheep.99sheep.99\nsheep.96\nsheep.96sheep.96sheep.86\nsheep.82sheep.93\ndining table.99\nchair.99chair.90\nchair.99chair.98\nchair.96chair.86chair.99\nbowl.81chair.96tv.99\nbottle.99\nwine gla ss.99wine gla ss1.00bowl.85\nknife.83wine gla ss1.00 wine gla ss.93wine gla ss.97\nfork.95Figure 2. Mask R-CNN results on the COCO test set. These results are based on ResNet-101 [ 15], achieving a mask AP of 35.7 and\nrunning at 5 fps. Masks are shown in color, and bounding box, category, and con\ufb01dences are also shown.\ningly minor change, RoIAlign has a large impact: it im-\nproves mask accuracy by relative 10% to 50%, showing\nbigger gains under stricter localization metrics. Second, we\nfound it essential to decouple mask and class prediction: we\npredict a binary mask for each class independently, without\ncompetition among classes, and rely on the network\u2019s RoI\nclassi\ufb01cation branch to predict the category. In contrast,\nFCNs usually perform per-pixel multi-class categorization,\nwhich couples segmentation and classi\ufb01cation, and based\non our experiments works poorly for instance segmentation.\nWithout bells and whistles, Mask R-CNN surpasses all\nprevious state-of-the-art single-model results on the COCO\ninstance segmentation task [ 23], including the heavily-\nengineered entries from the 2016 competition winner. As\na by-product, our method also excels on the COCO object\ndetection task. In ablation experiments, we evaluate multi-\nple basic instantiations, which allows us to demonstrate its\nrobustness and analyze the effects of core factors.\nOur models can run at about 200ms per frame on a GPU,\nand training on COCO takes one to two days on a single\n8-GPU machine. We believe the fast train and test speeds,\ntogether with the framework\u2019s \ufb02exibility and accuracy, will\nbene\ufb01t and ease future research on instance segmentation.\nFinally, we showcase the generality of our framework\nvia the task of human pose estimation on the COCO key-\npoint dataset [ 23]. By viewing each keypoint as a one-hot\nbinary mask, with minimal modi\ufb01cation Mask R-CNN can\nbe applied to detect instance-speci\ufb01c poses. Without tricks,\nMask R-CNN surpasses the winner of the 2016 COCO key-\npoint competition, and at the same time runs at 5 fps.",
        "b694e288-1ca0-42d0-a380-3104a8a8bd21": "Our models can run at about 200ms per frame on a GPU,\nand training on COCO takes one to two days on a single\n8-GPU machine. We believe the fast train and test speeds,\ntogether with the framework\u2019s \ufb02exibility and accuracy, will\nbene\ufb01t and ease future research on instance segmentation.\nFinally, we showcase the generality of our framework\nvia the task of human pose estimation on the COCO key-\npoint dataset [ 23]. By viewing each keypoint as a one-hot\nbinary mask, with minimal modi\ufb01cation Mask R-CNN can\nbe applied to detect instance-speci\ufb01c poses. Without tricks,\nMask R-CNN surpasses the winner of the 2016 COCO key-\npoint competition, and at the same time runs at 5 fps. Mask\nR-CNN, therefore, can be seen more broadly as a \ufb02exible\nframework for instance-level recognition and can be readily\nextended to more complex tasks.\nWe will release code to facilitate future research.2. Related Work\nR-CNN: The Region-based CNN (R-CNN) approach [ 10]\nto bounding-box object detection is to attend to a manage-\nable number of candidate object regions [ 33,16] and evalu-\nate convolutional networks [ 20,19] independently on each\nRoI. R-CNN was extended [ 14,9] to allow attending to RoIs\non feature maps using RoIPool, leading to fast speed and\nbetter accuracy. Faster R-CNN [ 29] advanced this stream\nby learning the attention mechanism with a Region Pro-\nposal Network (RPN). Faster R-CNN is \ufb02exible and robust\nto many follow-up improvements ( e.g., [30,22,17]), and is\nthe current leading framework in several benchmarks.\nInstance Segmentation: Driven by the effectiveness of R-\nCNN, many approaches to instance segmentation are based\nonsegment proposals . Earlier methods [ 10,12,13,6] re-\nsorted to bottom-up segments [ 33,2]. DeepMask [ 27] and\nfollowing works [ 28,5] learn to propose segment candi-\ndates, which are then classi\ufb01ed by Fast R-CNN. In these\nmethods, segmentation precedes recognition, which is slow\nand less accurate. Likewise, Dai et al. [7] proposed a com-\nplex multiple-stage cascade that predicts segment propos-\nals from bounding-box proposals, followed by classi\ufb01ca-\ntion. Instead, our method is based on parallel prediction of\nmasks and class labels, which is simpler and more \ufb02exible.\nMost recently, Li et al. [21] combined the segment pro-\nposal system in [ 5] and object detection system in [ 8] for\n\u201cfully convolutional instance segmentation\u201d (FCIS). The\ncommon idea in [ 5,8,21] is to predict a set of position-\nsensitive output channels fully convolutionally. These\nchannels simultaneously address object classes, boxes, and\nmasks, making the system fast. But FCIS exhibits system-\natic errors on overlapping instances and creates spurious\nedges (Figure 5), showing that it is challenged by the fun-\ndamental dif\ufb01culties of segmenting instances.\n22962",
        "54a98303-fa89-43f1-9ad1-57d2ca3e23d7": "3. Mask R-CNN\nMask R-CNN is conceptually simple: Faster R-CNN has\ntwo outputs for each candidate object, a class label and a\nbounding-box offset; to this we add a third branch that out-\nputs the object mask. Mask R-CNN is thus a natural and in-\ntuitive idea. But the additional mask output is distinct from\nthe class and box outputs, requiring extraction of much \ufb01ner\nspatial layout of an object. Next, we introduce the key ele-\nments of Mask R-CNN, including pixel-to-pixel alignment,\nwhich is the main missing piece of Fast/Faster R-CNN.\nFaster R-CNN: We begin by brie\ufb02y reviewing the Faster\nR-CNN detector [ 29]. Faster R-CNN consists of two stages.\nThe \ufb01rst stage, called a Region Proposal Network (RPN),\nproposes candidate object bounding boxes. The second\nstage, which is in essence Fast R-CNN [ 9], extracts features\nusing RoIPool from each candidate box and performs clas-\nsi\ufb01cation and bounding-box regression. The features used\nby both stages can be shared for faster inference. We re-\nfer readers to [ 17] for latest, comprehensive comparisons\nbetween Faster R-CNN and other frameworks.\nMask R-CNN: Mask R-CNN adopts the same two-stage\nprocedure, with an identical \ufb01rst stage (which is RPN). In\nthe second stage, in parallel to predicting the class and box\noffset, Mask R-CNN also outputs a binary mask for each\nRoI. This is in contrast to most recent systems, where clas-\nsi\ufb01cation depends on mask predictions ( e.g. [27,7,21]).\nOur approach follows the spirit of Fast R-CNN [ 9] that\napplies bounding-box classi\ufb01cation and regression in par-\nallel (which turned out to largely simplify the multi-stage\npipeline of original R-CNN [ 10]).\nFormally, during training, we de\ufb01ne a multi-task loss on\neach sampled RoI as L=Lcls+Lbox+Lmask . The clas-\nsi\ufb01cation loss Lclsand bounding-box loss Lboxare identi-\ncal as those de\ufb01ned in [ 9]. The mask branch has a Km2-\ndimensional output for each RoI, which encodes Kbinary\nmasks of resolution m\u00d7m, one for each of the Kclasses.\nTo this we apply a per-pixel sigmoid, and de\ufb01ne Lmask as\nthe average binary cross-entropy loss. For an RoI associated\nwith ground-truth class k,Lmask is only de\ufb01ned on the k-th\nmask (other mask outputs do not contribute to the loss).\nOur de\ufb01nition of Lmask allows the network to generate\nmasks for every class without competition among classes;\nwe rely on the dedicated classi\ufb01cation branch to predict the\nclass label used to select the output mask. This decouples\nmask and class prediction. This is different from common\npractice when applying FCNs [ 24] to semantic segmenta-\ntion, which typically uses a per-pixel softmax and a multino-\nmial cross-entropy loss. In that case, masks across classes\ncompete; in our case, with a per-pixel sigmoid and a binary\nloss, they do not. We show by experiments that this formu-\nlation is key for good instance segmentation results.Mask Representation: A mask encodes an input object\u2019s\nspatial layout. Thus, unlike class labels or box offsets\nthat are inevitably collapsed into short output vectors by\nfully-connected ( fc) layers, extracting the spatial structure\nof masks can be addressed naturally by the pixel-to-pixel\ncorrespondence provided by convolutions.\nSpeci\ufb01cally, we predict an m\u00d7mmask from each RoI\nusing an FCN [ 24]. This allows each layer in the mask\nbranch to maintain the explicit m\u00d7mobject spatial layout\nwithout collapsing it into a vector representation that lacks\nspatial dimensions. Unlike previous methods that resort to\nfclayers for mask prediction [ 27,28,7], our fully convolu-\ntional representation requires fewer parameters, and is more\naccurate as demonstrated by experiments.\nThis pixel-to-pixel behavior requires our RoI features,\nwhich themselves are small feature maps, to be well aligned\nto faithfully preserve the explicit per-pixel spatial corre-\nspondence. This motivated us to develop the following\nRoIAlign layer that plays a key role in mask prediction.",
        "a682143c-e463-4e63-9528-0f1a874edd66": "Speci\ufb01cally, we predict an m\u00d7mmask from each RoI\nusing an FCN [ 24]. This allows each layer in the mask\nbranch to maintain the explicit m\u00d7mobject spatial layout\nwithout collapsing it into a vector representation that lacks\nspatial dimensions. Unlike previous methods that resort to\nfclayers for mask prediction [ 27,28,7], our fully convolu-\ntional representation requires fewer parameters, and is more\naccurate as demonstrated by experiments.\nThis pixel-to-pixel behavior requires our RoI features,\nwhich themselves are small feature maps, to be well aligned\nto faithfully preserve the explicit per-pixel spatial corre-\nspondence. This motivated us to develop the following\nRoIAlign layer that plays a key role in mask prediction.\nRoIAlign: RoIPool [ 9] is a standard operation for extract-\ning a small feature map ( e.g., 7\u00d77) from each RoI. RoIPool\n\ufb01rstquantizes a \ufb02oating-number RoI to the discrete granu-\nlarity of the feature map, this quantized RoI is then subdi-\nvided into spatial bins which are themselves quantized, and\n\ufb01nally feature values covered by each bin are aggregated\n(usually by max pooling). Quantization is performed, e.g.,\non a continuous coordinate xby computing [x/16], where\n16 is a feature map stride and [\u00b7]is rounding; likewise, quan-\ntization is performed when dividing into bins ( e.g., 7\u00d77).\nThese quantizations introduce misalignments between the\nRoI and the extracted features. While this may not impact\nclassi\ufb01cation, which is robust to small translations, it has a\nlarge negative effect on predicting pixel-accurate masks.\nTo address this, we propose an RoIAlign layer that re-\nmoves the harsh quantization of RoIPool, properly aligning\nthe extracted features with the input. Our proposed change\nis simple: we avoid any quantization of the RoI boundaries\nor bins ( i.e., we usex/16instead of [x/16]). We use bilinear\ninterpolation [ 18] to compute the exact values of the input\nfeatures at four regularly sampled locations in each RoI bin,\nand aggregate the result (using max or average).2\nRoIAlign leads to large improvements as we show in\n\u00a74.2. We also compare to the RoIWarp operation proposed\nin [7]. Unlike RoIAlign, RoIWarp overlooked the align-\nment issue and was implemented in [ 7] as quantizing RoI\njust like RoIPool. So even though RoIWarp also adopts\nbilinear resampling motivated by [ 18], it performs on par\nwith RoIPool as shown by experiments (more details in Ta-\nble2c), demonstrating the crucial role of alignment.\n2We sample four regular locations, so that we can evaluate either max\nor average pooling. In fact, interpolating only a single value at each bin\ncenter (without pooling) is nearly as effective. One could also sample more\nthan four locations per bin, which we found to give diminishing returns.\n32963",
        "349ccb5b-1f33-47ca-b61a-74f429cda664": "Network Architecture: To demonstrate the generality of\nour approach, we instantiate Mask R-CNN with multiple\narchitectures. For clarity, we differentiate between: (i) the\nconvolutional backbone architecture used for feature ex-\ntraction over an entire image, and (ii) the network head\nfor bounding-box recognition (classi\ufb01cation and regression)\nand mask prediction that is applied separately to each RoI.\nWe denote the backbone architecture using the nomen-\nclature network-depth-features . We evaluate ResNet [ 15]\nand ResNeXt [ 35] networks of depth 50 or 101 layers. The\noriginal implementation of Faster R-CNN with ResNets\n[15] extracted features from the \ufb01nal convolutional layer\nof the 4-th stage, which we call C4. This backbone with\nResNet-50, for example, is denoted by ResNet-50-C4. This\nis a common choice used in [ 15,7,17,31].\nWe also explore another more effective backbone re-\ncently proposed by Lin et al. [22], called a Feature Pyra-\nmid Network (FPN). FPN uses a top-down architecture with\nlateral connections to build an in-network feature pyramid\nfrom a single-scale input. Faster R-CNN with an FPN back-\nbone extracts RoI features from different levels of the fea-\nture pyramid according to their scale, but otherwise the\nrest of the approach is similar to vanilla ResNet. Using a\nResNet-FPN backbone for feature extraction with Mask R-\nCNN gives excellent gains in both accuracy and speed. For\nfurther details on FPN, we refer readers to [ 22].\nFor the network head we closely follow architectures\npresented in previous work to which we add a fully con-\nvolutional mask prediction branch. Speci\ufb01cally, we ex-\ntend the Faster R-CNN box heads from the ResNet [ 15]\nand FPN [ 22] papers. Details are shown in Figure 3. The\nhead on the ResNet-C4 backbone includes the 5-th stage of\nResNet (namely, the 9-layer \u2018res5\u2019 [ 15]), which is compute-\nintensive. For FPN, the backbone already includes res5 and\nthus allows for a more ef\ufb01cient head that uses fewer \ufb01lters.\nWe note that our mask branches have a straightforward\nstructure. More complex designs have the potential to im-\nprove performance but are not the focus of this work.\n3.1. Implementation Details\nWe set hyper-parameters following existing Fast/Faster\nR-CNN work [ 9,29,22]. Although these decisions were\nmade for object detection in original papers [ 9,29,22], we\nfound our instance segmentation system is robust to them.\nTraining: As in Fast R-CNN, an RoI is considered positive\nif it has IoU with a ground-truth box of at least 0.5 and\nnegative otherwise. The mask loss Lmask is de\ufb01ned only on\npositive RoIs. The mask target is the intersection between\nan RoI and its associated ground-truth mask.\nWe adopt image-centric training [ 9]. Images are resized\nsuch that their scale (shorter edge) is 800 pixels [ 22]. Each\nmini-batch has 2 images per GPU and each image has N\nsampled RoIs, with a ratio of 1:3 of positive to negativesave\nRoI\nRoI14\u00d714\n\u00d72567\u00d77\n\u00d7256\n14\u00d714\n\u00d72561024\n28\u00d728\n\u00d72561024\nmask14\u00d714\n\u00d7256class\nbox2048RoI res57\u00d77\n\u00d710247\u00d77\n\u00d72048\n\u00d74class\nbox\n14\u00d714\n\u00d780\nmask28\u00d728\n\u00d780Faster R-CNN\nw/ ResNet [ 15]Faster R-CNN\nw/ FPN [ 22]\nFigure 3. Head Architecture : We extend two existing Faster R-\nCNN heads [ 15,22]. Left/Right panels show the heads for the\nResNet C4 and FPN backbones, from [ 15] and [ 22], respectively,\nto which a mask branch is added. Numbers denote spatial resolu-\ntion and channels. Arrows denote either conv, deconv, or fclayers\nas can be inferred from context (conv preserves spatial dimension\nwhile deconv increases it).",
        "2001f915-9d36-48ca-bd23-bc310d981532": "Head Architecture : We extend two existing Faster R-\nCNN heads [ 15,22]. Left/Right panels show the heads for the\nResNet C4 and FPN backbones, from [ 15] and [ 22], respectively,\nto which a mask branch is added. Numbers denote spatial resolu-\ntion and channels. Arrows denote either conv, deconv, or fclayers\nas can be inferred from context (conv preserves spatial dimension\nwhile deconv increases it). All convs are 3 \u00d73, except the output\nconv which is 1 \u00d71, deconvs are 2 \u00d72 with stride 2, and we use\nReLU [ 25] in hidden layers. Left: \u2018res5\u2019 denotes ResNet\u2019s \ufb01fth\nstage, which for simplicity we altered so that the \ufb01rst conv oper-\nates on a 7 \u00d77 RoI with stride 1 (instead of 14 \u00d714 / stride 2 as in\n[15]).Right : \u2018\u00d74\u2019 denotes a stack of four consecutive convs.\n[9].Nis 64 for the C4 backbone (as in [ 9,29]) and 512 for\nFPN (as in [ 22]). We train on 8 GPUs (so effective mini-\nbatch size is 16) for 160k iterations, with a learning rate of\n0.02 which is decreased by 10 at the 120k iteration. We use\na weight decay of 0.0001 and a momentum of 0.9.\nThe RPN anchors span 5 scales and 3 aspect ratios, fol-\nlowing [ 22]. For convenient ablation, RPN is trained sep-\narately and does not share features with Mask R-CNN, un-\nless speci\ufb01ed. For every entry in this paper, RPN and Mask\nR-CNN have the same backbones and so they are shareable.\nInference: At test time, the proposal number is 300 for the\nC4 backbone (as in [ 29]) and 1000 for FPN (as in [ 22]). We\nrun the box prediction branch on these proposals, followed\nby non-maximum suppression [ 11]. The mask branch is\nthen applied to the highest scoring 100 detection boxes. Al-\nthough this differs from the parallel computation used in\ntraining, it speeds up inference and improves accuracy (due\nto the use of fewer, more accurate RoIs). The mask branch\ncan predict Kmasks per RoI, but we only use the k-th mask,\nwherekis the predicted class by the classi\ufb01cation branch.\nThem\u00d7m\ufb02oating-number mask output is then resized to\nthe RoI size, and binarized at a threshold of 0.5.\nNote that since we only compute masks on the top 100\ndetection boxes, Mask R-CNN adds a small overhead to its\nFaster R-CNN counterpart ( e.g.,\u223c20% on typical models).\n4. Experiments: Instance Segmentation\nWe perform a thorough comparison of Mask R-CNN to\nthe state of the art along with comprehensive ablation exper-\niments. We use the COCO dataset [ 23] for all experiments.\nWe report the standard COCO metrics including AP (aver-\naged over IoU thresholds), AP 50, AP75, and AP S, APM,\nAPL(AP at different scales). Unless otherwise noted, AP\n42964",
        "56ee6367-46c2-4022-bd99-30077057b628": "horse1.00horse1.00horse1.00\nbus1.00bus1.00\ncar.98truck.88\ncar.93\ncar.78car.98\ncar.91 car.96car.99\ncar.94car.99car.98 truck.86\ncar.99car.95car1.00car.93 car.98car.95\ncar.97car.87\ncar.99\ncar.82car.78car.93\ncar.95\ncar.97\nperson.99traffic light.73\nperso n1.00\nperson.99person.95person.93\nperson.93\nperso n1.00person.98\nskateboard.82\nsuitcase1.00\nsuitcase.99suitcase.96suitcase1.00\nsuitcase.93suitcase.98\nsuitcase.88suitcase.72stop si gn.88\nperso n1.00 perso n1.00\nperso n1.00perso n1.00\nperson.99person.99\nbench.76skateboard.91\nskateboard.83handbag.81\nsurfb oard1.00perso n1.00perso n1.00 surfb oard1.00perso n1.00person.98\nsurfb oard1.00perso n1.00\nsurfb oard.98 surfb oard1.00person.91\nperson.74\nperso n1.00perso n1.00\nperso n1.00perso n1.00perso n1.00 perso n1.00 person.98person.99\nperso n1.00person.99 umbre lla1.00\nperson.95umbre lla.99umbre lla.97umbre lla.97\numbre lla.96\numbre lla1.00\nbackpack.96umbre lla.98\nbackpack.95person.80\nbackpack.98\nbicycle.93umbre lla.89person.89handbag.97\nhandbag.85\nperso n1.00perso n1.00 perso n1.00perso n1.00perso n1.00perso n1.00\nmotor cycle.72kite.89\nperson.99kite.95\nperson.99\nperso n1.00person.81person.72kite.93\nperson.89kite1.00\nperson.98\nperso n1.00kite.84kite.97\nperson.80\nhandbag.80person.99kite.82\nperson.98 person.96kite.98\nperson.99person.82kite.81\nperson.95 person.84kite.98kite.72\nkite.99kite.84kite.99\nperson.94person.72person.98kite.95\nperson.98person.77kite.73\nperson.78 person.71 person. 87kite.88kite.88\nperson.94kite.86kite.89\nzebra.99\nzebra1.00zebra1.00\nzebra.99zebra1.00zebra.96\nzebra.74\nzebra.96zebra.99zebra.90\nzebra.88zebra.76\ndining table.91dining table.78\nchair.97person.99\nperson.86\nchair.94chair.98person.95\nchair.95person.97\nchair.92chair.99person.97\nperson.99person.94 person.99person.87\nperson.99\nchair.83person.94person.99person.98\nchair.87chair.95person.97\nperson.96\nchair.99person.86person.89\nchair.89\nwine gla ss.93person.98person.88person.97\nperson.88person.88\nperson.91 chair.96person.95\nperson.77person.92\nwine gla ss.94cup.83\nwine gla ss.94\nwine gla ss.83\ncup.91chair.85 dining table.96\nwine gla ss.91person.96\ncup.98person.83\ndining table.75\ncup.96person.72\nwine gla ss.80chair.98person.81person.82\ndining table.81\nchair.85chair.78\ncup.",
        "4456ee43-d378-4af8-bb9a-2d54e4a4d079": "99person.97\nperson.99person.94 person.99person.87\nperson.99\nchair.83person.94person.99person.98\nchair.87chair.95person.97\nperson.96\nchair.99person.86person.89\nchair.89\nwine gla ss.93person.98person.88person.97\nperson.88person.88\nperson.91 chair.96person.95\nperson.77person.92\nwine gla ss.94cup.83\nwine gla ss.94\nwine gla ss.83\ncup.91chair.85 dining table.96\nwine gla ss.91person.96\ncup.98person.83\ndining table.75\ncup.96person.72\nwine gla ss.80chair.98person.81person.82\ndining table.81\nchair.85chair.78\ncup.75person.77\ncup.71wine gla ss.80cup.79cup.93\ncup.71\nperson.99person.99\nperso n1.00perso n1.00\nfrisb ee1.00\nperson.80person.82\nelephant1.00elephant1.00elephant1.00\nelephant.97\nelephant.99\nperso n1.00perso n1.00\ndining table.95perso n1.00person.88\nwine gla ss1.00bottle.97\nwine gla ss1.00wine gla ss.99tv.98 tv.84\nperso n1.00\nbench.97person.98perso n1.00 perso n1.00\nhandbag.73person.86 potted plant.92\nbird.93person.76 person.98 person.78 person.78 backpack.88 handbag.91\ncell phone.77clock.73\nperson.99perso n1.00person.98\nperso n1.00perso n1.00 perso n1.00person.99\nperson.99 person.99 perso n1.00 perso n1.00person.98 person.99\nhandbag.88perso n1.00 person.98person.92\nhandbag.99person.97\nperson.95\nhandbag.88traffic light.99\nperson.95\nperson.87person.95traffic light.87\ntraffic light.71\nperson.80person.95 person.95 person.73person.74\ntie.85\ncar.99\ncar.86car.97car1.00 car.95car.97traffic light1.00traffic light.99\ncar.99person.99car.95\ncar.97car.98car.98\ncar.91\ncar1.00car.96car.96\nbicycle.86car.97car.97\ncar.97car.94car.95\ncar.94car.81\nperson.87\nparki ng meter.98car.89\ndonut1.00donut.90\ndonut.88donut.81\ndonut.95\ndonut.96donut1.00 donut.98\ndonut.99donut.94donut.97donut.99\ndonut.98donut1.00\ndonut.95donut1.00\ndonut.98donut.98donut.99\ndonut.96\ndonut.89donut.96donut.95donut.98donut.89\ndonut.93donut.95\ndonut.90donut.89\ndonut.89donut.89\ndonut.86donut.86\nperso n1.00perso n1.00 perso n1.00\nperso n1.00perso n1.00perso n1.00\nperso n1.00\ndog1.00baseba ll bat.99\nbaseba ll bat.85\nbaseba ll bat.98\ntruck.92\ntruck.99truck.96truck.99truck.97\nbus.99truck.93bus.90\nperso n1.00perso n1.00horse.77horse.99\ncow.93person.96perso n1.00\nperson.99horse.97\nperson.98 person.97person.98\nperson.96\nperso n1.00\ntennis rac ket1.00chair.73person.90person.77\nperson.97\nperson.81person.",
        "c0502ce5-fc22-4f7f-be7f-184a9b9dc31f": "89donut.89\ndonut.86donut.86\nperso n1.00perso n1.00 perso n1.00\nperso n1.00perso n1.00perso n1.00\nperso n1.00\ndog1.00baseba ll bat.99\nbaseba ll bat.85\nbaseba ll bat.98\ntruck.92\ntruck.99truck.96truck.99truck.97\nbus.99truck.93bus.90\nperso n1.00perso n1.00horse.77horse.99\ncow.93person.96perso n1.00\nperson.99horse.97\nperson.98 person.97person.98\nperson.96\nperso n1.00\ntennis rac ket1.00chair.73person.90person.77\nperson.97\nperson.81person.87\nperson.71 person.96 person.99 person.98 person.94chair.97\nchair.80\nchair.71chair.94chair.92chair.99chair.93\nchair.99\nchair.91chair.81chair.98chair.83chair.81chair.81\nchair.93\nsports ba ll.99\nperso n1.00\ncouch.82perso n1.00\nperson.99perso n1.00perso n1.00perso n1.00 person.99skateboard.99\nperson.90person.98person.99person.91\nperson.99perso n1.00\nperson.80\nskateboard.98Figure 4. More results of Mask R-CNN on COCO test images, using ResNet-101-FPN and running at 5 fps, with 35.7 mask AP (Table 1).\nbackbone AP AP 50 AP75 APS APM APL\nMNC [ 7] ResNet-101-C4 24.6 44.3 24.8 4.7 25.9 43.6\nFCIS [ 21] +OHEM ResNet-101-C5-dilated 29.2 49.5 - 7.1 31.3 50.0\nFCIS+++ [ 21] +OHEM ResNet-101-C5-dilated 33.6 54.5 - - - -\nMask R-CNN ResNet-101-C4 33.1 54.9 34.8 12.1 35.6 51.1\nMask R-CNN ResNet-101-FPN 35.7 58.0 37.8 15.5 38.1 52.4\nMask R-CNN ResNeXt-101-FPN 37.1 60.0 39.4 16.9 39.9 53.5\nTable 1. Instance segmentation mask AP on COCO test-dev . MNC [ 7] and FCIS [ 21] are the winners of the COCO 2015 and 2016\nsegmentation challenges, respectively. Without bells and whistles, Mask R-CNN outperforms the more complex FCIS+++, which includes\nmulti-scale train/test, horizontal \ufb02ip test, and OHEM [ 30]. All entries are single-model results.\nis evaluating using mask IoU. As in previous work [ 3,22],\nwe train using the union of 80k train images and a 35k sub-\nset of val images ( trainval35k ), and report ablations on\nthe remaining 5k subset of val images ( minival ). We also\nreport results on test-dev [23],which has no disclosed\nlabels . Upon publication, we will upload our full results on\ntest-std to the public leaderboard, as recommended.\n4.1. Main Results\nWe compare Mask R-CNN to the state-of-the-art meth-\nods in instance segmentation in Table 1. All instantia-\ntions of our model outperform baseline variants of previ-\nous state-of-the-art models. This includes MNC [ 7] and\nFCIS [ 21], the winners of the COCO 2015 and 2016\nsegmentation challenges, respectively. Without bells and\nwhistles, Mask R-CNN with ResNet-101-FPN backbone\noutperforms FCIS+++ [ 21], which includes multi-scale\ntrain/test, horizontal \ufb02ip test, and online hard example min-\ning (OHEM) [ 30].",
        "25a56e53-93c6-45a3-9ec8-30a323466028": "We also\nreport results on test-dev [23],which has no disclosed\nlabels . Upon publication, we will upload our full results on\ntest-std to the public leaderboard, as recommended.\n4.1. Main Results\nWe compare Mask R-CNN to the state-of-the-art meth-\nods in instance segmentation in Table 1. All instantia-\ntions of our model outperform baseline variants of previ-\nous state-of-the-art models. This includes MNC [ 7] and\nFCIS [ 21], the winners of the COCO 2015 and 2016\nsegmentation challenges, respectively. Without bells and\nwhistles, Mask R-CNN with ResNet-101-FPN backbone\noutperforms FCIS+++ [ 21], which includes multi-scale\ntrain/test, horizontal \ufb02ip test, and online hard example min-\ning (OHEM) [ 30]. While outside the scope of this work, weexpect many such improvements to be applicable to ours.\nMask R-CNN outputs are visualized in Figures 2and4.\nMask R-CNN achieves good results even under challeng-\ning conditions. In Figure 5we compare our Mask R-CNN\nbaseline and FCIS+++ [ 21]. FCIS+++ exhibits systematic\nartifacts on overlapping instances, suggesting that it is chal-\nlenged by the fundamental dif\ufb01culty of instance segmenta-\ntion. Mask R-CNN shows no such artifacts.\n4.2. Ablation Experiments\nWe run a number of ablations to analyze Mask R-CNN.\nResults are shown in Table 2and discussed in detail next.\nArchitecture: Table 2ashows Mask R-CNN with various\nbackbones. It bene\ufb01ts from deeper networks (50 vs. 101)\nand advanced designs including FPN and ResNeXt3. We\nnote that notall frameworks automatically bene\ufb01t from\ndeeper or advanced networks (see benchmarking in [ 17]).\n3We use the 64 \u00d74d variant of ResNeXt [ 35].\n52965",
        "c3f0ce8f-a48c-44a1-be04-2bf6fc64faaf": "perso n1.00\nperso n1.00\nperso n1.00perso n1.00umbre lla1.00 umbre lla.99\ncar.99 car.93\ngiraffe1.00 giraffe1.00\nperso n1.00perso n1.00perso n1.00 perso n1.00\nperson.95\nsports ba ll1.00sports ba ll.98perso n1.00\nperso n1.00\nperso n1.00\ntie.95\ntie1.00\nFCIS Mask R-CNN\ntrain1.00\ntrain.99\ntrain.80\nperso n1.00 perso n1.00perso n1.00\nperso n1.00perso n1.00 perso n1.00\nskateboard.98person.99 person.99\nskateboard.99handbag.93\nFigure 5. FCIS+++ [ 21] (top) vs. Mask R-CNN (bottom, ResNet-101-FPN). FCIS exhibits systematic artifacts on overlapping objects.\nnet-depth-features AP AP 50 AP75\nResNet-50-C4 30.3 51.2 31.5\nResNet-101-C4 32.7 54.2 34.3\nResNet-50-FPN 33.6 55.2 35.3\nResNet-101-FPN 35.4 57.3 37.5\nResNeXt-101-FPN 36.7 59.5 38.9\n(a)Backbone Architecture : Better back-\nbones bring expected gains: deeper networks\ndo better, FPN outperforms C4 features, and\nResNeXt improves on ResNet.AP AP 50 AP75\nsoftmax 24.8 44.1 25.1\nsigmoid 30.3 51.2 31.5\n+5.5 +7.1 +6.4\n(b)Multinomial vs. Independent Masks\n(ResNet-50-C4): Decoupling via per-\nclass binary masks (sigmoid) gives large\ngains over multinomial masks (softmax).align? bilinear? agg. AP AP 50 AP75\nRoIPool [9] max 26.9 48.8 26.4\nRoIWarp [7]\u2713 max 27.2 49.2 27.1\n\u2713 ave 27.1 48.9 27.1\nRoIAlign\u2713\u2713 max 30.2 51.0 31.8\n\u2713\u2713 ave 30.3 51.2 31.5\n(c)RoIAlign (ResNet-50-C4): Mask results with various RoI\nlayers. Our RoIAlign layer improves AP by \u223c3 points and\nAP75by\u223c5 points. Using proper alignment is the only factor\nthat contributes to the large gap between RoI layers.\nAP AP 50 AP75 APbbAPbb\n50APbb\n75\nRoIPool 23.6 46.5 21.6 28.2 52.7 26.9\nRoIAlign 30.9 51.8 32.1 34.0 55.3 36.4\n+7.3 + 5.3 +10.5 +5.8 +2.6 +9.5\n(d)RoIAlign (ResNet-50- C5,stride 32 ): Mask-level and box-level\nAP using large-stride features. Misalignments are more severe than\nwith stride-16 features (Table 2c), resulting in massive accuracy gaps.mask branch AP AP 50 AP75\nMLP fc: 1024\u21921024\u219280\u00b728231.5 53.7 32.8\nMLP fc: 1024\u21921024\u21921024\u219280\u00b728231.5 54.0 32.6\nFCN conv: 256 \u2192256\u2192256\u2192256\u2192256\u219280 33.6 55.2 35.3\n(e)Mask Branch (ResNet-50-FPN): Fully convolutional networks (FCN) vs.\nmulti-layer perceptrons (MLP, fully-connected) for mask prediction. FCNs im-\nprove results as they take advantage of explicitly encoding spatial layout.\nTable 2. Ablations for Mask R-CNN.",
        "eb11a42b-995e-420a-8fee-8005ae056d25": "Misalignments are more severe than\nwith stride-16 features (Table 2c), resulting in massive accuracy gaps.mask branch AP AP 50 AP75\nMLP fc: 1024\u21921024\u219280\u00b728231.5 53.7 32.8\nMLP fc: 1024\u21921024\u21921024\u219280\u00b728231.5 54.0 32.6\nFCN conv: 256 \u2192256\u2192256\u2192256\u2192256\u219280 33.6 55.2 35.3\n(e)Mask Branch (ResNet-50-FPN): Fully convolutional networks (FCN) vs.\nmulti-layer perceptrons (MLP, fully-connected) for mask prediction. FCNs im-\nprove results as they take advantage of explicitly encoding spatial layout.\nTable 2. Ablations for Mask R-CNN. We train on trainval35k , test onminival , and report mask AP unless otherwise noted.\nMultinomial vs. Independent Masks: Mask R-CNN de-\ncouples mask and class prediction: as the existing box\nbranch predicts the class label, we generate a mask for each\nclass without competition among classes (by a per-pixel sig-\nmoid and a binary loss). In Table 2b, we compare this to\nusing a per-pixel softmax and a multinomial loss (as com-\nmonly used in FCN [ 24]). This alternative couples the tasks\nof mask and class prediction, and results in a severe loss\nin mask AP (5.5 points). This suggests that once the in-\nstance has been classi\ufb01ed as a whole (by the box branch),\nit is suf\ufb01cient to predict a binary mask without concern for\nthe categories, which makes the model easier to train.\nClass-Speci\ufb01c vs. Class-Agnostic Masks: Our default in-\nstantiation predicts class-speci\ufb01c masks, i.e., onem\u00d7m\nmask per class. Interestingly, Mask R-CNN with class-\nagnostic masks ( i.e., predicting a single m\u00d7moutput re-\ngardless of class) is nearly as effective: it has 29.7 mask AP\nvs. 30.3 for the class-speci\ufb01c counterpart on ResNet-50-C4.\nThis further highlights the division of labor in our approach\nwhich largely decouples classi\ufb01cation and segmentation.\nRoIAlign: An evaluation of our proposed RoIAlign layer is\nshown in Table 2c. For this experiment we use the ResNet-\n50-C4 backbone, which has stride 16. RoIAlign improvesAP by about 3 points over RoIPool, with much of the gain\ncoming at high IoU (AP 75). RoIAlign is insensitive to\nmax/average pool; we use average in the rest of the paper.\nAdditionally, we compare with RoIWarp proposed in\nMNC [ 7] that also adopt bilinear sampling. As discussed in\n\u00a73, RoIWarp still quantizes the RoI, losing alignment with\nthe input. As can be seen in Table 2c, RoIWarp performs\non par with RoIPool and much worse than RoIAlign. This\nhighlights that proper alignment is key.\nWe also evaluate RoIAlign with a ResNet-50-C5 back-\nbone, which has an even larger stride of 32 pixels. We use\nthe same head as in Figure 3(right), as the res5 head is not\napplicable. Table 2dshows that RoIAlign improves mask\nAP by a massive 7.3 points, and mask AP 75by 10.5 points\n(50% relative improvement ). Moreover, we note that with\nRoIAlign, using stride-32 C5 features (30.9 AP) is more ac-\ncurate than using stride-16 C4 features (30.3 AP, Table 2c).\nRoIAlign largely resolves the long-standing challenge of\nusing large-stride features for detection and segmentation.\nFinally, RoIAlign shows a gain of 1.5 mask AP and 0.5\nbox AP when used with FPN, which has \ufb01ner multi-level\nstrides. For keypoint detection that requires \ufb01ner alignment,\nRoIAlign shows large gains even with FPN (Table 6).\n62966",
        "ff596b04-a6f1-4167-a8a2-70e4cc22c002": "backbone APbbAPbb\n50APbb\n75APbb\nSAPbb\nMAPbb\nL\nFaster R-CNN+++ [ 15] ResNet-101-C4 34.9 55.7 37.4 15.6 38.7 50.9\nFaster R-CNN w FPN [ 22] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2\nFaster R-CNN by G-RMI [ 17]Inception-ResNet-v2 [ 32] 34.7 55.5 36.7 13.5 38.1 52.0\nFaster R-CNN w TDM [ 31] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1\nFaster R-CNN, RoIAlign ResNet-101-FPN 37.3 59.6 40.3 19.8 40.2 48.8\nMask R-CNN ResNet-101-FPN 38.2 60.3 41.7 20.1 41.1 50.2\nMask R-CNN ResNeXt-101-FPN 39.8 62.3 43.4 22.1 43.2 51.2\nTable 3. Object detection single-model results (bounding box AP), vs. state-of-the-art on test-dev . Mask R-CNN using ResNet-101-\nFPN outperforms the base variants of all previous state-of-the-art models (the mask output is ignored in these experiments). The gains of\nMask R-CNN over [ 22] come from using RoIAlign (+1.1 APbb), multitask training (+0.9 APbb), and ResNeXt-101 (+1.6 APbb).\nMask Branch: Segmentation is a pixel-to-pixel task and\nwe exploit the spatial layout of masks by using an FCN.\nIn Table 2e, we compare multi-layer perceptrons (MLP)\nand FCNs, using a ResNet-50-FPN backbone. Using FCNs\ngives a 2.1 mask AP gain over MLPs. We note that we\nchoose this backbone so that the conv layers of the FCN\nhead are not pre-trained, for a fair comparison with MLP.\n4.3. Bounding Box Detection Results\nWe compare Mask R-CNN to the state-of-the-art COCO\nbounding-box object detection in Table 3. For this result,\neven though the full Mask R-CNN model is trained, only\nthe classi\ufb01cation and box outputs are used at inference (the\nmask output is ignored). Mask R-CNN using ResNet-101-\nFPN outperforms the base variants of all previous state-of-\nthe-art models, including the single-model variant of G-\nRMI [ 17], the winner of the COCO 2016 Detection Chal-\nlenge. Using ResNeXt-101-FPN, Mask R-CNN further im-\nproves results, with a margin of 3.0 points box AP over\nthe best previous single model entry from [ 31] (which used\nInception-ResNet-v2-TDM).\nAs a further comparison, we trained a version of Mask\nR-CNN but without the mask branch, denoted by \u201cFaster\nR-CNN, RoIAlign\u201d in Table 3. This model performs better\nthan the model presented in [ 22] due to RoIAlign. On the\nother hand, it is 0.9 points box AP lower than Mask R-CNN.\nThis gap of Mask R-CNN on box detection is therefore due\nsolely to the bene\ufb01ts of multi-task training.\nLastly, we note that Mask R-CNN attains a small gap\nbetween its mask and box AP: e.g., 2.7 points between 37.1\n(mask, Table 1) and 39.8 (box, Table 3). This indicates that\nour approach largely closes the gap between object detec-\ntion and the more challenging instance segmentation task.\n4.4. Timing\nInference: We train a ResNet-101-FPN model that shares\nfeatures between the RPN and Mask R-CNN stages, follow-\ning the 4-step training of Faster R-CNN [ 29].",
        "db3d9e57-f35c-4393-b4a9-a668693e8deb": "On the\nother hand, it is 0.9 points box AP lower than Mask R-CNN.\nThis gap of Mask R-CNN on box detection is therefore due\nsolely to the bene\ufb01ts of multi-task training.\nLastly, we note that Mask R-CNN attains a small gap\nbetween its mask and box AP: e.g., 2.7 points between 37.1\n(mask, Table 1) and 39.8 (box, Table 3). This indicates that\nour approach largely closes the gap between object detec-\ntion and the more challenging instance segmentation task.\n4.4. Timing\nInference: We train a ResNet-101-FPN model that shares\nfeatures between the RPN and Mask R-CNN stages, follow-\ning the 4-step training of Faster R-CNN [ 29]. This model\nruns at 195ms per image on an Nvidia Tesla M40 GPU (plus\n15ms CPU time resizing the outputs to the original resolu-\ntion), and achieves statistically the same mask AP as the\nunshared one. We also report that the ResNet-101-C4 vari-ant takes \u223c400ms as it has a heavier box head (Figure 3), so\nwe do not recommend using the C4 variant in practice.\nAlthough Mask R-CNN is fast, we note that our design\nis not optimized for speed, and better speed/accuracy trade-\noffs could be achieved [ 17],e.g., by varying image sizes and\nproposal numbers, which is beyond the scope of this paper.\nTraining: Mask R-CNN is also fast to train. Training with\nResNet-50-FPN on COCO trainval35k takes 32 hours\nin our synchronized 8-GPU implementation (0.72s per 16-\nimage mini-batch), and 44 hours with ResNet-101-FPN. In\nfact, fast prototyping can be completed in less than one day\nwhen training on the train set. We hope such rapid train-\ning will remove a major hurdle in this area and encourage\nmore people to perform research on this challenging topic.\n5. Mask R-CNN for Human Pose Estimation\nOur framework can easily be extended to human pose\nestimation. We model a keypoint\u2019s location as a one-hot\nmask, and adopt Mask R-CNN to predict Kmasks, one for\neach ofKkeypoint types ( e.g., left shoulder, right elbow).\nThis task helps demonstrate the \ufb02exibility of Mask R-CNN.\nWe note that minimal domain knowledge for human pose\nis exploited by our system, as the experiments are mainly to\ndemonstrate the generality of the Mask R-CNN framework.\nWe expect that domain knowledge ( e.g., modeling struc-\ntures [ 4]) will be complementary to our simple approach,\nbut it is beyond the scope of this paper.\nImplementation Details: We make minor modi\ufb01cations to\nthe segmentation system when adapting it for keypoints.\nFor each of the Kkeypoints of an instance, the training\ntarget is a one-hot m\u00d7mbinary mask where only a single\npixel is labeled as foreground. During training, for each vis-\nible ground-truth keypoint, we minimize the cross-entropy\nloss over an m2-way softmax output (which encourages a\nsingle point to be detected). We note that as in instance seg-\nmentation, the Kkeypoints are still treated independently.\nWe adopt the ResNet-FPN variant, and the keypoint head\narchitecture is similar to that in Figure 3(right). The key-\npoint head consists of a stack of eight 3 \u00d73 512-d conv lay-\ners, followed by a deconv layer and 2 \u00d7bilinear upscaling,\nproducing an output resolution of 56 \u00d756. We found that\n72967",
        "e1ebe309-db45-4dc3-9937-a58e6b6b3cb8": "Figure 6. Keypoint detection results on COCO test using Mask R-CNN (ResNet-50-FPN), with person segmentation masks predicted\nfrom the same model. This model has a keypoint AP of 63.1 and runs at 5 fps.\nAPkpAPkp\n50APkp\n75APkp\nMAPkp\nL\nCMU-Pose+++ [ 4] 61.8 84.9 67.5 57.1 68.2\nG-RMI [ 26]\u202062.4 84.0 68.5 59.1 68.1\nMask R-CNN , keypoint-only 62.7 87.0 68.4 57.4 71.1\nMask R-CNN , keypoint & mask 63.1 87.3 68.7 57.8 71.4\nTable 4. Keypoint detection AP on COCO test-dev . Ours\n(ResNet-50-FPN) is a single model that runs at 5 fps. CMU-\nPose+++ [ 4] is the 2016 competition winner that uses multi-scale\ntesting, post-processing with CPM [ 34], and \ufb01ltering with an ob-\nject detector, adding a cumulative \u223c5 points (clari\ufb01ed in personal\ncommunication).\u2020: G-RMI was trained on COCO plus MPII [ 1]\n(25k images), using two models (Inception-ResNet-v2 + ResNet-\n101). As they use more data, this is nota direct comparison with\nMask R-CNN.\na relatively high resolution output (compared to masks) is\nrequired for keypoint-level localization accuracy.\nModels are trained on all COCO trainval35k im-\nages that contain annotated keypoints. To reduce over\ufb01t-\nting, as this training set is smaller, we train the models us-\ning image scales randomly sampled from [640, 800] pixels;\ninference is on a single scale of 800 pixels. We train for 90k\niterations, starting from a learning rate of 0.02 and reducing\nit by 10 at 60k and 80k iterations. We use bounding-box\nnon-maximum suppression with a threshold of 0.5. Other\nimplementations are identical as in \u00a73.1.\nExperiments on Human Pose Estimation: We evaluate\nthe person keypoint AP (APkp) using ResNet-50-FPN. We\nhave experimented with ResNet-101 and found it achieves\nsimilar results, possibly because deeper models bene\ufb01t from\nmore training data, but this dataset is relatively small.\nTable 4shows that our result (62.7 APkp) is 0.9 points\nhigher than the COCO 2016 keypoint detection winner [ 4]\nthat uses a multi-stage processing pipeline (see caption of\nTable 4). Our method is considerably simpler and faster.\nMore importantly, we have a uni\ufb01ed model that can si-\nmultaneously predict boxes, segments, and keypoints while\nrunning at 5 fps. Adding a segment branch (for the per-APbb\nperson APmask\nperson APkp\nFaster R-CNN 52.5 - -\nMask R-CNN, mask-only 53.6 45.8 -\nMask R-CNN, keypoint-only 50.7 - 64.2\nMask R-CNN, keypoint & mask 52.0 45.1 64.7\nTable 5. Multi-task learning of box, mask, and keypoint about the person\ncategory, evaluated on minival . All entries are trained on the same data\nfor fair comparisons. The backbone is ResNet-50-FPN. The entry with\n64.2 AP on minival has 62.7 AP on test-dev . The entry with 64.7\nAP onminival has 63.1 AP on test-dev (see Table 4).\nAPkpAPkp\n50APkp\n75APkp\nMAPkp\nL\nRoIPool 59.8 86.2 66.7 55.1 67.4\nRoIAlign 64.2 86.6 69.7 58.7 73.0\nTable 6. RoIAlign vs. RoIPool for keypoint detection on minival .\nson category) improves the APkpto 63.1 (Table 4) on\ntest-dev .",
        "39795960-cc3e-484e-b76c-e98cc3b8fcd0": "All entries are trained on the same data\nfor fair comparisons. The backbone is ResNet-50-FPN. The entry with\n64.2 AP on minival has 62.7 AP on test-dev . The entry with 64.7\nAP onminival has 63.1 AP on test-dev (see Table 4).\nAPkpAPkp\n50APkp\n75APkp\nMAPkp\nL\nRoIPool 59.8 86.2 66.7 55.1 67.4\nRoIAlign 64.2 86.6 69.7 58.7 73.0\nTable 6. RoIAlign vs. RoIPool for keypoint detection on minival .\nson category) improves the APkpto 63.1 (Table 4) on\ntest-dev . More ablations of multi-task learning on\nminival are in Table 5. Adding the mask branch to the\nbox-only ( i.e., Faster R-CNN) or keypoint-only versions\nconsistently improves these tasks. However, adding the\nkeypoint branch reduces the box/mask AP slightly, suggest-\ning that while keypoint detection bene\ufb01ts from multitask\ntraining, it does not in turn help the other tasks. Neverthe-\nless, learning all three tasks jointly enables a uni\ufb01ed system\nto ef\ufb01ciently predict all outputs simultaneously (Figure 6).\nWe also investigate the effect of RoIAlign on keypoint\ndetection (Table 6). Though this ResNet-50-FPN backbone\nhas \ufb01ner strides ( e.g., 4 pixels on the \ufb01nest level), RoIAlign\nstill shows signi\ufb01cant improvement over RoIPool and in-\ncreases APkpby 4.4 points. This is because keypoint detec-\ntions are more sensitive to localization accuracy. This again\nindicates that alignment is essential for pixel-level localiza-\ntion, including masks and keypoints.\nGiven the effectiveness of Mask R-CNN for extracting\nobject bounding boxes, masks, and keypoints, we expect it\nbe an effective framework for other instance-level tasks.\n82968",
        "d9f61872-3e64-41ee-99b7-aa02adc98929": "References\n[1] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2D\nhuman pose estimation: New benchmark and state of the art\nanalysis. In CVPR , 2014. 8\n[2] P. Arbel \u00b4aez, J. Pont-Tuset, J. T. Barron, F. Marques, and\nJ. Malik. Multiscale combinatorial grouping. In CVPR ,\n2014. 2\n[3] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-\noutside net: Detecting objects in context with skip pooling\nand recurrent neural networks. In CVPR , 2016. 5\n[4] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh. Realtime multi-\nperson 2d pose estimation using part af\ufb01nity \ufb01elds. In CVPR ,\n2017. 7,8\n[5] J. Dai, K. He, Y . Li, S. Ren, and J. Sun. Instance-sensitive\nfully convolutional networks. In ECCV , 2016. 2\n[6] J. Dai, K. He, and J. Sun. Convolutional feature masking for\njoint object and stuff segmentation. In CVPR , 2015. 2\n[7] J. Dai, K. He, and J. Sun. Instance-aware semantic segmen-\ntation via multi-task network cascades. In CVPR , 2016. 2,3,\n4,5,6\n[8] J. Dai, Y . Li, K. He, and J. Sun. R-FCN: Object detection via\nregion-based fully convolutional networks. In NIPS , 2016. 2\n[9] R. Girshick. Fast R-CNN. In ICCV , 2015. 1,2,3,4,6\n[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In CVPR , 2014. 2,3\n[11] R. Girshick, F. Iandola, T. Darrell, and J. Malik. Deformable\npart models are convolutional neural networks. In CVPR ,\n2015. 4\n[12] B. Hariharan, P. Arbel \u00b4aez, R. Girshick, and J. Malik. Simul-\ntaneous detection and segmentation. In ECCV . 2014. 2\n[13] B. Hariharan, P. Arbel \u00b4aez, R. Girshick, and J. Malik. Hyper-\ncolumns for object segmentation and \ufb01ne-grained localiza-\ntion. In CVPR , 2015. 2\n[14] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\nin deep convolutional networks for visual recognition. In\nECCV . 2014. 1,2\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR , 2016. 2,4,7\n[16] J. Hosang, R. Benenson, P. Doll \u00b4ar, and B. Schiele. What\nmakes for effective detection proposals? PAMI , 2015. 2\n[17] J. Huang, V . Rathod, C. Sun, M. Zhu, A. Korattikara,\nA. Fathi, I. Fischer, Z. Wojna, Y . Song, S. Guadarrama, et al.\nSpeed/accuracy trade-offs for modern convolutional object\ndetectors. In CVPR , 2017. 2,3,4,5,7\n[18] M. Jaderberg, K. Simonyan, A. Zisserman, and\nK. Kavukcuoglu. Spatial transformer networks. In\nNIPS , 2015. 3\n[19] A. Krizhevsky, I. Sutskever, and G. Hinton.",
        "e55be5f6-f8fd-42a1-82cb-c1bbbab68fb2": "What\nmakes for effective detection proposals? PAMI , 2015. 2\n[17] J. Huang, V . Rathod, C. Sun, M. Zhu, A. Korattikara,\nA. Fathi, I. Fischer, Z. Wojna, Y . Song, S. Guadarrama, et al.\nSpeed/accuracy trade-offs for modern convolutional object\ndetectors. In CVPR , 2017. 2,3,4,5,7\n[18] M. Jaderberg, K. Simonyan, A. Zisserman, and\nK. Kavukcuoglu. Spatial transformer networks. In\nNIPS , 2015. 3\n[19] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-\nsi\ufb01cation with deep convolutional neural networks. In NIPS ,\n2012. 2\n[20] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. Backpropagation\napplied to handwritten zip code recognition. Neural compu-\ntation , 1989. 2[21] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei. Fully convolutional\ninstance-aware semantic segmentation. In CVPR , 2017. 2,\n3,5,6\n[22] T.-Y . Lin, P. Doll \u00b4ar, R. Girshick, K. He, B. Hariharan, and\nS. Belongie. Feature pyramid networks for object detection.\nInCVPR , 2017. 2,4,5,7\n[23] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll \u00b4ar, and C. L. Zitnick. Microsoft COCO: Com-\nmon objects in context. In ECCV , 2014. 2,4,5\n[24] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In CVPR , 2015. 1,3,6\n[25] V . Nair and G. E. Hinton. Recti\ufb01ed linear units improve re-\nstricted boltzmann machines. In ICML , 2010. 4\n[26] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tomp-\nson, C. Bregler, and K. Murphy. Towards accurate multi-\nperson pose estimation in the wild. In CVPR , 2017. 8\n[27] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to seg-\nment object candidates. In NIPS , 2015. 2,3\n[28] P. O. Pinheiro, T.-Y . Lin, R. Collobert, and P. Doll \u00b4ar. Learn-\ning to re\ufb01ne object segments. In ECCV , 2016. 2,3\n[29] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-\nwards real-time object detection with region proposal net-\nworks. In NIPS , 2015. 1,2,3,4,7\n[30] A. Shrivastava, A. Gupta, and R. Girshick. Training region-\nbased object detectors with online hard example mining. In\nCVPR , 2016. 2,5\n[31] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be-\nyond skip connections: Top-down modulation for object de-\ntection. arXiv:1612.06851 , 2016. 4,7\n[32] C. Szegedy, S. Ioffe, and V . Vanhoucke. Inception-v4,\ninception-resnet and the impact of residual connections on\nlearning. In ICLR Workshop , 2016.",
        "eacaaa76-7d88-41b4-8f4c-520c3d839acb": "In NIPS , 2015. 1,2,3,4,7\n[30] A. Shrivastava, A. Gupta, and R. Girshick. Training region-\nbased object detectors with online hard example mining. In\nCVPR , 2016. 2,5\n[31] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be-\nyond skip connections: Top-down modulation for object de-\ntection. arXiv:1612.06851 , 2016. 4,7\n[32] C. Szegedy, S. Ioffe, and V . Vanhoucke. Inception-v4,\ninception-resnet and the impact of residual connections on\nlearning. In ICLR Workshop , 2016. 7\n[33] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.\nSmeulders. Selective search for object recognition. IJCV ,\n2013. 2\n[34] S.-E. Wei, V . Ramakrishna, T. Kanade, and Y . Sheikh. Con-\nvolutional pose machines. In CVPR , 2016. 8\n[35] S. Xie, R. Girshick, P. Doll \u00b4ar, Z. Tu, and K. He. Aggregated\nresidual transformations for deep neural networks. In CVPR ,\n2017. 4,5\n92969",
        "7bc5a5a6-08d1-4746-8247-a1c7a617519c": "Faster R-CNN: Towards Real-Time Object\nDetection with Region Proposal Networks\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun\nAbstract\u2014 State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances\nlike SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation\nas a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the\ndetection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts\nobject bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which\nare used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional\nfeatures\u2014using the recently popular terminology of neural networks with \u2019attention\u2019 mechanisms, the RPN component tells the uni\ufb01ed\nnetwork where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps ( including all steps )o na\nGPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300\nproposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning\nentries in several tracks. Code has been made publicly available.\nIndex Terms\u2014 Object detection, region proposal, convolutional neural network\n\u00c7\n1I NTRODUCTION\nRECENT advances in object detection are driven by the\nsuccess of region proposal methods (e.g., [4]) and\nregion-based convolutional neural networks (R-CNNs) [5].\nAlthough region-based CNNs were computationally expen-\nsive as originally developed in [5], their cost has been drasti-\ncally reduced thanks to sharing convolutions across\nproposals [1], [2]. The latest incarnation, Fast R-CNN [2],\nachieves near real-time rates using very deep networks [3],\nwhen ignoring the time spent on region proposals . Now, pro-\nposals are the test-time computational bottleneck in state-\nof-the-art detection systems.\nRegion proposal methods typically rely on inexpensive\nfeatures and economical inference schemes. Selective Search\n[4], one of the most popular methods, greedily merges super-\npixels based on engineered low-level features. Yet when com-\npared to ef\ufb01cient detection networks [2], Selective Search is an\norder of magnitude slower, at 2 seconds per image in a CPU\nimplementation. EdgeBoxes [6] currently provides the best\ntradeoff between proposal quality and speed, at 0.2 seconds\nper image. Nevertheless, the region proposal step still con-\nsumes as much running time as the detection network.\nOne may note that fast region-based CNNs take advantage\nof GPUs, while the region proposal methods used in research\nare implemented on the CPU, making such runtimecomparisons inequitable. An obvious way to accelerate pro-\nposal computation is to re-implement it for the GPU. This\nmay be an effective engineering solution, but re-implementa-\ntion ignores the down-stream detection network and there-\nfore misses important opportunities for sharing computation.\nIn this paper, we show that an algorithmic change\u2014com-\nputing proposals with a deep convolutional neural net-\nwork\u2014leads to an elegant and effective solution where\nproposal computation is nearly cost-free given the detection\nnetwork\u2019s computation. To this end, we introduce novel\nRegion Proposal Networks (RPNs) that share convolutional\nlayers with state-of-the-art object detection networks [1], [2].\nBy sharing convolutions at test-time, the marginal cost for\ncomputing proposals is small (e.g., 10 ms per image).\nOur observation is that the convolutional feature maps\nused by region-based detectors, like Fast R-CNN, can also\nbe used for generating region proposals. On top of these\nconvolutional features, we construct an RPN by adding a\nfew additional convolutional layers that simultaneously\nregress region bounds and objectness scores at each location\non a regular grid. The RPN is thus a kind of fully convolu-\ntional network (FCN) [7] and can be trained end-to-end spe-\nci\ufb01cally for the task for generating detection proposals.",
        "43a28897-e3d9-4796-95b2-58dd9e7d9554": "To this end, we introduce novel\nRegion Proposal Networks (RPNs) that share convolutional\nlayers with state-of-the-art object detection networks [1], [2].\nBy sharing convolutions at test-time, the marginal cost for\ncomputing proposals is small (e.g., 10 ms per image).\nOur observation is that the convolutional feature maps\nused by region-based detectors, like Fast R-CNN, can also\nbe used for generating region proposals. On top of these\nconvolutional features, we construct an RPN by adding a\nfew additional convolutional layers that simultaneously\nregress region bounds and objectness scores at each location\non a regular grid. The RPN is thus a kind of fully convolu-\ntional network (FCN) [7] and can be trained end-to-end spe-\nci\ufb01cally for the task for generating detection proposals.\nRPNs are designed to ef\ufb01ciently predict region proposals\nwith a wide range of scales and aspect ratios. In contrast to\nprevalent methods [1], [2], [8], [9] that use pyramids of\nimages (Fig. 1a) or pyramids of \ufb01lters (Fig. 1b), we introduce\nnovel \u201canchor\u201d boxes that serve as references at multiple\nscales and aspect ratios. Our scheme can be thought of as a\npyramid of regression references (Fig. 1c), which avoids\nenumerating images or \ufb01lters of multiple scales or aspect\nratios. This model performs well when trained and tested\nusing single-scale images and thus bene\ufb01ts running speed.\nTo unify RPNs with Fast R-CNN [2] object detection\nnetworks, we propose a training scheme that alternates/C15S. Ren is with University of Science and Technology of China, Hefei,\nAnhui 230026, China. E-mail: sqren@mail.ustc.edu.cn.\n/C15K. He and J. Sun are with Visual Computing Group, Microsoft Research,\nBeijing 100080, China. E-mail: {kahe, jiansun}@microsoft.com.\n/C15R. Girshick is with Facebook AI Research, Seattle, WA 98109.\nE-mail: rbg@fb.com.\nManuscript received 29 Dec. 2015; revised 21 Apr. 2016; accepted 28 May\n2016. Date of publication 5 June 2016; date of current version 12 May 2017.\nRecommended for acceptance by D. Hoiem.\nFor information on obtaining reprints of this article, please send e-mail to:\nreprints@ieee.org, and reference the Digital Object Identi\ufb01er below.\nDigital Object Identi\ufb01er no. 10.1109/TPAMI.2016.2577031IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 6, JUNE 2017 1137\n0162-8828 /C2232016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "920b16c9-fc64-4232-bd02-d34d064df8b1": "between \ufb01ne-tuning for the region proposal task and then\n\ufb01ne-tuning for object detection, while keeping the proposals\n\ufb01xed. This scheme converges quickly and produces a uni-\n\ufb01ed network with convolutional features that are shared\nbetween both tasks.1\nWe comprehensively evaluate our method on the PAS-\nCAL VOC detection benchmarks [11] where RPNs with Fast\nR-CNNs produce detection accuracy better than the strong\nbaseline of Selective Search with Fast R-CNNs. Meanwhile,\nour method waives nearly all computational burdens of\nSelective Search at test-time\u2014the effective running time for\nproposals is just 10 milliseconds. Using the expensive very\ndeep models of [3], our detection method still has a frame\nrate of 5 fps ( including all steps ) on a GPU, and thus is a practi-\ncal object detection system in terms of both speed and accu-\nracy. We also report results on the MS COCO dataset [12]\nand investigate the improvements on PASCAL VOC using\nthe COCO data. Code has been made publicly available at\nhttps://github.com/shaoqingren/faster_rcnn\n(in MATLAB) and https://github.com/rbgirshick/\npy-faster-rcnn (in Python).\nA preliminary version of this manuscript was published\npreviously [10]. Since then, the frameworks of RPN and\nFaster R-CNN have been adopted and generalized to other\nmethods, such as 3D object detection [13], part-based detec-\ntion [14], instance segmentation [15], and image captioning\n[16]. Our fast and effective object detection system has also\nbeen built in commercial systems such as at Pinterests [17],\nwith user engagement improvements reported.\nIn ILSVRC and COCO 2015 competitions, Faster R-CNN\nand RPN are the basis of several 1st-place entries [18] in the\ntracks of ImageNet detection, ImageNet localization, COCO\ndetection, and COCO segmentation. RPNs completely learn\nto propose regions from data, and thus can easily bene\ufb01t\nfrom deeper and more expressive features (such as the 101-\nlayer residual nets adopted in [18]). Faster R-CNN and RPN\nare also used by several other leading entries in these com-\npetitions.2These results suggest that our method is not only\na cost-ef\ufb01cient solution for practical usage, but also an effec-\ntive way of improving object detection accuracy.\n2R ELATED WORK\nObject Proposals. There is a large literature on object proposal\nmethods. Comprehensive surveys and comparisons ofobject proposal methods can be found in [19], [20], [21].\nWidely used object proposal methods include those based\non grouping super-pixels (e.g., Selective Search [4], CPMC\n[22], MCG [23]) and those based on sliding windows (e.g.,\nobjectness in windows [24], EdgeBoxes [6]). Object proposal\nmethods were adopted as external modules independent of\nthe detectors (e.g., Selective Search [4] object detectors, R-\nCNN [5], and Fast R-CNN [2]).\nDeep Networks for Object Detection. The R-CNN method [5]\ntrains CNNs end-to-end to classify the proposal regions into\nobject categories or background. R-CNN mainly plays as a\nclassi\ufb01er, and it does not predict object bounds (except for\nre\ufb01ning by bounding box regression). Its accuracy depends\non the performance of the region proposal module (see com-\nparisons in [20]). Several papers have proposed ways of using\ndeep networks for predicting object bounding boxes [9], [25],\n[26], [27]. In the OverFeat method [9], a fully-connected layer\nis trained to predict the box coordinates for the localization\ntask that assumes a single object. The fully-connected layer is\nthen turned into a convolutional layer for detecting multiple\nclass-speci\ufb01c objects. The MultiBox methods [26], [27] gener-\nate region proposals from a network whose last fully-con-\nnected layer simultaneously predicts multiple class-agnostic\nboxes, generalizing the \u201csingle-box\u201d fashion of OverFeat.\nThese class-agnostic boxes are used as proposals for R-CNN\n[5]. The MultiBox proposal network is applied on a single\nimage crop or multiple large image crops (e.g., 224/C2224), in\ncontrast to our fully convolutional scheme. MultiBox does\nnot share features between the proposal and detection net-\nworks.",
        "12b54ac9-6c5c-4a58-b670-ded0db26d79b": "In the OverFeat method [9], a fully-connected layer\nis trained to predict the box coordinates for the localization\ntask that assumes a single object. The fully-connected layer is\nthen turned into a convolutional layer for detecting multiple\nclass-speci\ufb01c objects. The MultiBox methods [26], [27] gener-\nate region proposals from a network whose last fully-con-\nnected layer simultaneously predicts multiple class-agnostic\nboxes, generalizing the \u201csingle-box\u201d fashion of OverFeat.\nThese class-agnostic boxes are used as proposals for R-CNN\n[5]. The MultiBox proposal network is applied on a single\nimage crop or multiple large image crops (e.g., 224/C2224), in\ncontrast to our fully convolutional scheme. MultiBox does\nnot share features between the proposal and detection net-\nworks. We discuss OverFeat and MultiBox in more depth\nlater in context with our method. Concurrent with our work,\nthe DeepMask method [28] is developed for learning segmen-\ntation proposals.\nShared computation of convolutions [1], [2], [7], [9], [29]\nhas been attracting increasing attention for ef\ufb01cient, yet\naccurate, visual recognition. The OverFeat paper [9] com-\nputes convolutional features from an image pyramid for\nclassi\ufb01cation, localization, and detection. Adaptively-sized\npooling (SPP) [1] on shared convolutional feature maps is\ndeveloped for ef\ufb01cient region-based object detection [1],\n[30] and semantic segmentation [29]. Fast R-CNN [2] ena-\nbles end-to-end detector training on shared convolutional\nfeatures and shows compelling accuracy and speed.\n3F ASTER R-CNN\nOur object detection system, called Faster R-CNN, is\ncomposed of two modules. The \ufb01rst module is a deep fully\nconvolutional network that proposes regions, and the\nFig. 1. Different schemes for addressing multiple scales and sizes. (a) Pyramids of images and feature maps are built, and the classi\ufb01er is run at all sc ales.\n(b) Pyramids of \ufb01lters with multiple scales/sizes are run on the feature map. (c) We use pyramids of reference boxes in the regression functions.\n1.Since the publication of the conference version of this paper [10],\nwe have also found that RPNs can be trained jointly with Fast R-CNN\nnetworks leading to less training time.\n2.http://image-net.org/challenges/LSVRC/2015/results.1138 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 6, JUNE 2017\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "c2cb04e1-2717-4651-b46b-93062a5c54c4": "second module is the Fast R-CNN detector [2] that uses the\nproposed regions. The entire system is a single, uni\ufb01ed net-\nwork for object detection (Fig. 2). Using the recently popular\nterminology of neural networks with \u2018attention\u2019 [31] mecha-\nnisms, the RPN module tells the Fast R-CNN module where\nto look. In Section 3.1 we introduce the designs and proper-\nties of the network for region proposal. In Section 3.2 we\ndevelop algorithms for training both modules with features\nshared.\n3.1 Region Proposal Networks\nA Region Proposal Network takes an image (of any size)\nas input and outputs a set of rectangular object proposals,\neach with an objectness score.3We model this process\nwith a fully convolutional network [7], which we describe\nin this section. Because our ultimate goal is to share com-\nputation with a Fast R-CNN object detection network [2],\nwe assume that both nets share a common set of convolu-\ntional layers. In our experiments, we investigate the\nZeiler and Fergus model [32] (ZF), which has \ufb01ve share-\nable convolutional layers and the Simonyan and Zisser-\nman model [3] (VGG-16), which has 13 shareable\nconvolutional layers.\nTo generate region proposals, we slide a small network\nover the convolutional feature map output by the last\nshared convolutional layer. This small network takes as\ninput an n/C2nspatial window of the input convolutional\nfeature map. Each sliding window is mapped to a lower-\ndimensional feature (256-d for ZF and 512-d for VGG, with\nReLU [33] following). This feature is fed into two sibling\nfully-connected layers\u2014a box-regression layer ( reg) and a\nbox-classi\ufb01cation layer ( cls). We use n\u00bc3in this paper,\nnoting that the effective receptive \ufb01eld on the input image\nis large (171 and 228 pixels for ZF and VGG, respectively).This mini-network is illustrated at a single position in\nFig. 3 (left). Note that because the mini-network operates\nin a sliding-window fashion, the fully-connected layers are\nshared across all spatial locations. This architecture is natu-\nrally implemented with an n/C2nconvolutional layer fol-\nlowed by two sibling 1/C21convolutional layers (for reg\nand cls, respectively).\n3.1.1 Anchors\nAt each sliding-window location, we simultaneously pre-\ndict multiple region proposals, where the number of maxi-\nmum possible proposals for each location is denoted as k.\nSo the reglayer has 4koutputs encoding the coordinates of\nkboxes, and the clslayer outputs 2kscores that estimate\nprobability of object or not object for each proposal.4The k\nproposals are parameterized relative tokreference boxes,\nwhich we call anchors . An anchor is centered at the sliding\nwindow in question, and is associated with a scale and\naspect ratio (Fig. 3, left). By default we use three scales and\nthree aspect ratios, yielding k\u00bc9anchors at each sliding\nposition. For a convolutional feature map of a size W/C2H\n(typically /C242;400), there are WHk anchors in total.\nTranslation-Invariant Anchors . An important property of\nour approach is that it is translation invariant , both in terms\nof the anchors and the functions that compute proposals rel-\native to the anchors. If one translates an object in an image,\nthe proposal should translate and the same function should\nbe able to predict the proposal in either location. This trans-\nlation-invariant property is guaranteed by our method.5As\na comparison, the MultiBox method [27] uses k-means to\ngenerate 800 anchors, which are nottranslation invariant. So\nMultiBox does not guarantee that the same proposal is gen-\nerated if an object is translated.\nThe translation-invariant property also reduces the\nmodel size.",
        "870c085d-0179-4765-9f2c-2fbae662ddbd": "For a convolutional feature map of a size W/C2H\n(typically /C242;400), there are WHk anchors in total.\nTranslation-Invariant Anchors . An important property of\nour approach is that it is translation invariant , both in terms\nof the anchors and the functions that compute proposals rel-\native to the anchors. If one translates an object in an image,\nthe proposal should translate and the same function should\nbe able to predict the proposal in either location. This trans-\nlation-invariant property is guaranteed by our method.5As\na comparison, the MultiBox method [27] uses k-means to\ngenerate 800 anchors, which are nottranslation invariant. So\nMultiBox does not guarantee that the same proposal is gen-\nerated if an object is translated.\nThe translation-invariant property also reduces the\nmodel size. MultiBox has a \u00f04\u00fe1\u00de/C2800-dimensional fully-\nconnected output layer, whereas our method has a\n\u00f04\u00fe2\u00de/C29-dimensional convolutional output layer in the\ncase of k\u00bc9anchors.6As a result, our output layer has\n2:8/C2104parameters ( 512/C2\u00f04\u00fe2\u00de/C29for VGG-16), two\norders of magnitude fewer than MultiBox\u2019s output layer\nthat has 6:1/C2106parameters ( 1;536/C2\u00f04\u00fe1\u00de/C2800 for\nGoogleNet [34] in MultiBox [27]). If considering the feature\nprojection layers, our proposal layers still have an order of\nmagnitude fewer parameters than MultiBox.7We expect\nour method to have less risk of over\ufb01tting on small datasets,\nlike PASCAL VOC.\nMulti-Scale Anchors as Regression References .O u rd e s i g n\nof anchors presents a novel scheme for addressing multi-\nple scales (and aspect ratios). As shown in Fig. 1, there\nhave been two popular ways for multi-scale predictions.\nThe \ufb01rst way is based on image/feature pyramids, e.g., in\nFig. 2. Faster R-CNN is a single, uni\ufb01ed network for object detection.\nThe RPN module serves as the \u2018attention\u2019 of this uni\ufb01ed network.\n3.\u201cRegion\u201d is a generic term and in this paper we only consider rect-\nangular regions, as is common for many methods (e.g., [4], [6], [27]).\n\u201cObjectness\u201d measures membership to a set of object classes\nversus background.4.For simplicity we implement the clslayer as a two-class softmax\nlayer. Alternatively, one may use logistic regression to produce k\nscores.\n5.As is the case of FCNs [7], our network is translation invariant up\nto the network\u2019s total stride.\n6.4 is the dimension of regterm for each category, and 1 or 2 is the\ndimension of clsterm of sigmoid or softmax for each category.\n7.Considering the feature projection layers, our proposal layers\u2019\nparameter count is 3/C23/C2512/C2512\u00fe512/C26/C29\u00bc2:4/C2106; Multi-\nBox\u2019s proposal layers\u2019 parameter count is\n7/C27/C2\u00f064\u00fe96\u00fe64\u00fe64\u00de/C21;536\u00fe1;536/C25/C2800\u00bc27/C2106.REN ET AL.: FASTER R-CNN: TOWARDS REAL-TIME OBJECT DETECTION WITH REGION PROPOSAL NETWORKS 1139\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "7fb3c980-4308-4e5b-bb3f-449ffe430e3d": "DPM [8] and CNN-based methods [1], [2], [9]. The images\nare resized at multiple scales, and feature maps (HOG [8]\nor deep convolutional features [1], [2], [9]) are computed\nfor each scale (Fig. 1a). This way is often useful but is\ntime-consuming. The second way is to use sliding win-\ndows of multiple scales (and/or aspect ratios) on the fea-\nture maps. For example, in DPM [8], models of different\naspect ratios are trained separately using different \ufb01lter\nsizes (such as 5/C27and 7/C25). If this way is used to\naddress multiple scales, it can be thought of as a\n\u201cpyramid of \ufb01lters\u201d (Fig. 1b). The second way is usually\nadopted jointly with the \ufb01rst way [8].\nAs a comparison, our anchor-based method is built on\na pyramid of anchors , which is more cost-ef\ufb01cient. Our\nmethod classi\ufb01es and regresses bounding boxes with ref-\nerence to anchor boxes of multiple scales and aspect\nr a t i o s .I to n l yr e l i e so ni m a g e sa n df e a t u r em a p so fas i n -\ngle scale, and uses \ufb01lters (sliding windows on the feature\nmap) of a single size. We show by experiments the effects\nof this scheme for addressing multiple scales and sizes\n(Table 8).\nBecause of this multi-scale design based on anchors, we\ncan simply use the convolutional features computed on a\nsingle-scale image, as is also done by the Fast R-CNN detec-\ntor [2]. The design of multi-scale anchors is a key compo-\nnent for sharing features without extra cost for addressing\nscales.\n3.1.2 Loss Function\nFor training RPNs, we assign a binary class label (of being\nan object or not) to each anchor. We assign a positive label\nto two kinds of anchors: (i) the anchor/anchors with the\nhighest Intersection-over-Union (IoU) overlap with a\nground-truth box, or(ii) an anchor that has an IoU overlap\nhigher than 0.7 with any ground-truth box. Note that a sin-\ngle ground-truth box may assign positive labels to multiple\nanchors. Usually the second condition is suf\ufb01cient to deter-\nmine the positive samples; but we still adopt the \ufb01rst condi-\ntion for the reason that in some rare cases the second\ncondition may \ufb01nd no positive sample. We assign a nega-\ntive label to a non-positive anchor if its IoU ratio is lower\nthan 0.3 for all ground-truth boxes. Anchors that are neither\npositive nor negative do not contribute to the training\nobjective.With these de\ufb01nitions, we minimize an objective function\nfollowing the multi-task loss in Fast R-CNN [2]. Our loss\nfunction for an image is de\ufb01ned as\nL\u00f0fpig;ftig\u00de \u00bc1\nNclsX\niLcls\u00f0pi;p/C3\ni\u00de\n\u00fe/C211\nNregX\nip/C3\niLreg\u00f0ti;t/C3\ni\u00de:(1)\nHere, iis the index of an anchor in a mini-batch and piis the\npredicted probability of anchor ibeing an object. The\nground-truth label p/C3\niis 1 if the anchor is positive, and is 0 if\nthe anchor is negative. tiis a vector representing the 4\nparameterized coordinates of the predicted bounding box,\nandt/C3\niis that of the ground-truth box associated with a posi-\ntive anchor. The classi\ufb01cation loss Lclsis log loss over two\nclasses (object versus not object). For the regression loss, we\nuseLreg\u00f0ti;t/C3\ni\u00de\u00bcR\u00f0ti/C0t/C3\ni\u00dewhere Ris the robust loss func-\ntion (smooth L1) de\ufb01ned in [2]. The term p/C3\niLregmeans the\nregression loss is activated only for positive anchors\n(p/C3\ni\u00bc1) and is disabled otherwise ( p/C3\ni\u00bc0). The outputs of\ntheclsandreglayers consist of fpigandftigrespectively.",
        "a606df13-973b-470f-8e9b-c45dd9d78e57": "tiis a vector representing the 4\nparameterized coordinates of the predicted bounding box,\nandt/C3\niis that of the ground-truth box associated with a posi-\ntive anchor. The classi\ufb01cation loss Lclsis log loss over two\nclasses (object versus not object). For the regression loss, we\nuseLreg\u00f0ti;t/C3\ni\u00de\u00bcR\u00f0ti/C0t/C3\ni\u00dewhere Ris the robust loss func-\ntion (smooth L1) de\ufb01ned in [2]. The term p/C3\niLregmeans the\nregression loss is activated only for positive anchors\n(p/C3\ni\u00bc1) and is disabled otherwise ( p/C3\ni\u00bc0). The outputs of\ntheclsandreglayers consist of fpigandftigrespectively.\nThe two terms are normalized by Nclsand Nregand\nweighted by a balancing parameter /C21. In our current imple-\nmentation (as in the released code), the clsterm in Eqn.(1) is\nnormalized by the mini-batch size ( i.e.,Ncls\u00bc256) and the\nregterm is normalized by the number of anchor locations (\ni.e.,Nreg/C242;400). By default we set /C21\u00bc10, and thus both\nclsandregterms are roughly equally weighted. We show by\nexperiments that the results are insensitive to the values of\n/C21in a wide range (Table 9). We also note that the normaliza-\ntion as above is not required and could be simpli\ufb01ed.\nFor bounding box regression, we adopt the parameter-\nizations of the four coordinates following [5]:\ntx\u00bc\u00f0x/C0xa\u00de=wa;ty\u00bc\u00f0y/C0ya\u00de=ha;\ntw\u00bclog\u00f0w=w a\u00de;th\u00bclog\u00f0h=ha\u00de;\nt/C3x\u00bc\u00f0x/C3/C0xa\u00de=wa;t/C3y\u00bc\u00f0y/C3/C0ya\u00de=ha;\nt/C3w\u00bclog\u00f0w/C3=wa\u00de;t/C3\nh\u00bclog\u00f0h/C3=ha\u00de;(3)\nwhere x,y,w, and hdenote the box\u2019s center coordinates and\nits width and height. Variables x,xa, and x/C3are for the pre-\ndicted box, anchor box, and ground-truth box respectively\nFig. 3. Left: Region Proposal Network (RPN). Right : Example detections using RPN proposals on PASCAL VOC 2007 test. Our method detects\nobjects in a wide range of scales and aspect ratios.1140 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 6, JUNE 2017\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "9deb4fdd-fb8c-4a24-b0a6-99613a54fab8": "(likewise for y; w; h ). This can be thought of as bounding-\nbox regression from an anchor box to a nearby ground-truth\nbox.\nNevertheless, our method achieves bounding-box regres-\nsion by a different manner from previous RoI-based (Region\nof Interest) methods [1], [2]. In [1], [2], bounding-box regres-\nsion is performed on features pooled from arbitrarily sized\nRoIs, and the regression weights are shared by all region\nsizes. In our formulation, the features used for regression\nare of the same spatial size ( 3/C23) on the feature maps. To\naccount for varying sizes, a set of kbounding-box regressors\nare learned. Each regressor is responsible for one scale and\none aspect ratio, and the kregressors do notshare weights.\nAs such, it is still possible to predict boxes of various sizes\neven though the features are of a \ufb01xed size/scale, thanks to\nthe design of anchors.\n3.1.3 Training RPNs\nThe RPN can be trained end-to-end by back-propagation\nand stochastic gradient descent (SGD) [35]. We follow the\n\u201cimage-centric\u201d sampling strategy from [2] to train this net-\nwork. Each mini-batch arises from a single image that con-\ntains many positive and negative example anchors. It is\npossible to optimize for the loss functions of all anchors, but\nthis will bias towards negative samples as they are domi-\nnate. Instead, we randomly sample 256 anchors in an image\nto compute the loss function of a mini-batch, where the sam-\npled positive and negative anchors have a ratio of up to 1:1.\nIf there are fewer than 128 positive samples in an image, we\npad the mini-batch with negative ones.\nWe randomly initialize all new layers by drawing\nweights from a zero-mean Gaussian distribution with stan-\ndard deviation 0.01. All other layers (i.e., the shared convo-\nlutional layers) are initialized by pre-training a model for\nImageNet classi\ufb01cation [36], as is standard practice [5]. We\ntune all layers of the ZF net, and conv 31and up for the\nVGG net to conserve memory [2]. We use a learning rate of\n0.001 for 60 k mini-batches, and 0.0001 for the next 20k\nmini-batches on the PASCAL VOC dataset. We use a\nmomentum of 0.9 and a weight decay of 0.0005 [37]. Our\nimplementation uses Caffe [38].\n3.2 Sharing Features for RPN and Fast R-CNN\nThus far we have described how to train a network for\nregion proposal generation, without considering the region-\nbased object detection CNN that will utilize these proposals.\nFor the detection network, we adopt Fast R-CNN [2]. Next\nwe describe algorithms that learn a uni\ufb01ed network com-\nposed of RPN and Fast R-CNN with shared convolutional\nlayers (Fig. 2).\nBoth RPN and Fast R-CNN, trained independently, will\nmodify their convolutional layers in different ways. We\ntherefore need to develop a technique that allows for shar-\ning convolutional layers between the two networks, rather\nthan learning two separate networks. We discuss three\nways for training networks with features shared:\n(i)Alternating Training . In this solution, we \ufb01rst train\nRPN, and use the proposals to train Fast R-CNN. The net-\nwork tuned by Fast R-CNN is then used to initialize RPN,\nand this process is iterated. This is the solution that is used\nin all experiments in this paper.(ii)Approximate Joint Training . In this solution, the RPN\nand Fast R-CNN networks are merged into one network\nduring training as in Fig. 2. In each SGD iteration, the for-\nward pass generates region proposals which are treated just\nlike \ufb01xed, pre-computed proposals when training a Fast R-\nCNN detector. The backward propagation takes place as\nusual, where for the shared layers the backward propagated\nsignals from both the RPN loss and the Fast R-CNN loss are\ncombined. This solution is easy to implement. But this solu-\ntion ignores the derivative w.r.t the proposal boxes\u2019 coordi-\nnates that are also network responses, so is approximate.",
        "8e4ef6f9-7bf9-4a10-959b-33a7d6dbb29c": "The net-\nwork tuned by Fast R-CNN is then used to initialize RPN,\nand this process is iterated. This is the solution that is used\nin all experiments in this paper.(ii)Approximate Joint Training . In this solution, the RPN\nand Fast R-CNN networks are merged into one network\nduring training as in Fig. 2. In each SGD iteration, the for-\nward pass generates region proposals which are treated just\nlike \ufb01xed, pre-computed proposals when training a Fast R-\nCNN detector. The backward propagation takes place as\nusual, where for the shared layers the backward propagated\nsignals from both the RPN loss and the Fast R-CNN loss are\ncombined. This solution is easy to implement. But this solu-\ntion ignores the derivative w.r.t the proposal boxes\u2019 coordi-\nnates that are also network responses, so is approximate. In\nour experiments, we have empirically found this solver pro-\nduces close results (mAP 70.0 percent compared with 69.9\npercent of alternating training reported in Table 3), yet\nreduces the training time by about 25-50 percent comparing\nwith alternating training. This solver is included in our\nreleased Python code.\n(iii)Non-Approximate Joint Training . As discussed above,\nthe bounding boxes predicted by RPN are also functions of\nthe input. The RoI pooling layer [2] in Fast R-CNN accepts\nthe convolutional features and also the predicted bounding\nboxes as input, so a theoretically valid backpropagation\nsolver should also involve gradients w.r.t the box coordi-\nnates. These gradients are ignored in the above approximate\njoint training. In a non-approximate joint training solution,\nwe need an RoI pooling layer that is differentiable w.r.t the\nbox coordinates. This is a nontrivial problem and a solution\ncan be given by an \u201cRoI warping\u201d layer as developed in\n[15], which is beyond the scope of this paper.\nFour-Step Alternating Training . In this paper, we adopt a\npragmatic four-step training algorithm to learn shared fea-\ntures via alternating optimization. In the \ufb01rst step, we train\nthe RPN as described in Section 3.1.3. This network is initial-\nized with an ImageNet-pre-trained model and \ufb01ne-tuned\nend-to-end for the region proposal task. In the second step,\nwe train a separate detection network by Fast R-CNN using\nthe proposals generated by the step-1 RPN. This detection\nnetwork is also initialized by the ImageNet-pre-trained\nmodel. At this point the two networks do not share convolu-\ntional layers. In the third step, we use the detector network\nto initialize RPN training, but we \ufb01x the shared convolu-\ntional layers and only \ufb01ne-tune the layers unique to RPN.\nNow the two networks share convolutional layers. Finally,\nkeeping the shared convolutional layers \ufb01xed, we \ufb01ne-tune\nthe unique layers of Fast R-CNN. As such, both networks\nshare the same convolutional layers and form a uni\ufb01ed net-\nwork. A similar alternating training can be run for more\niterations, but we have observed negligible improvements.\n3.3 Implementation Details\nWe train and test both region proposal and object detection\nnetworks on images of a single scale [1], [2]. We re-scale the\nimages such that their shorter side is s\u00bc600pixels [2].\nMulti-scale feature extraction (using an image pyramid)\nmay improve accuracy but does not exhibit a good speed-\naccuracy trade-off [2]. On the re-scaled images, the total\nstride for both ZF and VGG nets on the last convolutional\nlayer is 16 pixels, and thus is /C2410 pixels on a typical PAS-\nCAL image before resizing ( /C24500/C2375). Even such a large\nstride provides good results, though accuracy may be fur-\nther improved with a smaller stride.REN ET AL.: FASTER R-CNN: TOWARDS REAL-TIME OBJECT DETECTION WITH REGION PROPOSAL NETWORKS 1141\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "74123ac3-9c0e-4d81-aeec-05e28a1977c4": "For anchors, we use three scales with box areas of 1282,\n2562, and 5122pixels, and three aspect ratios of 1:1, 1:2, and\n2:1. These hyper-parameters are notcarefully chosen for a\nparticular dataset, and we provide ablation experiments on\ntheir effects in the next section. As discussed, our solution\ndoes not need an image pyramid or \ufb01lter pyramid to predict\nregions of multiple scales, saving considerable running\ntime. Fig. 3 (right) shows the capability of our method for a\nwide range of scales and aspect ratios. Table 1 shows the\nlearned average proposal size for each anchor using the ZF\nnet. We note that our algorithm allows predictions that are\nlarger than the underlying receptive \ufb01eld. Such predictions\nare not impossible\u2014one may still roughly infer the extent of\nan object if only the middle of the object is visible.\nThe anchor boxes that cross image boundaries need to be\nhandled with care. During training, we ignore all cross-\nboundary anchors so they do not contribute to the loss. For\na typical 1;000/C2600image, there will be roughly 20,000\n(/C2560/C240/C29) anchors in total. With the cross-boundary\nanchors ignored, there are about 6,000 anchors per image\nfor training. If the boundary-crossing outliers are not\nignored in training, they introduce large, dif\ufb01cult to correct\nerror terms in the objective, and training does not converge.\nDuring testing, however, we still apply the fully convolu-\ntional RPN to the entire image. This may generate cross-\nboundary proposal boxes, which we clip to the image\nboundary.\nSome RPN proposals highly overlap with each other. To\nreduce redundancy, we adopt non-maximum suppression\n(NMS) on the proposal regions based on their clsscores. We\n\ufb01x the IoU threshold for NMS at 0.7, which leaves us about\n2,000 proposal regions per image. As we will show, NMS\ndoes not harm the ultimate detection accuracy, but substan-\ntially reduces the number of proposals. After NMS, we use\nthe top- Nranked proposal regions for detection. In the fol-\nlowing, we train Fast R-CNN using 2,000 RPN proposals,\nbut evaluate different numbers of proposals at test-time.\n4E XPERIMENTS\n4.1 Experiments on PASCAL VOC\nWe comprehensively evaluate our method on the PASCAL\nVOC 2007 detection benchmark [11]. This dataset consists of\nabout 5k trainval images and 5k test images over 20 object\ncategories. We also provide results on the PASCAL VOC\n2012 benchmark for a few models. For the ImageNet pre-\ntrained network, we use the \u201cfast\u201d version of ZF net [32]\nthat has \ufb01ve convolutional layers and three fully-connected\nlayers, and the public VGG-16 model8[3] that has 13 convo-\nlutional layers and three fully-connected layers. We primar-\nily evaluate detection mean Average Precision (mAP),\nbecause this is the actual metric for object detection (rather\nthan focusing on object proposal proxy metrics).Table 2 (top) shows Fast R-CNN results when trained\nand tested using various region proposal methods. These\nresults use the ZF net. For Selective Search (SS) [4], we gen-\nerate about 2,000 proposals by the \u201cfast\u201d mode. For Edge-\nBoxes (EB) [6], we generate the proposals by the default EB\nsetting tuned for 0.7 IoU. SS has an mAP of 58.7 percent and\nEB has an mAP of 58.6 percent under the Fast R-CNN\nframework. RPN with Fast R-CNN achieves competitive\nresults, with an mAP of 59.9 percent while using up to 300\nproposals.9Using RPN yields a much faster detection sys-\ntem than using either SS or EB because of shared convolu-\ntional computations; the fewer proposals also reduce the\nregion-wise fully-connected layers\u2019 cost (Table 5).\nAblation Experiments on RPN. To investigate the behavior\nof RPNs as a proposal method, we conducted several abla-\ntion studies. First, we show the effect of sharing convolu-\ntional layers between the RPN and Fast R-CNN detection\nnetwork.",
        "b57119a9-3197-44be-b12e-5bda13be3681": "SS has an mAP of 58.7 percent and\nEB has an mAP of 58.6 percent under the Fast R-CNN\nframework. RPN with Fast R-CNN achieves competitive\nresults, with an mAP of 59.9 percent while using up to 300\nproposals.9Using RPN yields a much faster detection sys-\ntem than using either SS or EB because of shared convolu-\ntional computations; the fewer proposals also reduce the\nregion-wise fully-connected layers\u2019 cost (Table 5).\nAblation Experiments on RPN. To investigate the behavior\nof RPNs as a proposal method, we conducted several abla-\ntion studies. First, we show the effect of sharing convolu-\ntional layers between the RPN and Fast R-CNN detection\nnetwork. To do this, we stop after the second step in the\nfour-step training process. Using separate networks reduces\nthe result slightly to 58.7 percent (RPN+ZF, unshared,\nTable 2). We observe that this is because in the third step\nwhen the detector-tuned features are used to \ufb01ne-tune the\nRPN, the proposal quality is improved.\nNext, we disentangle the RPN\u2019s in\ufb02uence on training the\nFast R-CNN detection network. For this purpose, we train a\nFast R-CNN model by using the 2,000 SS proposals and ZF\nnet. We \ufb01x this detector and evaluate the detection mAP by\nchanging the proposal regions used at test-time. In these\nablation experiments, the RPN does not share features with\nthe detector.\nReplacing SS with 300 RPN proposals at test-time leads\nto an mAP of 56.8 percent. The loss in mAP is because of the\ninconsistency between the training/testing proposals. This\nresult serves as the baseline for the following comparisons.\nSomewhat surprisingly, the RPN still leads to a competi-\ntive result (55.1 percent) when using the top-ranked 100\nproposals at test-time, indicating that the top-ranked RPN\nproposals are accurate. On the other extreme, using the top-\nranked 6,000 RPN proposals (without NMS) has a compara-\nble mAP (55.2 percent), suggesting NMS does not harm the\ndetection mAP and may reduce false alarms.\nNext, we separately investigate the roles of RPN\u2019s clsand\nregoutputs by turning off either of them at test-time. When\ntheclslayer is removed at test-time (thus no NMS/ranking\nis used), we randomly sample Nproposals from the\nunscored regions. The mAP is nearly unchanged with\nN\u00bc1;000(55.8 percent), but degrades considerably to 44.6\npercent when N\u00bc100. This shows that the clsscores\naccount for the accuracy of the highest ranked proposals.\nOn the other hand, when the reglayer is removed at test-\ntime (so the proposals become anchor boxes), the mAPTABLE 1\nThe Learned Average Proposal Size for Each Anchor Using the ZF Net (Numbers for s\u00bc600)\nanchor 1282, 2:1 1282, 1:1 1282, 1:2 2562, 2:1 2562, 1:1 2562, 1:2 5122, 2:1 5122, 1:1 5122, 1:2\nproposal 188/C2111 113 /C2114 70 /C292 416 /C2229 261 /C2284 174 /C2332 768 /C2437 499 /C2501 355 /C2715\n8.www.robots.ox.ac.uk/ /C24vgg/research/very_deep/.9.For RPN, the number of proposals (e.g., 300) is the maximum\nnumber for an image. RPN may produce fewer proposals after NMS,\nand thus the average number of proposals is smaller.1142 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 6, JUNE 2017\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "4ea1c1ee-ba77-4459-beb3-68e9f3307b16": "drops to 52.1 percent. This suggests that the high-quality\nproposals are mainly due to the regressed box bounds. The\nanchor boxes, though having multiple scales and aspect\nratios, are not suf\ufb01cient for accurate detection.\nWe also evaluate the effects of more powerful networks\non the proposal quality of RPN alone. We use VGG-16 to\ntrain the RPN, and still use the above detector of SS+ZF.\nThe mAP improves from 56.8 percent (using RPN+ZF) to\n59.2 percent (using RPN+VGG). This is a promising result,\nbecause it suggests that the proposal quality of RPN+VGG\nis better than that of RPN+ZF. Because proposals of RPN\n+ZF are competitive with SS (both are 58.7 percent when\nconsistently used for training and testing), we may expect\nRPN+VGG to be better than SS. The following experiments\njustify this hypothesis.\nPerformance of VGG-16. Table 3 shows the results of VGG-\n16 for both proposal and detection. Using RPN+VGG, the\nresult is 68.5 percent for unshared features, slightly higher\nthan the SS baseline. As shown above, this is because the\nproposals generated by RPN+VGG are more accurate than\nSS. Unlike SS that is pre-de\ufb01ned, the RPN is actively trained\nand bene\ufb01ts from better networks. For the feature- shared\nvariant, the result is 69.9 percent\u2014better than the strong SS\nbaseline, yet with nearly cost-free proposals. We furthertrain the RPN and detection network on the union set of\nPASCAL VOC 2007 trainval and 2012 trainval. The mAP is\n73.2 percent . Fig. 5 shows some results on the PASCAL\nVOC 2007 test set. On the PASCAL VOC 2012 test set\n(Table 4), our method has an mAP of 70.4 percent trained\non the union set of VOC 2007 trainval+test and VOC 2012\ntrainval. Tables 6 and 7 show the detailed numbers.\nIn Table 5 we summarize the running time of the entire\nobject detection system. SS takes 1-2 seconds depending on\ncontent (on average about 1.5 s), and Fast R-CNN with\nVGG-16 takes 320 ms on 2,000 SS proposals (or 223 ms if\nusing SVD on fully-connected layers [2]). Our system with\nVGG-16 takes in total 198 ms for both proposal and detec-\ntion. With the convolutional features shared, the RPN alone\nonly takes 10 ms computing the additional layers. Our\nregion-wise computation is also lower, thanks to fewer pro-\nposals (300 per image). Our system has a frame-rate of 17\nfps with the ZF net.\nSensitivities to Hyper-Parameters. In Table 8 we investigate\nthe settings of anchors. By default we use three scales and\nthree aspect ratios (69.9 percent mAP in Table 8). If using\njust one anchor at each position, the mAP drops by a consid-\nerable margin of 3-4 percent.",
        "8d51d153-db7b-4254-af00-ab5f22d2ffe2": "Our system with\nVGG-16 takes in total 198 ms for both proposal and detec-\ntion. With the convolutional features shared, the RPN alone\nonly takes 10 ms computing the additional layers. Our\nregion-wise computation is also lower, thanks to fewer pro-\nposals (300 per image). Our system has a frame-rate of 17\nfps with the ZF net.\nSensitivities to Hyper-Parameters. In Table 8 we investigate\nthe settings of anchors. By default we use three scales and\nthree aspect ratios (69.9 percent mAP in Table 8). If using\njust one anchor at each position, the mAP drops by a consid-\nerable margin of 3-4 percent. The mAP is higher if using\nthree scales (with 1 aspect ratio) or three aspect ratios (withTABLE 2\nDetection Results on PASCAL VOC 2007 Test Set (Trained on VOC 2007 Trainval)\ntrain-time region proposals test-time region proposals\nmethod # boxes method # proposals mAP (%)\nSS 2,000 SS 2,000 58.7\nEB 2,000 EB 2,000 58.6\nRPN+ZF, shared 2,000 RPN+ZF, shared 300 59.9\nablation experiments follow below\nRPN+ZF, unshared 2,000 RPN+ZF, unshared 300 58.7\nSS 2,000 RPN+ZF 100 55.1\nSS 2,000 RPN+ZF 300 56.8\nSS 2,000 RPN+ZF 1,000 56.3\nSS 2,000 RPN+ZF (no NMS) 6,000 55.2\nSS 2,000 RPN+ZF (no cls) 100 44.6\nSS 2,000 RPN+ZF (no cls) 300 51.4\nSS 2,000 RPN+ZF (no cls) 1,000 55.8\nSS 2,000 RPN+ZF (no reg) 300 52.1\nSS 2,000 RPN+ZF (no reg) 1,000 51.3\nSS 2,000 RPN+VGG 300 59.2\nThe detectors are Fast R-CNN with ZF, but using various proposal methods for training and testing.\nTABLE 3\nDetection Results on PASCAL VOC 2007 Test Set\nmethod # proposals data mAP (%)\nSS 2000 07 66:9/C14\nSS 2000 07+12 70.0\nRPN+VGG, unshared 300 07 68.5\nRPN+VGG, shared 300 07 69.9\nRPN+VGG, shared 300 07+12 73.2\nRPN+VGG, shared 300 COCO+07+12 78.8\nThe detector is Fast R-CNN and VGG-16. training data: \u201c07\u201d: VOC 2007\ntrainval, \u201c07+12\u201d: union set of VOC 2007 trainval and VOC 2012 trainval.\nFor RPN, the train-time proposals for Fast R-CNN are 2000./C14: this number\nwas reported in [2]; using the repository provided by this paper, this result is\nhigher (68.1).TABLE 4\nDetection Results on PASCAL VOC 2012 Test Set\nmethod # proposals data mAP (%)\nSS 2,000 12 65.7\nSS 2,000 07++12 68.4\nRPN+VGG, sharedy300 12 67.0\nRPN+VGG, sharedz300 07++12 70.4\nRPN+VGG, sharedx300 COCO+07++12 75.9\nThe detector is Fast R-CNN and VGG-16. Training data: \u201c07\u201d: VOC 2007\ntrainval, \u201c07++12\u201d: union set of VOC 2007 trainval+test and VOC 2012\ntrainval. For RPN, the train-time proposals for Fast R-CNN are 2000. y\nhttp://host.robots.ox.ac.uk:8080/anonymous/HZJTQA.html. z\nhttp://host.robots.ox.ac.uk:8080/anonymous/YNPLXB.html.",
        "fc831167-d308-4693-9c0f-2ac64738ce4d": "Training data: \u201c07\u201d: VOC 2007\ntrainval, \u201c07++12\u201d: union set of VOC 2007 trainval+test and VOC 2012\ntrainval. For RPN, the train-time proposals for Fast R-CNN are 2000. y\nhttp://host.robots.ox.ac.uk:8080/anonymous/HZJTQA.html. z\nhttp://host.robots.ox.ac.uk:8080/anonymous/YNPLXB.html. x\nhttp://host.robots.ox.ac.uk:8080/anonymous/XEDH10.html.REN ET AL.: FASTER R-CNN: TOWARDS REAL-TIME OBJECT DETECTION WITH REGION PROPOSAL NETWORKS 1143\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "d010d848-e7ee-474d-a3a2-1b44684c5222": "1 scale), demonstrating that using anchors of multiple sizes\nas the regression references is an effective solution. Using\njust three scales with one aspect ratio (69.8 percent) is as\ngood as using three scales with three aspect ratios on this\ndataset, suggesting that scales and aspect ratios are not dis-entangled dimensions for the detection accuracy. But we\nstill adopt these two dimensions in our designs to keep our\nsystem \ufb02exible.\nIn Table 9 we compare different values of /C21in Equa-\ntion (1). By default we use /C21\u00bc10which makes the two\nterms in Equation (1) roughly equally weighted after nor-\nmalization. Table 9 shows that our result is impacted just\nmarginally (by /C241) percent when /C21is within a scale of\nabout two orders of magnitude (1 to 100). This demonstrates\nthat the result is insensitive to /C21in a wide range.\nIn Table 10 we investigate the numbers of proposals in\ntesting.\nAnalysis of Recall-to-IoU. Next we compute the recall of\nproposals at different IoU ratios with ground-truth boxes. It\nis noteworthy that the Recall-to-IoU metric is just loosely\n[19], [20], [21] related to the ultimate detection accuracy. It is\nmore appropriate to use this metric to diagnose the proposal\nmethod than to evaluate it.\nIn Fig. 4, we show the results of using 300, 1,000, and\n2,000 proposals. We compare with SS, EB and MCG, andTABLE 5\nTiming (ms) on a K40 GPU, Except SS Proposal Is Evaluated in a CPU\nmodel system conv proposal region-wise total rate\nVGG SS + Fast R-CNN 146 1,510 174 1,830 0.5 fps\nVGG RPN + Fast R-CNN 141 10 47 198 5 fps\nZF RPN + Fast R-CNN 31 3 25 59 17 fps\n\u201cRegion-wise\u201d includes NMS, pooling, fully-connected, and softmax layers. See our released code for the pro\ufb01ling of running time.\nTABLE 7\nResults on PASCAL VOC 2012 Test Set with Fast R-CNN Detectors and VGG-16\nmethod # box data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nSS 2000 12 65.7 80.3 74.7 66.9 46.9 37.7 73.9 68.6 87.7 41.7 71.1 51.1 86.0 77.8 79.8 69.8 32.1 65.5 63.8 76.4 61.7\nSS 2000 07++12 68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65:780.4 64.2\nRPN 300 12 67.0 82.3 76.4 71.0 48.4 45.2 72.1 72.3 87.3 42.2 73.7 50.0 86.8 78.7 78.4 77.4 34.5 70.1 57.1 77.1 58.9\nRPN 300 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nRPN 300 COCO+07++12 75:987:483:676:862:959:681:982:091:354:982:659:089:085:584:7 84:1 52:278:965.5 85:470:2\nFor RPN, the train-time proposals for Fast R-CNN are 2,000.",
        "32d17fd3-efcd-40e6-a393-01bc0af95d1a": "4 77.4 34.5 70.1 57.1 77.1 58.9\nRPN 300 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nRPN 300 COCO+07++12 75:987:483:676:862:959:681:982:091:354:982:659:089:085:584:7 84:1 52:278:965.5 85:470:2\nFor RPN, the train-time proposals for Fast R-CNN are 2,000.TABLE 6\nResults on PASCAL VOC 2007 Test Set with Fast R-CNN Detectors and VGG-16\nmethod # box data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nSS 2,000 07 66.9 74.5 78.3 69.2 53.2 36.6 77.3 78.2 82.0 40.7 72.7 67.9 79.6 79.2 73.0 69.0 30.1 65.4 70.2 75.8 65.8\nSS 2,000 07+12 70.0 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4\nRPN* 300 07 68.5 74.1 77.2 67.7 53.9 51.0 75.1 79.2 78.9 50.7 78.0 61.1 79.1 81.9 72.2 75.9 37.2 71.4 62.5 77.4 66.4\nRPN 300 07 69.9 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6\nRPN 300 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\nRPN 300 COCO+07+12 78:884:382:077:768:965:788:188:488:963:686:370:885:987:6 80:1 82:3 53:680:475:886:678:9\nFor RPN, the train-time proposals for Fast R-CNN are 2,000. RPN* denotes the unsharing feature version.",
        "6a10ad7f-afdb-4e38-9bcc-8252cb65d638": "3 67.3 81.1 67.6\nRPN 300 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\nRPN 300 COCO+07+12 78:884:382:077:768:965:788:188:488:963:686:370:885:987:6 80:1 82:3 53:680:475:886:678:9\nFor RPN, the train-time proposals for Fast R-CNN are 2,000. RPN* denotes the unsharing feature version.\nTABLE 8\nDetection Results of Faster R-CNN on PASCAL VOC 2007\nTest Set Using Different Settings of Anchors\nsettings anchor scales aspect ratios mAP (%)\n1 scale, 1 ratio 1282 1:1 65.8\n2562 1:1 66.7\n1 scale, 3 ratios 1282 {2:1, 1:1, 1:2} 68.8\n2562 {2:1, 1:1, 1:2} 67.9\n3 scales, 1 ratios f1282;2562;5122g 1:1 69.8\n3 scales, 3 ratios f1282;2562;5122g{2:1, 1:1, 1:2} 69.9\nThe network is VGG-16. The training data is VOC 2007 trainval. The default\nsetting of using three scales and three aspect ratios (69.9 percent ) is the same\nas that in Table 3.\nTABLE 9\nDetection Results of Faster R-CNN on PASCAL VOC 2007\nTest Set Using Different Values of /C21in Equation (1)\n/C21 0.1 1 10 100\nmAP (%) 67.2 68.9 69.9 69.1\nThe network is VGG-16. The training data is VOC 2007 trainval. The default\nsetting of using /C21\u00bc10(69.9 percent) is the same as that in Table 3.TABLE 10\nDetection Results of Faster R-CNN on PASCAL VOC 2007\nTest Set Using Different Numbers of Proposals in Testing\n# proposals 50 100 150 200 300 500 1000\nmAP (%) 66.3 68.9 69.5 69.8 69.9 69.8 69.8\nThe network is VGG-16. The training data is VOC 2007 trainval. The default\nsetting of using 300 proposals is the same as that in Table 3.1144 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 6, JUNE 2017\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "1393ac3c-d923-4a2c-8251-37371ffb10b4": "Fig. 4. Recall versus IoU overlap ratio on the PASCAL VOC 2007 test set.\nFig. 5. Selected examples of object detection results on the PASCAL VOC 2007 test set using the Faster R-CNN system. The model is VGG-16 and\nthe training data is 07+12 trainval (73.2 percent mAP on the 2007 test set). Our method detects objects of a wide range of scales and aspect ratios.\nEach output box is associated with a category label and a softmax score in \u00bd0;1/C138. A score threshold of 0.6 is used to display these images. The run-\nning time for obtaining these results is 198 ms per image, including all steps .REN ET AL.: FASTER R-CNN: TOWARDS REAL-TIME OBJECT DETECTION WITH REGION PROPOSAL NETWORKS 1145\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "7831e7ff-57f1-438d-92d5-133bb6b73972": "theNproposals are the top- Nranked ones based on the\ncon\ufb01dence generated by these methods. The plots show\nthat the RPN method behaves gracefully when the number\nof proposals drops from 2,000 to 300. This explains why the\nRPN has a good ultimate detection mAP when using as few\nas 300 proposals. As we analyzed before, this property is\nmainly attributed to the clsterm of the RPN. The recall of\nSS, EB and MCG drops more quickly than RPN when the\nproposals are fewer.\nOne-Stage Detection Versus Two-Stage Proposal + Detection.\nThe OverFeat paper [9] proposes a detection method that\nuses regressors and classi\ufb01ers on sliding windows over con-\nvolutional feature maps. OverFeat is a one-stage ,class-speci\ufb01c\ndetection pipeline, and ours is a two-stage cascade consisting\nof class-agnostic proposals and class-speci\ufb01c detections. In\nOverFeat, the region-wise features come from a sliding win-\ndow of one aspect ratio over a scale pyramid. These features\nare used to simultaneously determine the location and cate-\ngory of objects. In RPN, the features are from square ( 3/C23)\nsliding windows and predict proposals relative to anchors\nwith different scales and aspect ratios. Though both meth-\nods use sliding windows, the region proposal task is only\nthe \ufb01rst stage of Faster R-CNN\u2014the downstream Fast R-\nCNN detector attends to the proposals to re\ufb01ne them. In the\nsecond stage of our cascade, the region-wise features\nare adaptively pooled [1], [2] from proposal boxes that more\nfaithfully cover the features of the regions. We believe these\nfeatures lead to more accurate detections.\nTo compare the one-stage and two-stage systems, we emu-\nlatethe OverFeat system (and thus also circumvent other dif-\nferences of implementation details) by one-stage Fast R-CNN.\nIn this system, the \u201cproposals\u201d are dense sliding windows of\nthree scales (128, 256, 512) and three aspect ratios (1:1, 1:2,\n2:1). Fast R-CNN is trained to predict class-speci\ufb01c scores and\nregress box locations from these sliding windows. Because\nthe OverFeat system adopts an image pyramid, we also evalu-\nate using convolutional features extracted from \ufb01ve scales.\nWe use those \ufb01ve scales as in [1], [2].\nTable 11 compares the two-stage system and two var-\niants of the one-stage system. Using the ZF model, the one-\nstage system has an mAP of 53.9 percent. This is lower thanthe two-stage system (58.7 percent) by 4.8 percent. This\nexperiment justi\ufb01es the effectiveness of cascaded region\nproposals and object detection. Similar observations are\nreported in [2], [39], where replacing SS region proposals\nwith sliding windows leads to /C246 percent degradation in\nboth papers. We also note that the one-stage system is\nslower as it has considerably more proposals to process.\n4.2 Experiments on MS COCO\nWe present more results on the Microsoft COCO object detec-\ntion dataset [12]. This dataset involves 80 object categories.\nWe experiment with the 80 k images on the training set, 40 k\nimages on the validation set, and 20 k images on the test-dev\nset. We evaluate the mAP averaged for IoU 2\u00bd0:5:0:05 : 0 :95/C138\n(COCO\u2019s standard metric, simply denoted as mAP@[.5, .95])\nand mAP@0.5 (PASCAL VOC\u2019s metric).\nThere are a few minor changes of our system made for\nthis dataset. We train our models on an 8-GPU implementa-\ntion, and the effective mini-batch size becomes eight for\nRPN (1 per GPU) and 16 for Fast R-CNN (2 per GPU). The\nRPN step and Fast R-CNN step are both trained for 240 k\niterations with a learning rate of 0.003 and then for 80k itera-\ntions with 0.0003. We modify the learning rates (starting\nwith 0.003 instead of 0.001) because the mini-batch size is\nchanged.",
        "9ada469b-0250-4f68-ad80-df322a2b2dfd": "There are a few minor changes of our system made for\nthis dataset. We train our models on an 8-GPU implementa-\ntion, and the effective mini-batch size becomes eight for\nRPN (1 per GPU) and 16 for Fast R-CNN (2 per GPU). The\nRPN step and Fast R-CNN step are both trained for 240 k\niterations with a learning rate of 0.003 and then for 80k itera-\ntions with 0.0003. We modify the learning rates (starting\nwith 0.003 instead of 0.001) because the mini-batch size is\nchanged. For the anchors, we use three aspect ratios and\nfour scales (adding 642), mainly motivated by handling\nsmall objects on this dataset. In addition, in our Fast R-CNN\nstep, the negative samples are de\ufb01ned as those with a maxi-\nmum IoU with ground truth in the interval of \u00bd0;0:5\u00de,\ninstead of \u00bd0:1;0:5\u00deused in [1], [2]. We note that in the\nSPPnet system [1], the negative samples in \u00bd0:1;0:5\u00deare\nused for network \ufb01ne-tuning, but the negative samples in\n\u00bd0;0:5\u00deare still visited in the SVM step with hard-negative\nmining. But the Fast R-CNN system [2] abandons the SVM\nstep, so the negative samples in \u00bd0;0:1\u00deare never visited.\nIncluding these \u00bd0;0:1\u00desamples improves mAP@0.5 on the\nCOCO dataset for both Fast R-CNN and Faster R-CNN sys-\ntems (but the impact is negligible on PASCAL VOC).\nThe rest of the implementation details are the same as on\nPASCAL VOC. In particular, we keep using 300 proposalsTABLE 11\nOne-Stage Detection versus Two-Stage Proposal + Detection\nproposals detector mAP (%)\nTwo-Stage RPN + ZF, unshared 300 Fast R-CNN + ZF, 1 scale 58.7\nOne-Stage dense, 3 scales, 3 aspect ratios 20,000 Fast R-CNN + ZF, 1 scale 53.8\nOne-Stage dense, 3 scales, 3 aspect ratios 20,000 Fast R-CNN + ZF, 5 scales 53.9\nDetection results are on the PASCAL VOC 2007 test set using the ZF model and Fast R-CNN. RPN uses unshared features.\nTABLE 12\nObject Detection Results (Percent) on the MS COCO Dataset\nmethod proposals training dataCOCO val COCO test-dev\nmAP@.5 mAP@ mAP@.5 mAP@\nFast R-CNN [2] SS, 2000 COCO train - - 35.9 19.7\nFast R-CNN [impl. in this paper] SS, 2000 COCO train 38.6 18.9 39.3 19.3\nFaster R-CNN RPN, 300 COCO train 41.5 21.2 42.1 21.5\nFaster R-CNN RPN, 300 COCO trainval - - 42.7 21.9\nThe Model is VGG-16.1146 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 6, JUNE 2017\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "e51df1c9-6248-4021-927b-10d693e307a5": "and single-scale ( s\u00bc600) testing. The testing time is still\nabout 200 ms per image on the COCO dataset.\nIn Table 12 we \ufb01rst report the results of the Fast R-CNN\nsystem [2] using the implementation in this paper. Our Fast\nR-CNN baseline has 39.3 percent mAP@0.5 on the test-dev\nset, higher than that reported in [2]. We conjecture that the\nreason for this gap is mainly due to the de\ufb01nition of the neg-\native samples and also the changes of the mini-batch sizes.\nWe also note that the mAP@[.5, .95] is just comparable.\nNext we evaluate our Faster R-CNN system. Using the\nCOCO training set to train, Faster R-CNN has 42.1 percent\nmAP@0.5 and 21.5 percent mAP@[.5, .95] on the COCO test-\ndev set. This is 2.8 percent higher for mAP@0.5 and 2.2percent higher for mAP@[.5, .95] than the Fast R-CNN coun-\nterpart under the same protocol (Table 12). This indicates\nthat RPN performs excellent for improving the localization\naccuracy at higher IoU thresholds. Using the COCO trainval\nset to train, Faster R-CNN has 42.7 percent mAP@0.5 and\n21.9 percent mAP@[.5, .95] on the COCO test-dev set. Fig. 6\nshows some results on the MS COCO test-dev set.\nFaster R-CNN in ILSVRC & COCO 2015 Competitions. We\nhave demonstrated that Faster R-CNN bene\ufb01ts more from\nbetter features, thanks to the fact that the RPN completely\nlearns to propose regions by neural networks. This observa-\ntion is still valid even when one increases the depth sub-\nstantially to over 100 layers [18]. Only by replacing VGG-16\nFig. 6. Selected examples of object detection results on the MS COCO test-dev set using the Faster R-CNN system. The model is VGG-16 and the\ntraining data is COCO trainval (42.7 percent mAP@0.5 on the test-dev set). Each output box is associated with a category label and a softmax score\nin\u00bd0;1/C138. A score threshold of 0.6 is used to display these images. For each image, one color represents one object category in that image.REN ET AL.: FASTER R-CNN: TOWARDS REAL-TIME OBJECT DETECTION WITH REGION PROPOSAL NETWORKS 1147\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "05653326-8a1c-434b-8b19-9ed5736a073b": "with a 101-layer residual net (ResNet-101) [18], the Faster R-\nCNN system increases the mAP from 41.5 percent/21.2 per-\ncent (VGG-16) to 48.4 percent/27.2 percent (ResNet-101) on\nthe COCO val set. With other improvements orthogonal to\nFaster R-CNN, He et al. [18] obtained a single-model result\nof 55.7 percent/34.9 percent and an ensemble result of 59.0\npercent/37.4 percent on the COCO test-dev set, which won\nthe 1st place in the COCO 2015 object detection competition.\nThe same system [18] also won the 1st place in the ILSVRC\n2015 object detection competition, surpassing the second\nplace by absolute 8.5 percent. RPN is also a building block\nof the 1st-place winning entries in ILSVRC 2015 localization\nand COCO 2015 segmentation competitions, for which the\ndetails are available in [18] and [15] respectively.\n4.3 From MS COCO to PASCAL VOC\nLarge-scale data is of crucial importance for improving\ndeep neural networks. Next, we investigate how the MS\nCOCO dataset can help with the detection performance on\nPASCAL VOC.\nAs a simple baseline, we directly evaluate the COCO\ndetection model on the PASCAL VOC dataset, without\n\ufb01ne-tuning on any PASCAL VOC data . This evaluation is\npossible because the categories on COCO are a superset\nof those on PASCAL VOC. The categories that are exclu-\nsive on COCO are ignored in this experiment, and the\nsoftmax layer is performed only on the 20 categories plus\nbackground. The mAP under this setting is 76.1 percent\non the PASCAL VOC 2007 test set (Table 13). This result\nis better than that trained on VOC07+12 (73.2 percent) by\nag o o dm a r g i n ,e v e nt h o u g ht h eP A S C A LV O Cd a t aa r e\nnot exploited.\nThen we \ufb01ne-tune the COCO detection model on the\nVOC dataset. In this experiment, the COCO model is in\nplace of the ImageNet-pre-trained model (that is used to ini-\ntialize the network weights), and the Faster R-CNN system\nis \ufb01ne-tuned as described in Section 3.2. Doing so leads to\n78.8 percent mAP on the PASCAL VOC 2007 test set. The\nextra data from the COCO set increases the mAP by 5.6 per-\ncent. Table 6 shows that the model trained on COCO+VOC\nhas the best AP for every individual category on PASCAL\nVOC 2007. This improvement is mainly resulted from fewer\nfalse alarms on background (Fig. 7). Similar improvements\nare observed on the PASCAL VOC 2012 test set (Table 13\nand Table 7). We note that the test-time speed of obtaining\nthese state-of-the-art results is still about 200 ms per image.5C ONCLUSION\nWe have presented RPNs for ef\ufb01cient and accurate region\nproposal generation. By sharing convolutional features\nwith the down-stream detection network, the region pro-\nposal step is nearly cost-free. Our method enables a uni-\n\ufb01ed, deep-learning-based object detection system to run\nat 5-17 fps. The learned RPN also improves region pro-\nposal quality and thus the overall object detection\naccuracy.\nACKNOWLEDGMENTS\nThis work was done when S. Ren was an intern at Microsoft\nResearch. The majority of this work was done when R. Gir-\nshick was with Microsoft Research.\nREFERENCES\n[1] K. He, X. Zhang, S. Ren, and J. Sun, \u201cSpatial pyramid pooling in\ndeep convolutional networks for visual recognition,\u201d in Proc. 13th\nEur. Conf. Comput. Vis. , 2014 pp. 346\u2013361.\n[2] R. Girshick, \u201cFast R-CNN,\u201d in Proc. IEEE Int. Conf. Comput. Vis. ,\n2015, pp. 1440\u20131448.",
        "686ff445-f163-491d-9ad1-5e39c22cd460": "Our method enables a uni-\n\ufb01ed, deep-learning-based object detection system to run\nat 5-17 fps. The learned RPN also improves region pro-\nposal quality and thus the overall object detection\naccuracy.\nACKNOWLEDGMENTS\nThis work was done when S. Ren was an intern at Microsoft\nResearch. The majority of this work was done when R. Gir-\nshick was with Microsoft Research.\nREFERENCES\n[1] K. He, X. Zhang, S. Ren, and J. Sun, \u201cSpatial pyramid pooling in\ndeep convolutional networks for visual recognition,\u201d in Proc. 13th\nEur. Conf. Comput. Vis. , 2014 pp. 346\u2013361.\n[2] R. Girshick, \u201cFast R-CNN,\u201d in Proc. IEEE Int. Conf. Comput. Vis. ,\n2015, pp. 1440\u20131448.\n[3] K. Simonyan and A. Zisserman, \u201cVery deep convolutional net-\nworks for large-scale image recognition,\u201d in Proc. Int. Conf. Learn.\nRepresentations , 2015.\n[4] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders,\n\u201cSelective search for object recognition,\u201d Int. J. Comput. Vis. ,\nvol. 104, no. 2, pp. 154\u2013171, Sep. 2013.\n[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik, \u201cRich feature\nhierarchies for accurate object detection and semantic\nsegmentation,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. ,\n2014, pp. 580\u2013587.\n[6] C. L. Zitnick and P. Doll /C19ar, \u201cEdge boxes: Locating object proposals\nfrom edges,\u201d in Proc. 13th Eur. Conf. Comput. Vis. , 2014, pp. 391\u2013\n405.\n[7] J. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional net-\nworks for semantic segmentation,\u201d in Proc. IEEE Conf. Comput.\nVis. Pattern Recognition , 2015, pp. 3431\u20133440.\n[8] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,\n\u201cObject detection with discriminatively trained part-based mod-\nels,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, pp. 1627\u2013\n1645, Sep. 2010.\n[9] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and\nY. LeCun, \u201cOverfeat: Integrated recognition, localization and\ndetection using convolutional networks,\u201d in Proc. Int. Conf. Learn.\nRepresentations , 2014.\n[10] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster R-CNN: Towards\nreal-time object detection with region proposal networks,\u201d in\nProc. Neural Inf. Process. Syst. , 2015, pp. 91\u201399.\n[11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\nA. Zisserman, \u201cThe PASCAL Visual Object Classes Challenge\nResults,\u201d Int. J. Comput. Vis. , vol. 88, no. 2, pp. 303\u2013338, Jun. 2007.TABLE 13\nDetection mAP (Percent) of Faster R-CNN on\nPASCAL VOC 2007 Test Set and 2012 Test Set\nUsing Different Training Data\ntraining data 2007 test 2012 test\nVOC07 69.9 67.0\nVOC07+12 73.2 -\nVOC07++12 - 70.4\nCOCO (no VOC) 76.1 73.0\nCOCO+VOC07+12 78.8 -\nCOCO+VOC07++12 - 75.9\nThe model is VGG-16. \u201cCOCO\u201d denotes that the COCO\ntrainval set is used for training.",
        "3ae524ed-d322-41f8-b292-292bb1eaa39c": "J. Comput. Vis. , vol. 88, no. 2, pp. 303\u2013338, Jun. 2007.TABLE 13\nDetection mAP (Percent) of Faster R-CNN on\nPASCAL VOC 2007 Test Set and 2012 Test Set\nUsing Different Training Data\ntraining data 2007 test 2012 test\nVOC07 69.9 67.0\nVOC07+12 73.2 -\nVOC07++12 - 70.4\nCOCO (no VOC) 76.1 73.0\nCOCO+VOC07+12 78.8 -\nCOCO+VOC07++12 - 75.9\nThe model is VGG-16. \u201cCOCO\u201d denotes that the COCO\ntrainval set is used for training. See also Tables 6 and 7.Fig. 7. Error analyses on models trained with and without MS COCO\ndata. The test set is PASCAL VOC 2007 test. Distribution of top-ranked\nCor (correct), Loc (false due to poor localization), Sim (confusion with a\nsimilar category), Oth (confusion with a dissimlar category), BG (\ufb01red on\nbackground) is shown, which is generated by the published diagnosis\ncode of [40].1148 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 39, NO. 6, JUNE 2017\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "13c0e0b9-114f-4f14-82ea-d0d43d1cbf4b": "[12] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll /C19ar, and C. L. Zitnick, \u201cMicrosoft COCO: Common objects\nin context,\u201d in Proc. Eur. Conf. Comput. Vis. , 2014, pp. 740\u2013755.\n[13] S. Song and J. Xiao, \u201cDeep sliding shapes for amodal 3d object\ndetection in RGB-D images,\u201d arXiv:1511.02300 , 2015.\n[14] J. Zhu, X. Chen, and A. L. Yuille, \u201cDeePM: A deep part-based\nmodel for object detection and semantic part localization,\u201d\narXiv:1511.07131 , 2015.\n[15] J. Dai, K. He, and J. Sun, \u201cInstance-aware semantic segmentation\nvia multi-task network cascades,\u201d arXiv:1512.04412 , 2015.\n[16] J. Johnson, A. Karpathy, and L. Fei-Fei, \u201cDensecap: Fully\nconvolutional localization networks for dense captioning,\u201d\narXiv:1511.07571 , 2015.\n[17] D. Kislyuk, Y. Liu, D. Liu, E. Tzeng, and Y. Jing, \u201cHuman curation\nand convnets: Powering item-to-item recommendations on\npinterest,\u201d arXiv:1511.04003 , 2015.\n[18] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for\nimage recognition,\u201d arXiv:1512.03385 , 2015.\n[19] J. Hosang, R. Benenson, and B. Schiele, \u201cHow good are detection\nproposals, really?\u201d presented at Proc. Brit. Mach. Vis. Conf. , Not-\ntingham, England, 2014.\n[20] J. Hosang, R. Benenson, P. Doll /C19ar, and B. Schiele, \u201cWhat makes for\neffective detection proposals?\u201d IEEE Trans. Pattern Anal. Mach.\nIntell. , vol. 38, no. 4, pp. 814\u2013830, Apr. 2015.\n[21] N. Chavali, H. Agrawal, A. Mahendru, and D. Batra, \u201cObject-pro-\nposal evaluation protocol is \u2019gameable\u2019,\u201d arXiv: 1505.05836 , 2015.\n[22] J. Carreira and C. Sminchisescu, \u201cCPMC: Automatic object seg-\nmentation using constrained parametric min-cuts,\u201d IEEE Trans.\nPattern Anal. Mach. Intell. , vol. 34, no. 7, pp. 1312\u20131328, Jul. 2012.\n[23] P. Arbel /C19aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\n\u201cMultiscale combinatorial grouping,\u201d in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit. , 2014, pp. 328\u2013335.\n[24] B. Alexe, T. Deselaers, and V. Ferrari, \u201cMeasuring the objectness\nof image windows,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol. 34,\nno. 11, pp. 2189\u20132202, Nov. 2012.\n[25] C. Szegedy, A. Toshev, and D. Erhan, \u201cDeep neural networks for\nobject detection,\u201d in Proc. Neural Inform. Process. Syst. , 2013,\npp. 2553\u20132561.\n[26] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, \u201cScalable\nobject detection using deep neural networks,\u201d in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. , 2014, pp. 2155\u20132162.\n[27] C. Szegedy, S. Reed, D. Erhan, and D. Anguelov, \u201cScalable, high-\nquality object detection,\u201d arXiv:1412.1441 (v1) , 2015.",
        "d24cfd3b-8f7f-400c-a9ea-bb643453ab90": "11, pp. 2189\u20132202, Nov. 2012.\n[25] C. Szegedy, A. Toshev, and D. Erhan, \u201cDeep neural networks for\nobject detection,\u201d in Proc. Neural Inform. Process. Syst. , 2013,\npp. 2553\u20132561.\n[26] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, \u201cScalable\nobject detection using deep neural networks,\u201d in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. , 2014, pp. 2155\u20132162.\n[27] C. Szegedy, S. Reed, D. Erhan, and D. Anguelov, \u201cScalable, high-\nquality object detection,\u201d arXiv:1412.1441 (v1) , 2015.\n[28] P. O. Pinheiro, R. Collobert, and P. Dollar, \u201cLearning to segment\nobject candidates,\u201d in Proc. Adv. Neural Inform. Process. Syst. , 2015,\npp. 1981\u20131989.\n[29] J. Dai, K. He, and J. Sun, \u201cConvolutional feature masking for joint\nobject and stuff segmentation,\u201d in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. , 2015, pp. 3992\u20134000.\n[30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, \u201cObject detection\nnetworks on convolutional feature maps,\u201d arXiv:1504.06066 , 2015.\n[31] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,\n\u201cAttention-based models for speech recognition,\u201d in Proc. Adv.\nNeural Inform. Process. Syst. , 2015, pp. 577\u2013585.\n[32] M. D. Zeiler and R. Fergus, \u201cVisualizing and understanding con-\nvolutional neural networks,\u201d in Proc. 13th Eur. Conf. Comput. Vis. ,\n2014, pp. 818\u2013833.\n[33] V. Nair and G. E. Hinton, \u201cRecti\ufb01ed linear units improve\nrestricted Boltzmann machines,\u201d in Proc. 27th Int. Conf. Mach.\nLearn. , 2010, pp. 807\u2013814.\n[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, and A. Rabinovich, \u201cGoing deeper with convolutions,\u201d\ninProc. IEEE Conf. Comput. Vis. Pattern Recognition , 2015, pp. 1\u20139.\n[35] Y. LeCun, et al., \u201cBackpropagation applied to handwritten zip\ncode recognition,\u201d Neural Comput. , vol. 1, pp. 541\u2013551, 1989.\n[36] O. Russakovsky, et al., \u201cImageNet Large Scale Visual Recognition\nChallenge,\u201d Int. J. Comput. Vis , vol. 115, pp. 211\u2013252, 2015.\n[37] A. Krizhevsky, I. Sutskever, and G. Hinton, \u201cImagenet classi\ufb01ca-\ntion with deep convolutional neural networks,\u201d in Proc. Neural Inf.\nProcess. Syst. , 2012, pp. 1097\u20131105.\n[38] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell, \u201cCaffe: Convolutional architecture\nfor fast feature embedding,\u201d in Proc. 22nd ACM Int. Conf. Multime-\ndia, 2014, pp. 675\u2013678.[39] K. Lenc and A. Vedaldi, \u201cR-CNN minus R,\u201d in Proc. Brit. Mach.\nVis. Conf. , 2015, pp. 5.1\u20135.12.\n[40] D. Hoiem, Y. Chodpathumwan, and Q. Dai, \u201cDiagnosing error in\nobject detectors,\u201d in Proc.",
        "78a90d0f-40fc-413f-aec5-2880a0af819c": "Syst. , 2012, pp. 1097\u20131105.\n[38] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell, \u201cCaffe: Convolutional architecture\nfor fast feature embedding,\u201d in Proc. 22nd ACM Int. Conf. Multime-\ndia, 2014, pp. 675\u2013678.[39] K. Lenc and A. Vedaldi, \u201cR-CNN minus R,\u201d in Proc. Brit. Mach.\nVis. Conf. , 2015, pp. 5.1\u20135.12.\n[40] D. Hoiem, Y. Chodpathumwan, and Q. Dai, \u201cDiagnosing error in\nobject detectors,\u201d in Proc. 12th Eur. Conf. Comput. Vis. , 2012,\npp. 340\u2013353.\nShaoqing Ren received the BS degree from the\nUniversity of Science and Technology of China in\n2011. He received the PhD degree in a joint PhD\nprogram between University of Science and\nTechnology of China and Microsoft Research\nAsia. His research interests are in computer\nvision, especially in detection and localization of\ngeneral objects and faces.\nKaiming He received the BS degree from Tsing-\nhua University in 2007, and the PhD degree from\nthe Chinese University of Hong Kong in 2011. He\nis a lead researcher at Microsoft Research Asia.\nHe joined Microsoft Research Asia in 2011. His\ncurrent research interests are deep learning for\nvisual recognition, including image classi\ufb01cation,\nobject detection, and semantic segmentation. He\nhas won the Best Paper Award at CVPR 2009.\nRoss Girshick is a research scientist at Face-\nbook AI Research (FAIR). He holds a PhD and\nMS in computer science, both from the University\nof Chicago where he studied under Pedro Fel-\nzenszwalb. Prior to joining FAIR, Ross was a\nResearcher at Microsoft Research and a Post-\ndoctoral Fellow at the University of California,\nBerkeley where he collaborated with Jitendra\nMalik and Trevor Darrell. During the course of\nPASCAL VOC object detection challenge, Ross\nparticipated in multiple winning object detection\nentries and was awarded a \u201clifetime achievement\u201d prize for his work on\nthe widely used Deformable Part Models. More recently, his framework\nfor Region-based Convolution Networks (R-CNN) has more than dou-\nbled the accuracy of previous state-of-the-art object detection methods.\nJian Sun received the BS, MS, and PhD degrees\nfrom Xian Jiaotong University in 1997, 2000, and\n2003. He is a principal researcher at Microsoft\nResearch Asia. He joined Microsoft Research\nAsia in July, 2003. His current major research\ninterests are computer vision, computational pho-\ntography, and deep learning. He has won the\nBest Paper Award at CVPR 2009.\n\"For more information on this or any other computing topic,\nplease visit our Digital Library at www.computer.org/publications/dlib.REN ET AL.: FASTER R-CNN: TOWARDS REAL-TIME OBJECT DETECTION WITH REGION PROPOSAL NETWORKS 1149\nAuthorized licensed use limited to: Harvard Library. Downloaded on May 08,2024 at 18:33:08 UTC from IEEE Xplore.  Restrictions apply.",
        "a3ea05b8-a178-4328-84d1-d591ca6200b7": "Domain Adaptive Faster R-CNN for Object Detection in the Wild\nYuhua Chen1Wen Li1Christos Sakaridis1Dengxin Dai1Luc Van Gool1,2\n1Computer Vision Lab, ETH Zurich2VISICS, ESAT/PSI, KU Leuven\n{yuhua.chen,liwen,csakarid,dai,vangool }@vision.ee.ethz.ch\nAbstract\nObject detection typically assumes that training and test\ndata are drawn from an identical distribution, which, how-\never, does not always hold in practice. Such a distribution\nmismatch will lead to a signi\ufb01cant performance drop. In\nthis work, we aim to improve the cross-domain robustness of\nobject detection. We tackle the domain shift on two levels:\n1) the image-level shift, such as image style, illumination,\netc., and 2) the instance-level shift, such as object appear-\nance, size, etc. We build our approach based on the recent\nstate-of-the-art Faster R-CNN model, and design two do-\nmain adaptation components, on image level and instance\nlevel, to reduce the domain discrepancy. The two domain\nadaptation components are based on H-divergence theory,\nand are implemented by learning a domain classi\ufb01er in ad-\nversarial training manner. The domain classi\ufb01ers on dif-\nferent levels are further reinforced with a consistency regu-\nlarization to learn a domain-invariant region proposal net-\nwork (RPN) in the Faster R-CNN model. We evaluate our\nnewly proposed approach using multiple datasets including\nCityscapes, KITTI, SIM10K, etc. The results demonstrate\nthe effectiveness of our proposed approach for robust ob-\nject detection in various domain shift scenarios.\n1. Introduction\nObject detection is a fundamental problem in computer\nvision. It aims at identifying and localizing all object in-\nstances of certain categories in an image. Driven by the\nsurge of deep convolutional networks (CNN) [32], many\nCNN-based object detection approaches have been pro-\nposed, drastically improving performance [21, 51, 20, 8, 19,\n39].\nWhile excellent performance has been achieved on the\nbenchmark datasets [12, 37], object detection in the real\nworld still faces challenges from the large variance in view-\npoints, object appearance, backgrounds, illumination, im-\nage quality, etc., which may cause a considerable domain\nshift between the training and test data. Taking autonomous\nFigure 1. Illustration of different datasets for autonomous driv-\ning: From top to bottom-right, example images are taken from:\nKITTI [17], Cityscapes [5], Foggy Cityscapes [49], SIM10K [30].\nThough all datasets cover urban scenes, images in those dataset\nvary in style, resolution, illumination, object size, etc. The visual\ndifference between those datasets presents a challenge for apply-\ning an object detection model learned from one domain to another\ndomain.\ndriving as an example, the camera type and setup used in a\nparticular car might differ from those used to collect train-\ning data, and the car might be in a different city where\nthe appearance of objects is different. Moreover, the au-\ntonomous driving system is expected to work reliably under\ndifferent weather conditions ( e.g.in rain and fog), while the\ntraining data is usually collected in dry weather with better\nvisibility. The recent trend of using synthetic data for train-\ning deep CNN models presents a similar challenge due to\nthe visual mismatch with reality. Several datasets focusing\non autonomous driving are illustrated in Figure 1, where we\ncan observe a considerable domain shift.\nSuch domain shifts have been observed to cause sig-\nni\ufb01cant performance drop [23]. Although collecting more\ntraining data could possibly alleviate the impact of domain\nshift, it is non-trivial because annotating bounding boxes is\nan expensive and time-consuming process. Therefore, it is\nhighly desirable to develop algorithms to adapt object de-\ntection models to a new domain that is visually different\nfrom the training domain.\nIn this paper, we address this cross-domain object detec-\ntion problem. We consider the unsupervised domain adap-\ntation scenario: full supervision is given in the source do-\nmain while no supervision is available in the target domain.\n1\n3339",
        "6ee9da30-54cf-47fb-a278-70d7b5cc193e": "Thus, the improved object detection in the target domain\nshould be achieved at no additional annotation cost.\nWe build an end-to-end deep learning model based on\nthe state-of-the-art Faster R-CNN model [48], referred to\nas Domain Adaptive Faster R-CNN. Based on the covari-\nate shift assumption, the domain shift could occur on im-\nage level ( e.g, image scale, image style, illumination, etc.)\nand instance level ( e.g, object appearance, size, etc.), which\nmotivates us to minimize the domain discrepancy on both\nlevels. To address the domain shift, we incorporate two do-\nmain adaptation components on image level and instance\nlevel into the Faster R-CNN model to minimize the H-\ndivergence between two domains. In each component, we\ntrain a domain classi\ufb01er and employ the adversarial training\nstrategy to learn robust features that are domain-invariant.\nWe further incorporate a consistency regularization between\nthe domain classi\ufb01ers on different levels to learn a domain-\ninvariant region proposal network (RPN) in the Faster R-\nCNN model.\nThe contribution of this work can be summarized as\nfollows: 1) We provide a theoretical analysis of the do-\nmain shift problem for cross-domain object detection from\na probabilistic perspective. 2) We design two domain adap-\ntation components to alleviate the domain discrepancy at\nthe image and instance levels, resp. 3) We further propose\na consistency regularization to encourage the RPN to be\ndomain-invariant. 4) We integrate the proposed components\ninto the Faster R-CNN model, and the resulting system can\nbe trained in an end-to-end manner.\nWe conduct extensive experiments to evaluate our Do-\nmain Adaptive Faster R-CNN using multiple datasets in-\ncluding Cityscapes [5],KITTI [17], SIM 10k [30], etc. The\nexperimental results clearly demonstrate the effectiveness\nof our proposed approach for addressing the domain shift\nof object detection in multiple scenarios with domain dis-\ncrepancies.\n2. Related Work\nObject Detection: Object detection dates back a long\ntime, resulting in a plentitude of approaches. Classical\nwork [9, 13, 56] usually formulated object detection as a\nsliding window classi\ufb01cation problem. The rise of deep\nconvolutional networks(CNNs) [32] \ufb01nds its origin in ob-\nject detection, where its successes have led to a swift\nparadigm shift. Among the large number of approaches\nproposed [21, 51, 20, 19, 39, 8], region-based CNNs (R-\nCNN) [21, 20, 60] have received signi\ufb01cant attention due\nto their effectiveness. This line of work was pioneered by\nR-CNN [21], which extracts region proposals from the im-\nage and a network is trained to classify each region of in-\nterest (ROI) independently. The idea has been extended\nby [20, 26] to share the convolution feature map among\nall ROIs. Faster R-CNN [21] produces object proposalswith a Region Proposal Network (RPN). It achieved state-\nof-the-art results and laid the foundation for many follow-\nup works [19, 39, 8, 36, 60]. Faster R-CNN is also highly\n\ufb02exible and can be extended to other tasks, e.g. instance\nsegmentation [7]. However, those works focused on the\nconventional setting without considering the domain adap-\ntation issue for object detection in the wild. In this paper,\nwe choose Faster R-CNN as our base detector, and improve\nits generalization ability for object detection in a new target\ndomain.\nDomain Adaptation: Domain adaptation has been\nwidely studied for image classi\ufb01cation in computer vi-\nsion [10, 11, 33, 23, 22, 14, 52, 40, 15, 18, 50, 45, 43, 35].\nConventional methods include domain transfer multiple\nkernel learning [10, 11], asymmetric metric learning [33],\nsubspace interpolation [23], geodesic \ufb02ow kernel [22], sub-\nspace alignment [14], covariance matrix alignment [52, 57],\netc. Recent works aim to improve the domain adaptability\nof deep neural networks, including [40, 15, 18, 50, 45, 43,\n34, 24, 41, 42].",
        "0fce7844-5392-43b7-98ca-1f37b1e98f98": "Domain Adaptation: Domain adaptation has been\nwidely studied for image classi\ufb01cation in computer vi-\nsion [10, 11, 33, 23, 22, 14, 52, 40, 15, 18, 50, 45, 43, 35].\nConventional methods include domain transfer multiple\nkernel learning [10, 11], asymmetric metric learning [33],\nsubspace interpolation [23], geodesic \ufb02ow kernel [22], sub-\nspace alignment [14], covariance matrix alignment [52, 57],\netc. Recent works aim to improve the domain adaptability\nof deep neural networks, including [40, 15, 18, 50, 45, 43,\n34, 24, 41, 42]. Different from those works, we focus on\nthe object detection problem, which is more challenging as\nboth object location and category need to be predicted.\nA few recent works have also been proposed to perform\nunpaired image translation between two sets of data, which\ncan be seen as pixel-level domain adaptation [62, 31, 59,\n38]. However, it is still a challenging issue to produce re-\nalistic images in high resolution as required by real-world\napplications like autonomous driving.\nDomain Adaptation Beyond Classi\ufb01cation: Compared\nto the research in domain adaptation for classi\ufb01cation, much\nless attention has been paid to domain adaptation for other\ncomputer vision tasks. Recently there are some works con-\ncerning tasks such as semantic segmentation [4, 27, 61],\n\ufb01ne-grained recognition [16] etc. For the task of detec-\ntion, [58] proposed to mitigate the domain shift problem\nof the deformable part-based model (DPM) by introducing\nan adaptive SVM. In a recent work [47], they use R-CNN\nmodel as feature extractor, then the features are aligned with\nthe subspace alignment method. There also exists work to\nlearn detectors from alternative sources, such as from im-\nages to videos [54], from 3D models [46, 53], or from syn-\nthetic models [25]. Previous works either cannot be trained\nin an end-to-end fashion, or focus on a speci\ufb01c case. In this\nwork, we build an end-to-end trainable model for object de-\ntection, which is, to the best of our knowledge, the \ufb01rst of\nits kind.\n3. Preliminaries\n3.1. Faster R\u00adCNN\nWe brie\ufb02y review the Faster R-CNN [60] model, which\nis the baseline model used in this work. Faster R-CNN is\na two-stage detector mainly consisting of three major com-\n3340",
        "1fcdc33b-4fd9-4506-9b36-5c5da9a27f17": "ponents: shared bottom convolutional layers, a region pro-\nposal network (RPN) and a region-of-interest (ROI) based\nclassi\ufb01er. The architecture is illustrated in the left part of\nFigure 2.\nFirst an input image is represented as a convolutional\nfeature map produced by the shared bottom convolutional\nlayers. Based on that feature map, RPN generates candi-\ndate object proposals, whereafter the ROI-wise classi\ufb01er\npredicts the category label from a feature vector obtained\nusing ROI-pooling. The training loss is composed of the\nloss of the RPN and the loss of the ROI classi\ufb01ers:\nLdet=Lrpn+Lroi (1)\nBoth training loss of the RPN and ROI classi\ufb01ers have\ntwo loss terms: one for classi\ufb01cation as how accurate the\npredicted probability is, and the other is a regression loss\non the box coordinates for better localization. Readers are\nreferred to [60] for more details about the architecture and\nthe training procedure.\n3.2. Distribution Alignment with H\u00addivergence\nTheH-divergence [1] is designed to measure the diver-\ngence between two sets of samples with different distribu-\ntions. Let us denote by xa feature vector. A source domain\nsample can be denoted as xSand a target domain sample\nasxT. We also denote by h:x\u2192 {0,1}a domain clas-\nsi\ufb01er, which aims to predict the source samples xSto be0,\nand target domain sample xTto be 1. Suppose His the set\nof possible domain classi\ufb01ers, the H-divergence de\ufb01nes the\ndistance between two domains as follows:\ndH(S,T) = 2(\n1\u2212min\nh\u2208H(\nerrS(h(x))+errT(h(x))))\n.\nwhereerrSanderrTare the prediction errors of h(x)on\nsource and target domain samples, resp. The above de\ufb01ni-\ntion implies that the domain distance dH(S,T)is inversely\nproportional to the error rate of the domain classi\ufb01er h. In\nother words, if the error is high for the best domain clas-\nsi\ufb01er, the two domains are hard to distinguish, so they are\nclose to each other, and v.v.\nIn deep neural networks, the feature vector xusually\ncomprises the activations after a certain layer. Let us de-\nnote byfthe network that produces x. To align the two\ndomains, we therefore need to enforce the networks fto\noutput feature vectors that minimize the domain distance\ndH(S,T)[15], which leads to:\nmin\nfdH(S,T)\u21d4max\nfmin\nh\u2208H{errS(h(x))+errT(h(x))}.\nThis can be optimized in an adversarial training manner.\nGanin and Lempitsky [15] implemented a gradient reverse\nlayer (GRL), and integrated it into a CNN for image classi-\n\ufb01cation in the unsupervised domain adaptation scenario.4. Domain Adaptation for Object Detection\nFollowing the common terminology in domain adapta-\ntion, we refer to the domain of the training data as source\ndomain, denoted by S, and to the domain of the test data\nas target domain, denoted by T. For instance, when using\nthe Cityscapes dataset for training and the KITTI dataset\nfor testing, Sis the Cityscapes dataset and Trepresents the\nKITTI dataset.\nWe also follow the classic setting of unsupervised do-\nmain adaptation, where we have access to images and full\nsupervision in the source domain ( i.e., bounding box and\nobject categories), but only unlabeled images are available\nfor the target domain. Our task is to learn an object detec-\ntion model adapted to the unlabeled target domain.\n4.1. A Probabilistic Perspective\nThe object detection problem can be viewed as learn-\ning the posterior P(C,B|I), whereIis the image repre-\nsentation, Bis the bounding-box of an object and C\u2208\n{1,...,K}the category of the object ( Kbeing the total\nnumber of categories).\nLet us denote the joint distribution of training samples\nfor object detection as P(C,B,I), and usePS(C,B,I)and\nPT(C,B,I)to denote the source domain joint distribution\nand the target domain joint distribution, resp.",
        "34c4287f-d0c9-4723-9066-8ca7b66a7422": "Our task is to learn an object detec-\ntion model adapted to the unlabeled target domain.\n4.1. A Probabilistic Perspective\nThe object detection problem can be viewed as learn-\ning the posterior P(C,B|I), whereIis the image repre-\nsentation, Bis the bounding-box of an object and C\u2208\n{1,...,K}the category of the object ( Kbeing the total\nnumber of categories).\nLet us denote the joint distribution of training samples\nfor object detection as P(C,B,I), and usePS(C,B,I)and\nPT(C,B,I)to denote the source domain joint distribution\nand the target domain joint distribution, resp. Note that here\nwe usePT(C,B,I)to analyze the domain shift problem,\nalthough the bounding box and category annotations ( i.e.,\nBandC) are unknown during training. When there is a\ndomain shift, PS(C,B,I)\u0338=PT(C,B,I).\nImage-Level Adaptation: Using the Bayes\u2019s Formula,\nthe joint distribution can be decomposed as:\nP(C,B,I) =P(C,B|I)P(I). (2)\nSimilar to the classi\ufb01cation problem, we make the covariate\nshift assumption for objection detection, i.e., the conditional\nprobability P(C,B|I)is the same for the two domains, and\nthe domain distribution shift is caused by the difference on\nthe marginal distribution P(I). In other words, the detec-\ntor is consistent between two domains: given an image, the\ndetection results should be the same regardless of which do-\nmain the image belongs. In the Faster R-CNN model, the\nimage representation Iis actually the feature map output\nof the base convolutional layers. Therefore, to handle the\ndomain shift problem, we should enforce the distribution\nof image representation from two domains to be the same\n(i.e.,PS(I) =PT(I)), which is referred to as image-level\nadaptation .\nInstance-Level Adaptation: On the other hand, the\njoint distribution can also be decomposed as:\nP(C,B,I) =P(C|B,I)P(B,I). (3)\nWith the covariate shift assumption, i.e., the conditional\nprobability P(C|B,I)is the same for the two domains, we\n3341",
        "d8c6e8c4-5ce6-4e58-a663-062150a30cf7": "Figure 2. An overview of our Domain Adaptive Faster R-CNN model: we tackle the domain shift on two levels, the image level and the\ninstance level. A domain classi\ufb01er is built on each level, trained in an adversarial training manner. A consistency regularizer is incorporated\nwithin these two classi\ufb01ers to learn a domain-invariant RPN for the Faster R-CNN model.\nhave that the domain distribution shift is from the difference\nin the marginal distribution P(B,I). Intuitively, this im-\nplies the semantic consistency between two domains: given\nthe same image region containing an object, its category\nlabels should be the same regardless of which domain it\ncomes from. Therefore, we can also enforce the distribution\nof instance representation from two domains to be the same\n(i.e.,PS(B,I) =PT(B,I)). We refer to it as instance-level\nalignment.\nHere the instance representation (B,I)refers to the fea-\ntures extracted from the image region in the ground truth\nbounding box for each instance. Although the bounding-\nbox annotation is unavailable for the target domain, we can\nobtain it via P(B,I) =P(B|I)P(I), whereP(B|I)is a\nbounding box predictor ( e.g, RPN in Faster R-CNN). This\nholds only when P(B|I)is domain-invariant, for which we\nprovide a solution below.\nJoint Adaptation: Ideally, one can perform domain\nalignment on either the image or instance level. Consider-\ning thatP(B,I) =P(B|I)P(I)and the conditional distri-\nbutionP(B|I)is assumed to be the same and non-zero for\ntwo domains, thus we have:\nPS(I) =PT(I)\u21d4PS(B,I) =PT(B,I). (4)\nIn other words, if the distributions of the image-level rep-\nresentations are identical for two domains, the distributions\nof the instance-level representations are also identical, and\nv.v. Yet, it is generally non-trivial to perfectly estimate the\nconditional distribution P(B|I). The reasons are two-fold:\n1) in practice it may be hard to perfectly align the marginal\ndistributions P(I), which means the input for estimating\nP(B|I)is somehow biased, and 2) the bounding box an-\nnotation is only available for source domain training data,\ntherefore P(B|I)is learned using the source domain data\nonly, which is easily biased toward the source domain.\nTo this end, we propose to perform domain distributionalignment on both the image and instance levels, and to ap-\nply a consistency regularization to alleviate the bias in esti-\nmatingP(B|I). As introduced in Section 3.2, to align the\ndistributions of two domains, one needs to train a domain\nclassi\ufb01er h(x). In the context of object detection, xcan be\nthe image-level representation Ior the instance-level repre-\nsentation (B,I). From a probabilistic perspective, h(x)can\nbe seen as estimating a sample x\u2019s probability belonging to\nthe target domain.\nThus, by denoting the domain label as D, the image-level\ndomain classi\ufb01er can be viewed as estimating P(D|I), and\nthe instance-level domain classi\ufb01er can be seen as estimat-\ningP(D|B,I). By using the Bayes\u2019 theorem, we obtain:\nP(D|B,I)P(B|I) =P(B|D,I)P(D|I). (5)\nIn particular, P(B|I)is a domain-invariant bounding box\npredictor, and P(B|D,I)a domain-dependent bounding\nbox predictor. Recall that in practice we can only learn\na domain-dependent bounding box predictor P(B|D,I),\nsince we have no bounding box annotations for the target\ndomain. Thus, by enforcing the consistency between two\ndomain classi\ufb01ers, i.e.,P(D|B,I) =P(D|I), we could\nlearnP(B|D,I)to approach P(B|I).\n4.2. Domain Adaptation Components\nThis section introduces two domain adaptation compo-\nnents for the image and instance levels, used to align the\nfeature representation distributions on those two levels.\nImage-Level Adaptation: In the Faster R-CNN model,\nthe image-level representation refers to the feature map out-\nputs of the base convolutional layers (see the green paral-\nlelogram in Figure 2).",
        "856012c7-c00f-4748-a3dd-8e65056bc7a7": "Recall that in practice we can only learn\na domain-dependent bounding box predictor P(B|D,I),\nsince we have no bounding box annotations for the target\ndomain. Thus, by enforcing the consistency between two\ndomain classi\ufb01ers, i.e.,P(D|B,I) =P(D|I), we could\nlearnP(B|D,I)to approach P(B|I).\n4.2. Domain Adaptation Components\nThis section introduces two domain adaptation compo-\nnents for the image and instance levels, used to align the\nfeature representation distributions on those two levels.\nImage-Level Adaptation: In the Faster R-CNN model,\nthe image-level representation refers to the feature map out-\nputs of the base convolutional layers (see the green paral-\nlelogram in Figure 2). To eliminate the domain distribution\nmismatch on the image level, we employ a patch-based do-\nmain classi\ufb01er as shown in the lower right part of Figure 2.\nIn particular, we train a domain classi\ufb01er on each activa-\ntion from the feature map. Since the receptive \ufb01eld of each\n3342",
        "dea6da70-e2ec-4e2b-8d4b-89b47d14b6e2": "activation corresponds to an image patch of the input image\nIi, the domain classi\ufb01er actually predicts the domain label\nfor each image patch.\nThe bene\ufb01ts of this choice are twofold: 1) aligning\nimage-level representations generally helps to reduce the\nshift caused by the global image difference such as image\nstyle, image scale, illumination, etc. A similar patch-based\nloss has shown to be effective in recent work on style trans-\nfer [29], which also deals with the global transformation,\nand 2) the batch size is usually very small for training an\nobject detection network, due to the use of high-resolution\ninput. This patch-based design is helpful to increase the\nnumber of training samples for training the domain classi-\n\ufb01er.\nLet us denote by Dithe domain label of the i-th training\nimage, with Di= 0 for the source domain and Di= 1\nfor the target domain. We denote as \u03c6u,v(Ii)the activation\nlocated at (u,v)of the feature map of the i-th image after\nthe base convolutional layers. By denoting the output of the\ndomain classi\ufb01er as p(u,v)\ni and using the cross entropy loss,\nthe image-level adaptation loss can be written as:\nLimg=\u2212\u2211\ni,u,v[\nDilogp(u,v)\ni+(1\u2212Di)log(1\u2212p(u,v)\ni)]\n.\n(6)\nAs discussed in Section 3.2, to align the domain distri-\nbutions, we should simultaneously optimize the parameters\nof the domain classi\ufb01er to minimize the above domain clas-\nsi\ufb01cation loss, and also optimize the parameters of the base\nnetwork to maximize this loss. For the implementation we\nuse the gradient reverse layer (GRL) [15], whereas the or-\ndinary gradient descent is applied for training the domain\nclassi\ufb01er. The sign of the gradient is reversed when passing\nthrough the GRL layer to optimize the base network.\nInstance-Level Adaptation: The instance-level rep-\nresentation refers to the ROI-based feature vectors before\nfeeding into the \ufb01nal category classi\ufb01ers ( i.e., the rectangles\nafter the \u201cFC\u201d layer in Figure 2). Aligning the instance-\nlevel representations helps to reduce the local instance dif-\nference such as object appearance, size, viewpoint etc. Sim-\nilar to the image-level adaptation, we train a domain classi-\n\ufb01er for the feature vectors to align the instance-level distri-\nbution. Let us denote the output of the instance-level do-\nmain classi\ufb01er for the j-th region proposal in the i-th image\naspi,j. The instance-level adaptation loss can now be writ-\nten as:\nLins=\u2212\u2211\ni,j[\nDilogpi,j+(1\u2212Di)log(1\u2212pi,j)]\n.(7)\nWe also add a gradient reverse layer before the domain clas-\nsi\ufb01er to apply the adversarial training strategy.\nConsistency Regularization: As analyzed in Sec-\ntion 4.1, enforcing consistency between the domain clas-si\ufb01er on different levels helps to learn the cross-domain ro-\nbustness of bounding box predictor ( i.e., RPN in the Faster\nR-CNN model). Therefore, we further impose a consis-\ntency regularizer. Since the image-level domain classi\ufb01er\nproduces an output for each activation of the image-level\nrepresentation I, we take the average over all activations in\nthe image as its image-level probability. The consistency\nregularizer can be written as:\nLcst=\u2211\ni,j\u22251\n|I|\u2211\nu,vp(u,v)\ni\u2212pi,j\u22252, (8)\nwhere|I|denotes the total number of activations in a feature\nmap, and \u2225\u00b7\u2225 is the\u21132distance.\n4.3. Network Overview\nAn overview of our network is shown in Figure 2. We\naugment the Faster R-CNN base architecture with our do-\nmain adaptation components, which leads to our Domain\nAdaptive Faster R-CNN model.\nThe left part of Figure 2 is the original Faster R-CNN\nmodel. The bottom convolutional layers are shared between\nall components. Then the RPN and ROI pooling layers are\nbuilt on top, followed by two fully connected layers to ex-\ntract the instance-level features.\nThree novel components are introduced in our Domain\nAdaptive Faster R-CNN.",
        "9ee9be3f-8e38-4b17-bd1a-b553c562be9d": "4.3. Network Overview\nAn overview of our network is shown in Figure 2. We\naugment the Faster R-CNN base architecture with our do-\nmain adaptation components, which leads to our Domain\nAdaptive Faster R-CNN model.\nThe left part of Figure 2 is the original Faster R-CNN\nmodel. The bottom convolutional layers are shared between\nall components. Then the RPN and ROI pooling layers are\nbuilt on top, followed by two fully connected layers to ex-\ntract the instance-level features.\nThree novel components are introduced in our Domain\nAdaptive Faster R-CNN. The image-level domain classi\ufb01er\nis added after the last convolution layer and the instance-\nlevel domain classi\ufb01er is added to the end of the ROI-wise\nfeatures. The two classi\ufb01ers are linked with a consistency\nloss to encourage the RPN to be domain-invariant. The \ufb01-\nnal training loss of the proposed network is a summation of\neach individual part, which can be written as:\nL=Ldet+\u03bb(Limg+Lins+Lcst) (9)\nwhere\u03bbis a trade-off parameter to balance the Faster R-\nCNN loss and our newly added domain adaptation compo-\nnents. The network can be trained in an end-to-end manner\nusing a standard SGD algorithm. Note that the adversar-\nial training for domain adaptation components is achieved\nby using the GRL layer, which automatically reverses the\ngradient during propagation. The overall network in Fig-\nure 2 is used in the training phase. During inference, one\ncan remove the domain adaptation components, and sim-\nply use the original Faster R-CNN architecture with adapted\nweights.\n5. Experiments\n5.1. Experiment Setup\nWe adopt the unsupervised domain adaptation protocol\nin our experiments. The training data consists of two parts:\n3343",
        "ff76f18e-52e7-43a2-9075-87b120c1eeb1": "thesource training data for which images and their annota-\ntions (bounding boxes and object categories) are provided,\nand the target training data for which only unlabeled im-\nages are available.\nTo validate the proposed approach, for all domain shift\nscenarios, we report the \ufb01nal results of our model as well as\nthe results by combining different components ( i.e., image-\nlevel adaptation, instance-level adaptation, and the consis-\ntency regularization). To our best knowledge, this is the\n\ufb01rst work proposed to improve Faster R-CNN for cross-\ndomain object detection. We include the original Faster R-\nCNN model as a baseline, which is trained using the source\ndomain training data, without considering domain adapta-\ntion. For all experiments, we report mean average preci-\nsions (mAP) with a threshold of 0.5 for evaluation.\nUnless otherwise stated, all training and test images are\nresized such that the shorter side has a length of 500pixels\nto \ufb01t in GPU memory, and we set \u03bb= 0.1for all experi-\nments. We follow [48] to set the hyper-parameters. Speci\ufb01-\ncally, the models are initialized using weights pretrained on\nImageNet. We \ufb01netune the network with a learning rate of\n0.001for50k iterations and then reduce the learning rate to\n0.0001 for another 20k iterations. Each batch is composed\nof2images, one from the source domain and one from the\ntarget domain. A momentum of 0.9 and a weight decay of\n0.0005 is used in our experiments.\n5.2. Experimental Results\nIn this section we evaluate our proposed Domain Adap-\ntive Faster R-CNN model for object detection in three dif-\nferent domain shift scenarios: 1) learning from synthetic\ndata, where the training data is captured from video games,\nwhile the test data comes from the real world; 2) driving in\nadverse weather , where the training data is taken in good\nweather conditions, while the test data in foggy weather; 3)\ncross camera adaptation , where the training data and test\ndata are captured with different camera setups.\n5.2.1 Learning from Synthetic Data\nAs computer graphics technique advances, using synthetic\ndata to train CNNs becomes increasingly popular. Nonethe-\nless, synthetic data still exhibits a clear visual difference\nwith real world images, and usually there is a performance\ngap with models trained on real data. Our \ufb01rst experiment\nis to investigate the effectiveness of the proposed method in\nthis scenario. We use the SIM 10k [30] dataset as the source\ndomain, and the Cityscapes dataset as the target domain.\nDatasets: SIM 10k [30] consists of 10,000 images\nwhich are rendered by the gaming engine Grand Theft\nAuto(GTA V). In SIM 10k , bounding boxes of 58,701cars\nare provided in the 10,000training images. All images are\nused in the training. The Cityscapes [5] dataset is an urbanimg ins cons car AP\nFaster R-CNN 30.12\nOurs\u2713 33.03\n\u2713 35.79\n\u2713 \u2713 37.86\n\u2713 \u2713 \u2713 38.97\nTable 1. The average precision (AP) of Caron the Cityscapes val-\nidation set. The models are trained using the SIM 10k dataset as\nthe source domain and the Cityscapes training set as the target do-\nmain. imgis short for image-level alignment ,insforinstance-level\nalignment andcons is short for our consistency loss\nscene dataset for driving scenarios. The images are cap-\ntured by a car-mounted video camera. It has 2,975images\nin the training set, and 500images in the validation set. We\nuse the unlabeled images from the training set as the target\ndomain to adapt our detector, and the results are reported\non the validation set. There are 8categories with instance\nlabels in Cityscapes , but only caris used in this experi-\nment since only caris annotated in SIM 10k . Note that\ntheCityscapes dataset is not dedicated to detection, thus we\ntake the tightest rectangles of its instance masks as ground-\ntruth bounding boxes.\nResults: The results of the different methods are sum-\nmarized in Table 1.",
        "be39f30f-dfc8-4b9a-a151-1cbcc3dd5719": "imgis short for image-level alignment ,insforinstance-level\nalignment andcons is short for our consistency loss\nscene dataset for driving scenarios. The images are cap-\ntured by a car-mounted video camera. It has 2,975images\nin the training set, and 500images in the validation set. We\nuse the unlabeled images from the training set as the target\ndomain to adapt our detector, and the results are reported\non the validation set. There are 8categories with instance\nlabels in Cityscapes , but only caris used in this experi-\nment since only caris annotated in SIM 10k . Note that\ntheCityscapes dataset is not dedicated to detection, thus we\ntake the tightest rectangles of its instance masks as ground-\ntruth bounding boxes.\nResults: The results of the different methods are sum-\nmarized in Table 1. Speci\ufb01cally, compared with Faster\nR-CNN, we achieve +2.9%performance gain using the\nimage-level adaptation component only, and +5.6%using\ninstance-level alignment only. This proves that our pro-\nposed image-level adaptation and instance-level adaptation\ncomponents can reduce the domain shift on each level ef-\nfectively. Combining those two components yields an im-\nprovement of 7.7%, which validates our conjecture on the\nnecessity of reducing domain shifts on both levels. By fur-\nther applying the consistency regularization, our Domain\nAdaptive Faster R-CNN model improves the Faster R-CNN\nmodel by +8.8%, which achieves 38.97% in terms of AP.\n5.2.2 Driving in Adverse Weather\nWe proceed with our evaluation by studying domain shift\nbetween weather conditions. Weather condition is an im-\nportant source of domain discrepancy, as scenes are visu-\nally different as weather conditions change. Whether a de-\ntection system can perform faithfully in different weather\nconditions is critical for a safe autonomous driving sys-\ntem [44, 49]. In this section, we investigate the ability to\ndetect objects when we adapt a model from normal to foggy\nweather.\nDatasets: Cityscapes is used as our source domain, with\nimages dominantly obtained in clear weather. In this ex-\nperiment we report our results on categories with instance\nannotations: person ,rider ,car,truck ,bus,train ,motorcycle\nandbicycle .\nFor the target domain, we use the Foggy Cityscapes\n3344",
        "51f3224e-c4ef-47cc-9742-dcb9c4715541": "img ins cons person rider car truck bus train mcycle bicycle mAP\nFaster R-CNN 17.8 23.6 27.1 11.9 23.8 9.1 14.4 22.8 18.8\nOurs\u2713 22.9 30.7 39.0 20.1 27.5 17.7 21.4 25.9 25.7\n\u2713 23.6 30.6 38.6 20.8 40.5 12.8 17.1 26.1 26.3\n\u2713 \u2713 24.2 31.2 39.1 19.1 36.2 19.2 17.1 27.0 26.6\n\u2713 \u2713 \u2713 25.0 31.0 40.5 22.1 35.3 20.2 20.0 27.1 27.6\nTable 2. Quantitative results on the Foggy Cityscapes validation set, models are trained on the Cityscapes training set.\nimg ins cons K\u2192C C\u2192K\nFaster R-CNN 30.2 53.5\nOurs\u2713 36.6 60.9\n\u2713 34.6 57.6\n\u2713 \u2713 37.3 62.7\n\u2713 \u2713 \u2713 38.5 64.1\nTable 3. Quantitative analysis of adaptation result between KITTI\nandCityscapes . We report AP of Caron both directions. e.g.K\n\u2192C and C\u2192K.\ndataset that was recently presented in [49]. Foggy\nCityscapes is a synthetic foggy dataset in that it simulates\nfog on real scenes. The images are rendered using the im-\nages and depth maps from Cityscapes . Examples can be\nfound at Figure 1 and also in the original paper [49]. The\nsemantic annotations and data split of Foggy Cityscapes are\ninherited from Cityscapes , making it ideal to study the do-\nmain shift caused by weather condition.\nResult: Table 2 presents our results and those of other\nbaselines. Similar observations apply as in the learning\nfrom synthetic data scenario. Combining all components,\nour adaptive Faster R-CNN improves the baseline Faster\nR-CNN model by +8.6%. Besides, we can see that the\nimprovement generalizes well across different categories,\nwhich suggests that the proposed technique can also reduce\ndomain discrepancy across different object classes.\n5.2.3 Cross Camera Adaptation\nDomain shift commonly exists even between real datasets\ntaken under similar weather conditions, as different dataset\nare captured using different setups, with different image\nquality/resolution, and usually exhibit some data bias when\ncollecting the dataset [55]. For detection, different datasets\nalso vary drastically in scale, size and class distribution,\nsometimes it is dif\ufb01cult to determine the source of a domain\nshift. In this part, we focus on studying adaptation between\ntwo real datasets, as we take KITTI andCityscapes as our\ndatasets.\nDatasets: We use KITTI training set which contains\n7,481images. The dataset is used in both adaptation and\nevaluation. Images have original resolution of 1250\u00d7375,\nand are resized so that shorter length is 500 pixels long.Correct Mislocalization Background\nCor:\n29.6%\nMisloc:\n29.3%BG:\n41.1%\n(a) Faster RCNNCor:\n44.3%\nMisloc:\n38.2%BG:\n17.4%\n(b) Ours (Ins. Only)Cor:\n58.5%Misloc:\n36.2%BG:\n5.2%\n(c) Ours (Img Only)\nFigure 3. Error Analysis of Top Ranked Detections\nCityscapes is used as the other domain. Consistent with the\n\ufb01rst experiment, we evaluate our method using AP of car,\nResults: We apply the proposed method in both adapta-\ntion directions, we denote KITTI toCityscapes asK\u2192C\nand vice versa. Table 3 compares our method to other base-\nlines. A clear performance improvement is achieved by our\nproposed Adaptive Faster R-CNN model over other base-\nlines. And our method is useful for both adaptation direc-\ntionsK\u2192CandC\u2192K.\n5.3.",
        "dbb46945-021c-44ec-9571-ff4c468e9929": "Only)Cor:\n58.5%Misloc:\n36.2%BG:\n5.2%\n(c) Ours (Img Only)\nFigure 3. Error Analysis of Top Ranked Detections\nCityscapes is used as the other domain. Consistent with the\n\ufb01rst experiment, we evaluate our method using AP of car,\nResults: We apply the proposed method in both adapta-\ntion directions, we denote KITTI toCityscapes asK\u2192C\nand vice versa. Table 3 compares our method to other base-\nlines. A clear performance improvement is achieved by our\nproposed Adaptive Faster R-CNN model over other base-\nlines. And our method is useful for both adaptation direc-\ntionsK\u2192CandC\u2192K.\n5.3. Error Analysis on Top Ranked Detections\nIn the previous sections, we have shown that both image-\nlevel and instance-level alignment help to decrease domain\ndiscrepancy. To further validate the individual effect of\nimage-level adaptation and instance-level adaptation, we\nanalyze the accuracies caused by most con\ufb01dent detections\nfor models using adaptation components on different levels.\nWe use KITTI\u2192Cityscapes as a study case. We select\n20,000predictions with highest con\ufb01dence for the vanilla\nFaster R-CNN model, our model with only image-level\nadaptation, and our model with only instance-level adap-\ntation, respectively. Inspired by [28], we categorize the de-\ntections into 3 error types: correct: The detection has an\noverlap greater than 0.5 with ground-truth. mis-localized:\nThe detection has a overlap with ground-truth of 0.3 to 0.5,\nandbackground: the detection has an overlap smaller than\n0.3, which means it takes a background as a false positive.\nThe results are shown in Figure 3. From the \ufb01gure we\ncan observe that each individual component (image-level\nor instance-level adaptation) improves the number of cor-\nrect detections (green color), and dramatically reduces the\n3345",
        "92e7ed56-f631-434c-b5e5-ec5d2b0626cf": "200 400 600 800 1000 1200\nTarget Domain Scale510152025303540Average Precisionnon-adapt\nimg + inst align\nimg align\ninst align\nFigure 4. AP at different scales : Source images from KITTI are\n\ufb01xed at a scale of 500 pixels, and we resize the target images from\nCityscapes to different scales.\nnumber of false positives (other colors). Moreover, we also\nobserve that the model using instance-level alignment gives\nhigher background error than the model using image-level\nalignment. The reason might be the image-level alignment\nimproves RPN more directly, which produces region pro-\nposals with better localization performance.\n5.4. Image\u00adlevel v.s.Instance\u00adlevel Alignment\nImage scale has shown to play a vital role in many com-\nputer vision tasks [3, 2, 6]. To further analyze the impact\nof image-level and instance-level adaptation, we conduct\nexperiment on KITTI\u2192Cityscapes by varying the image\nscales. Because different cameras are used in two datasets,\nthe different camera parameters might lead to a scale drift\nbetween two domains.\nIn particular, we refer to the shorter length of an image as\nitsscale . To study how image scale affects our two domain\nadaptation components, we vary the size of images in the\ntarget domain to see how this affects the behavior of the two\ncomponents while the scale in the source domain is \ufb01xed to\n500pixels. For ef\ufb01ciency, we use a smaller VGG-M model\nas the backbone, and all other settings remain identical.\nWe plot the performance of different models in Figure 4.\nBy varying the scale of target images, we observe that the\nperformance of the vanilla Faster R-CNN ( i.e., non-adapt)\ndrops signi\ufb01cantly when the scales are mismatched. Com-\nparing the two adaptation models, the image-level adapta-\ntion model is more robust to scale change than the instance-\nlevel adaptation model.\nThe reason behind this is that the scale change is a global\ntransformation, which affects all instances and background.\nAnd in our design, global domain shift is mainly tackled\nby image-level alignment, and instance-level alignment is\nused to minimize instance-level discrepancy. When there\nis a serious global domain shift, the localization error of\ninstance proposals goes up, thus the accuracy of instance-\nlevel alignment is damaged by deviating proposals. Never-\ntheless, using both always yields the best results across allFaster R-CNN Ours(w/o) Ours\nmIoU 18.8 28.5 30.3\nTable 4. Mean best Overlap between with groundtruth bounding\nboxes by top 300 proposals from RPN in different models, in\nwhich Ours(w/o) denotes our model without using consistency\nregularization.\nscales. Contrary to the vanilla Faster R-CNN, our model can\nbene\ufb01t from high resolution of target images, and performs\nincreasingly better as the scale rises from 200to1,000pix-\nels.\n5.5. Consistency Regularization\nAs discussed in Section 4.2, we impose a consistency\nregularization on domain classi\ufb01ers at two different levels\nfor learning a robust RPN. To show the bene\ufb01t of using con-\nsistency regularization, we take KITTI\u2192Cityscapes as an\nexample to study the performance of RPN before and after\nusing the consistency regularization in Table 4. The max-\nimum achievable mean overlap between the top 300 pro-\nposals from RPN and the ground-truth is used for measure-\nment. The vanilla Faster R-CNN model is also included as\na baseline. As shown in the table, without using consistency\nregularizer, our model improves Faster R-CNN from 18.8%\nto28.5%in terms of mIoU, due to the use of image-level\nand instance-level adaptation. By further imposing the con-\nsistency regularizer, the performance of RPN can be further\nimproved to 30.3%, which indicates the consistency regu-\nlarizer encourages the RPN to be more robust.\n6. Conclusion\nIn this paper, we have introduced the Domain Adap-\ntive Faster R-CNN model, an effective approach for cross-\ndomain object detection. With our approach, one can ob-\ntain a robust object detector for a new domain without us-\ning any additional labeled data. Our approach is built on the\nstate-of-the-art Faster R-CNN model.",
        "8b5fc768-6588-47da-8513-c34c0cfa6915": "The vanilla Faster R-CNN model is also included as\na baseline. As shown in the table, without using consistency\nregularizer, our model improves Faster R-CNN from 18.8%\nto28.5%in terms of mIoU, due to the use of image-level\nand instance-level adaptation. By further imposing the con-\nsistency regularizer, the performance of RPN can be further\nimproved to 30.3%, which indicates the consistency regu-\nlarizer encourages the RPN to be more robust.\n6. Conclusion\nIn this paper, we have introduced the Domain Adap-\ntive Faster R-CNN model, an effective approach for cross-\ndomain object detection. With our approach, one can ob-\ntain a robust object detector for a new domain without us-\ning any additional labeled data. Our approach is built on the\nstate-of-the-art Faster R-CNN model. Based on our theoret-\nical analysis for cross-domain object detection, we propose\nan image-level adaptation component and an instance-level\ncomponent to alleviate the performance drop caused by do-\nmain shift. The adaptation components are based on adver-\nsarial training of H-divergence. A consistency regularizer is\nfurther applied to learn a domain-invariant RPN. Our model\ncan be trained end-to-end using the standard SGD optimiza-\ntion technique. Our approach is validated on various do-\nmain shift scenarios, and the adaptive method outperforms\nbaseline Faster R-CNN by a clear margin, thus demonstrat-\ning its effectiveness for cross-domain object detection.\nAcknowledgements This work is supported by armasu-\nisse. Wen Li is the corresponding author. Christos Sakaridis\nand Dengxin Dai are supported by Toyota Motor Europe via\nTRACE-Zurich.\n3346",
        "b99d2d35-94fd-47ce-88ee-0be6d484d638": "References\n[1] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira,\nand J. W. Vaughan. A theory of learning from different do-\nmains. Machine learning , 79(1):151\u2013175, 2010.\n[2] L.-C. Chen, Y . Yang, J. Wang, W. Xu, and A. L. Yuille. At-\ntention to scale: Scale-aware semantic image segmentation.\nInCVPR , 2016.\n[3] Y . Chen, D. Dai, J. Pont-Tuset, and L. Van Gool. Scale-aware\nalignment of hierarchical image segmentation. In CVPR ,\n2016.\n[4] Y . Chen, W. Li, and L. Van Gool. ROAD: Reality oriented\nadaptation for semantic segmentation of urban scenes. In\nCVPR , 2018.\n[5] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\nR. Benenson, U. Franke, S. Roth, and B. Schiele. The\nCityscapes dataset for semantic urban scene understanding.\nInCVPR , 2016.\n[6] D. Dai, Y . Wang, Y . Chen, and L. Van Gool. Is image super-\nresolution helpful for other vision tasks? In WACV , 2016.\n[7] J. Dai, K. He, and J. Sun. Instance-aware semantic segmen-\ntation via multi-task network cascades. In CVPR , 2016.\n[8] J. Dai, Y . Li, K. He, and J. Sun. R-FCN: Object detection via\nregion-based fully convolutional networks. In NIPS , 2016.\n[9] N. Dalal and B. Triggs. Histograms of oriented gradients for\nhuman detection. In CVPR , 2005.\n[10] L. Duan, I. W. Tsang, and D. Xu. Domain transfer multiple\nkernel learning. TPAMI , 34(3):465\u2013479, 2012.\n[11] L. Duan, D. Xu, I. W. Tsang, and J. Luo. Visual event\nrecognition in videos by learning from web data. TPAMI ,\n34(9):1667\u20131680, 2012.\n[12] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and\nA. Zisserman. The Pascal visual object classes (VOC) chal-\nlenge. IJCV , 88(2):303\u2013338, 2010.\n[13] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\nmanan. Object detection with discriminatively trained part-\nbased models. TPAMI , 32(9):1627\u20131645, 2010.\n[14] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Un-\nsupervised visual domain adaptation using subspace align-\nment. In ICCV , 2013.\n[15] Y . Ganin and V . Lempitsky. Unsupervised domain adaptation\nby backpropagation. In ICML , 2015.\n[16] T. Gebru, J. Hoffman, and L. Fei-Fei. Fine-grained recogni-\ntion in the wild: A multi-task domain adaptation approach.\narXiv:1709.02476 , 2017.\n[17] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets\nrobotics: The KITTI dataset. The International Journal of\nRobotics Research , 32(11):1231\u20131237, 2013.\n[18] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and\nW. Li. Deep reconstruction-classi\ufb01cation networks for un-\nsupervised domain adaptation. In ECCV , 2016.\n[19] S. Gidaris and N. Komodakis. Object detection via a multi-\nregion and semantic segmentation-aware CNN model. In\nICCV , 2015.",
        "7995c51c-0309-4a33-81d6-d5473deb7163": "Fine-grained recogni-\ntion in the wild: A multi-task domain adaptation approach.\narXiv:1709.02476 , 2017.\n[17] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets\nrobotics: The KITTI dataset. The International Journal of\nRobotics Research , 32(11):1231\u20131237, 2013.\n[18] M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and\nW. Li. Deep reconstruction-classi\ufb01cation networks for un-\nsupervised domain adaptation. In ECCV , 2016.\n[19] S. Gidaris and N. Komodakis. Object detection via a multi-\nregion and semantic segmentation-aware CNN model. In\nICCV , 2015.\n[20] R. Girshick. Fast R-CNN. In ICCV , 2015.\n[21] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In CVPR , 2014.[22] B. Gong, Y . Shi, F. Sha, and K. Grauman. Geodesic \ufb02ow\nkernel for unsupervised domain adaptation. In CVPR , 2012.\n[23] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation\nfor object recognition: An unsupervised approach. In ICCV ,\n2011.\n[24] P. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers. As-\nsociative domain adaptation. In ICCV , 2017.\n[25] H. Hattori, V . Naresh Boddeti, K. M. Kitani, and T. Kanade.\nLearning scene-speci\ufb01c pedestrian detectors without real\ndata. In CVPR , 2015.\n[26] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\nin deep convolutional networks for visual recognition. In\nECCV , 2014.\n[27] J. Hoffman, D. Wang, F. Yu, and T. Darrell. FCNs in the\nwild: Pixel-level adversarial and constraint-based adapta-\ntion. arXiv:1612.02649 , 2016.\n[28] D. Hoiem, Y . Chodpathumwan, and Q. Dai. Diagnosing error\nin object detectors. In ECCV , 2012.\n[29] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for\nreal-time style transfer and super-resolution. In ECCV , 2016.\n[30] M. Johnson-Roberson, C. Barto, R. Mehta, S. N. Sridhar,\nK. Rosaen, and R. Vasudevan. Driving in the matrix: Can\nvirtual worlds replace human-generated annotations for real\nworld tasks? In ICRA , 2017.\n[31] T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to\ndiscover cross-domain relations with generative adversarial\nnetworks. In ICCV , 2017.\n[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassi\ufb01cation with deep convolutional neural networks. In\nNIPS , 2012.\n[33] B. Kulis, K. Saenko, and T. Darrell. What you saw is not\nwhat you get: Domain adaptation using asymmetric kernel\ntransforms. In CVPR , 2011.\n[34] D. Li, Y . Yang, Y .-Z. Song, and T. M. Hospedales. Deeper,\nbroader and artier domain generalization. In ICCV , 2017.\n[35] W. Li, Z. Xu, D. Xu, D. Dai, and L. Van Gool. Domain gen-\neralization and adaptation using low rank exemplar SVMs.\nTPAMI , 2017.\n[36] T.-Y .",
        "e6562a7d-d8c9-4238-ae6e-7a2c99154281": "Imagenet\nclassi\ufb01cation with deep convolutional neural networks. In\nNIPS , 2012.\n[33] B. Kulis, K. Saenko, and T. Darrell. What you saw is not\nwhat you get: Domain adaptation using asymmetric kernel\ntransforms. In CVPR , 2011.\n[34] D. Li, Y . Yang, Y .-Z. Song, and T. M. Hospedales. Deeper,\nbroader and artier domain generalization. In ICCV , 2017.\n[35] W. Li, Z. Xu, D. Xu, D. Dai, and L. Van Gool. Domain gen-\neralization and adaptation using low rank exemplar SVMs.\nTPAMI , 2017.\n[36] T.-Y . Lin, P. Doll \u00b4ar, R. Girshick, K. He, B. Hariharan, and\nS. Belongie. Feature pyramid networks for object detection.\narXiv:1612.03144 , 2016.\n[37] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll \u00b4ar, and C. L. Zitnick. Microsoft COCO: Com-\nmon objects in context. In ECCV , 2014.\n[38] M.-Y . Liu, T. Breuel, and J. Kautz. Unsupervised image-to-\nimage translation networks. In NIPS , 2017.\n[39] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y .\nFu, and A. C. Berg. SSD: Single shot multibox detector. In\nECCV , 2016.\n[40] M. Long, Y . Cao, J. Wang, and M. I. Jordan. Learning trans-\nferable features with deep adaptation networks. In ICML ,\n2015.\n[41] H. Lu, L. Zhang, Z. Cao, W. Wei, K. Xian, C. Shen, and\nA. van den Hengel. When unsupervised domain adaptation\nmeets tensor representations. In ICCV , 2017.\n[42] F. Maria Carlucci, L. Porzi, B. Caputo, E. Ricci, and\nS. Rota Bulo. AutoDIAL: Automatic domain alignment lay-\ners. In ICCV , 2017.\n3347",
        "e515f468-7395-4d6f-bbdb-074349cb016b": "[43] S. Motiian, M. Piccirilli, D. A. Adjeroh, and G. Doretto. Uni-\n\ufb01ed deep supervised domain adaptation and generalization.\nInICCV , 2017.\n[44] S. G. Narasimhan and S. K. Nayar. Vision and the atmo-\nsphere. IJCV , 48(3):233\u2013254, 2002.\n[45] P. Panareda Busto and J. Gall. Open set domain adaptation.\nInICCV , 2017.\n[46] X. Peng, B. Sun, K. Ali, and K. Saenko. Learning deep ob-\nject detectors from 3D models. In ICCV , 2015.\n[47] A. Raj, V . P. Namboodiri, and T. Tuytelaars. Subspace\nalignment based domain adaptation for RCNN detector. In\nBMVC , 2015.\n[48] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-\nwards real-time object detection with region proposal net-\nworks. In NIPS , 2015.\n[49] C. Sakaridis, D. Dai, and L. Van Gool. Semantic foggy scene\nunderstanding with synthetic data. IJCV , 2018.\n[50] O. Sener, H. O. Song, A. Saxena, and S. Savarese. Learning\ntransferrable representations for unsupervised domain adap-\ntation. In NIPS , 2016.\n[51] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and\nY . LeCun. OverFeat: Integrated recognition, localization and\ndetection using convolutional networks. arXiv:1312.6229 ,\n2013.\n[52] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy\ndomain adaptation. In AAAI , 2016.\n[53] B. Sun and K. Saenko. From virtual to reality: Fast adap-\ntation of virtual object detectors to real domains. In BMVC ,\n2014.\n[54] K. Tang, V . Ramanathan, L. Fei-Fei, and D. Koller. Shifting\nweights: Adapting object detectors from image to video. In\nNIPS , 2012.\n[55] A. Torralba and A. A. Efros. Unbiased look at dataset bias.\nInCVPR , 2011.\n[56] P. Viola and M. Jones. Rapid object detection using a boosted\ncascade of simple features. In CVPR , 2001.\n[57] Y . Wang, W. Li, D. Dai, and L. Van Gool. Deep\ndomain adaptation by geodesic distance minimization.\narXiv:1707.09842 , 2017.\n[58] J. Xu, S. Ramos, D. V \u00b4azquez, and A. M. Lopez. Do-\nmain adaptation of deformable part-based models. TPAMI ,\n36(12):2367\u20132380, 2014.\n[59] Z. Yi, H. Zhang, P. T. Gong, et al. DualGAN: Unsupervised\ndual learning for image-to-image translation. In ICCV , 2017.\n[60] L. Zhang, L. Lin, X. Liang, and K. He. Is faster R-CNN\ndoing well for pedestrian detection? In ECCV , 2016.\n[61] Y . Zhang, P. David, and B. Gong. Curriculum domain adap-\ntation for semantic segmentation of urban scenes. In ICCV ,\n2017.\n[62] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-\nto-image translation using cycle-consistent adversarial net-\nworks. In ICCV , 2017.\n3348",
        "108d8f2e-b0f0-4658-9d8e-e1764ff52d28": "Attention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.comNoam Shazeer\u2217\nGoogle Brain\nnoam@google.comNiki Parmar\u2217\nGoogle Research\nnikip@google.comJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.comAidan N. Gomez\u2217\u2020\nUniversity of Toronto\naidan@cs.toronto.edu\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217\u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring signi\ufb01cantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature.\n1 Introduction\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\nin particular, have been \ufb01rmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [31, 21, 13].\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
        "ce9df98f-c18c-4d9e-a547-5b03d7a8fcfb": "Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstatesht, as a function of the previous hidden state ht\u22121and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsigni\ufb01cant improvements in computational ef\ufb01ciency through factorization tricks [ 18] and conditional\ncomputation [ 26], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2,16]. In all but a few cases [ 22], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for signi\ufb01cantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more dif\ufb01cult to learn dependencies between distant positions [ 11]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].\nTo the best of our knowledge, however, the Transformer is the \ufb01rst transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [14, 15] and [8].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29].\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an output\nsequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive\n[9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\nsub-layers. The \ufb01rst is a multi-head self-attention mechanism, and the second is a simple, position-\n2",
        "efe65447-e65d-408c-bfd3-936f42915f26": "Figure 1: The Transformer - model architecture.\nwise fully connected feed-forward network. We employ a residual connection [ 10] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n3",
        "e04354f4-c862-42d6-b96c-0566a1a6c709": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nquery with all keys, divide each by\u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\nthe matrix of outputs as:\nAttention(Q,K,V ) = softmax(QKT\n\u221adk)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof1\u221adk. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-ef\ufb01cient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1\u221adk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it bene\ufb01cial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the \ufb01nal values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\u00b7k=\u2211dk\ni=1qiki, has mean 0and variance dk.\n4",
        "e939a4ce-bba4-443e-b3a9-4d18ac9ce0c8": "MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\nwhere head i= Attention( QWQ\ni,KWK\ni,VWV\ni)\nWhere the projections are parameter matrices WQ\ni\u2208Rdmodel\u00d7dk,WK\ni\u2208Rdmodel\u00d7dk,WV\ni\u2208Rdmodel\u00d7dv\nandWO\u2208Rhdv\u00d7dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[31, 2, 8].\n\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation \ufb02ow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0,xW 1+b1)W2+b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 24]. In the embedding layers, we multiply those weights by\u221admodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n5",
        "1b2bedd8-aa0a-4c58-a952-e04b2f6366d7": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2)O(1) O(logk(n))\nSelf-Attention (restricted) O(r\u00b7n\u00b7d)O(1) O(n/r)\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and \ufb01xed [8].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i)=sin(pos/100002i/d model)\nPE(pos,2i+1)=cos(pos/100002i/d model)\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0to10000\u00b72\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any \ufb01xed offset k,PEpos+kcan be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\nlengthnis smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\n6",
        "50f6892c-20c7-4c97-a077-be4a2602b0d6": "the input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\norO(logk(n))in the case of dilated convolutions [ 15], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k\u00b7n\u00b7d+n\u00b7d2). Even with k=n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side bene\ufb01t, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the signi\ufb01cantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [ 31]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [ 17] with\u03b21= 0.9,\u03b22= 0.98and\u03f5= 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate =d\u22120.5\nmodel\u00b7min(step_num\u22120.5,step _num\u00b7warmup _steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the \ufb01rst warmup _steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup _steps = 4000 .\n5.4 Regularization\nWe employ three types of regularization during training:\nResidual Dropout We apply dropout [ 27] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop= 0.1.\n7",
        "9fa3a647-d409-4d62-ad67-d8d2adfb37e2": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModelBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [15] 23.75\nDeep-Att + PosUnk [32] 39.2 1.0\u00b71020\nGNMT + RL [31] 24.6 39.92 2.3\u00b710191.4\u00b71020\nConvS2S [8] 25.16 40.46 9.6\u00b710181.5\u00b71020\nMoE [26] 26.03 40.56 2.0\u00b710191.2\u00b71020\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0\u00b71020\nGNMT + RL Ensemble [31] 26.30 41.16 1.8\u00b710201.1\u00b71021\nConvS2S Ensemble [8] 26.36 41.29 7.7\u00b710191.2\u00b71021\nTransformer (base model) 27.3 38.1 3.3\u00b71018\nTransformer (big) 28.4 41.0 2.3\u00b71019\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls= 0.1[30]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The con\ufb01guration of this model is\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop= 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4and length penalty \u03b1= 0.6[31]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [31].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of \ufb02oating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision \ufb02oating-point capacity of each GPU5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8",
        "dd29fac7-18f3-4a41-81dd-ebf825238856": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d modeldffh d kdvPdrop\u03f5lstrain PPL BLEU params\nsteps (dev) (dev)\u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be bene\ufb01cial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-\ufb01tting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\nresults to the base model.\n7 Conclusion\nIn this work, we presented the Transformer, the \ufb01rst sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained signi\ufb01cantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to ef\ufb01ciently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\n9",
        "abab9326-5a42-4b1f-95fc-9527e8fee2aa": "References\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733 , 2016.\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR , abs/1406.1078, 2014.\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357 , 2016.\n[7]Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 , 2013.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770\u2013778, 2016.\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient \ufb02ow in\nrecurrent nets: the dif\ufb01culty of learning long-term dependencies, 2001.\n[12] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735\u20131780, 1997.\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[14] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR) , 2016.\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n2017.\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nInInternational Conference on Learning Representations , 2017.\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[20] Samy Bengio \u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS) , 2016.\n10",
        "dc51b7f4-f722-4876-af5f-30d2e1108215": "[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n[22] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing , 2016.\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\n[24] O\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859 , 2016.\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine\nLearning Research , 15(1):1929\u20131958, 2014.\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28 , pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\u20133112, 2014.\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n11",
        "01e8d9f2-afbb-4cda-9ecb-a4b73b7e3410": "84    COMMUNICATIONS OF THE ACM   |  JUNE 2017  |   VOL. 60  |   NO. 6research highlights \nDOI:10.1145/3065386\nImageNet Classification with Deep \nConvolutional Neural Networks\nBy Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton\nAbstract\nWe trained a large, deep convolutional neural network to \nclassify the 1.2 million high-resolution images in the \nImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error \nrates of 37.5% and 17.0%, respectively, which is considerably \nbetter than the previous state-of-the-art. The neural network, \nwhich has 60 million parameters and 650,000 neurons, con-\nsists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a \nfinal 1000-way softmax. To make training faster, we used non-\nsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the \nfully connected layers we employed a recently developed reg-\nularization method called \u201cdropout\u201d that proved to be very \neffective. We also entered a variant of this model in the \nILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the sec-\nond-best entry.\n1. PROLOGUE\nFour years ago, a paper by Yann LeCun and his collaborators \nwas rejected by the leading computer vision conference on the grounds that it used neural networks and therefore pro-\nvided no insight into how to design a vision system. At the \ntime, most computer vision researchers believed that a vision \nsystem needed to be carefully hand-designed using a detailed \nunderstanding of the nature of the task. They assumed that the task of classifying objects in natural images would never \nbe solved by simply presenting examples of images and the \nnames of the objects they contained to a neural network that acquired all of its knowledge from this training data.\nWhat many in the vision research community failed to \nappreciate was that methods that require careful hand-engi-\nneering by a programmer who understands the domain do \nnot scale as well as methods that replace the programmer with a powerful general-purpose learning procedure. With \nenough computation and enough data, learning beats pro -\ngramming for complicated tasks that require the integration \nof many different, noisy cues.\nFour years ago, while we were at the University of Toronto, \nour deep neural network called SuperVision almost halved the error rate for recognizing objects in natural images and \ntriggered an overdue paradigm shift in computer vision. \nFigure 4 shows some examples of what SuperVision can do.\nSuperVision evolved from the multilayer neural networks that were widely investigated in the 1980s. These networks used multiple layers of feature detectors that were all learned \nfrom the training data. Neuroscientists and psychologists had \nhypothesized that a hierarchy of such feature detectors would provide a robust way to recognize objects but they had no idea \nhow such a hierarchy could be learned. There was great excite-\nment in the 1980s because several different research groups \ndiscovered that multiple layers of feature detectors could be \ntrained efficiently using a relatively straight-forward algorithm called backpropagation\n18, 22, 27, 33 to compute, for each image, \nhow the classification performance of the whole network \ndepended on the value of the weight on each connection.\nBackpropagation worked well for a variety of tasks, but in \nthe 1980s it did not live up to the very high expectations of its advocates. In particular, it proved to be very difficult to learn \nnetworks with many layers and these were precisely the net-\nworks that should have given the most impressive results. \nMany researchers concluded, incorrectly, that learning a deep neural network from random initial weights was just too \ndifficult. Twenty years later, we know what went wrong: for \ndeep neural networks to shine, they needed far more labeled data and hugely more computation.\n2. INTRODUCTION\nCurrent approaches to object recognition make essential \nuse of machine learning methods. To improve their perfor-mance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfit-ting.",
        "6d9ab157-3e1d-4027-8af5-c3fe180eaaa1": "Backpropagation worked well for a variety of tasks, but in \nthe 1980s it did not live up to the very high expectations of its advocates. In particular, it proved to be very difficult to learn \nnetworks with many layers and these were precisely the net-\nworks that should have given the most impressive results. \nMany researchers concluded, incorrectly, that learning a deep neural network from random initial weights was just too \ndifficult. Twenty years later, we know what went wrong: for \ndeep neural networks to shine, they needed far more labeled data and hugely more computation.\n2. INTRODUCTION\nCurrent approaches to object recognition make essential \nuse of machine learning methods. To improve their perfor-mance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfit-ting. Until recently, datasets of labeled images were rela-\ntively small\u2014on the order of tens of thousands of images \n(e.g., NORB,\n19 Caltech-101/256,8, 10 and CIFAR-10/10014). \nSimple recognition tasks can be solved quite well with datas-\nets of this size, especially if they are augmented with label-\npreserving transformations. For example, the current-best error rate on the MNIST digit-recognition task (<0.3%) \napproaches human performance.\n5 But objects in realistic \nsettings exhibit considerable variability, so to learn to recog-nize them it is necessary to use much larger training sets. \nAnd indeed, the shortcomings of small image datasets have been widely recognized (e.g., Ref.\n25), but it has only recently \nbecome possible to collect labeled datasets with millions of \nThe original version of this paper was published in  \nthe Proceedings of the 25th International Conference on Neu-\nral Information Processing Systems (Lake Tahoe, NV, Dec. \n2012), 1097\u20131105.",
        "804c9dc9-b198-4486-8ccf-65108dfbb6bb": "JUNE 2017  |   VOL. 60  |   NO. 6  |   COMMUNICATIONS OF THE ACM    85images. The new larger datasets include LabelMe,28 which \nconsists of hundreds of thousands of fully segmented \nimages, and ImageNet,7 which consists of over 15 million \nlabeled high-resolution images in over 22,000 categories.\nTo learn about thousands of objects from millions of \nimages, we need a model with a large learning capacity. \nHowever, the immense complexity of the object recogni-\ntion task means that this problem cannot be specified even \nby a dataset as large as ImageNet, so our model should also \nhave lots of prior knowledge to compensate for all the data \nwe do not have. Convolutional neural networks (CNNs) con-stitute one such class of models.\n9, 15, 17, 19, 21, 26, 32 Their capacity \ncan be controlled by varying their depth and breadth, and \nthey also make strong and mostly correct assumptions \nabout the nature of images (namely, stationarity of statis-tics and locality of pixel dependencies). Thus, compared to \nstandard feedforward neural networks with similarly sized \nlayers, CNNs have much fewer connections and parameters \nand so they are easier to train, while their theoretically best \nperformance is likely to be only slightly worse.\nDespite the attractive qualities of CNNs, and despite the \nrelative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-  \nresolution images. Luckily, current GPUs, paired with a \nhighly optimized implementation of 2D convolution, are \npowerful enough to facilitate the training of interestingly-\nlarge CNNs, and recent datasets such as ImageNet contain \nenough labeled examples to train such models without severe overfitting.\nThe specific contributions of this paper are as follows: we \ntrained one of the largest CNNs to date on the subsets of ImageNet used in the ImageNet Large-Scale Visual \nRecognition Challenge (ILSVRC)-2010 and ILSVRC-2012 \ncompetitions\n2 and achieved by far the best results ever \nreported on these datasets. We wrote a highly optimized GPU \nimplementation of 2D convolution and all the other opera-\ntions inherent in training CNNs, which we make available publicly.\na Our network contains a number of new and \nunusual features which improve its performance and reduce \nits training time, which are detailed in Section 4. The size of \nour network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several \neffective techniques for preventing overfitting, which are \ndescribed in Section 5. Our final network contains five convo-\nlutional and three fully connected layers, and this depth \nseems to be important: we found that removing any convolu-tional layer (each of which contains no more than 1% of the \nmodel\u2019s parameters) resulted in inferior performance.\nIn the end, the network\u2019s size is limited mainly by the \namount of memory available on current GPUs and by the amount of training time that we are willing to tolerate. Our \nnetwork takes between 5 and 6 days to train on two GTX 580 \n3GB GPUs. All of our experiments suggest that our results can \nbe improved simply by waiting for faster GPUs and bigger datasets to become available.3. THE DATASET\nImageNet is a dataset of over 15 million labeled high- resolution \nimages belonging to roughly 22,000 categories. The images \nwere collected from the web and labeled by human labelers \nusing Amazon\u2019s Mechanical Turk crowd-sourcing tool. Starting \nin 2010, as part of the Pascal Visual Object Challenge, an annual \ncompetition called the ImageNet Large- Scale Visual \nRecognition Challenge (ILSVRC) has been held. ILSVRC uses a \nsubset of ImageNet with roughly 1000 images in each of 1000 \ncategories. In all, there are roughly 1.2 million training images, \n50,000 validation images, and 150,000 testing images.\nILSVRC-2010 is the only version of ILSVRC for which the \ntest set labels are available, so this is the version on which we performed most of our experiments. Since we also entered \nour model in the ILSVRC-2012 competition, in Section 7 we \nreport our results on this version of the dataset as well, for which test set labels are unavailable.",
        "2c1ab317-014b-4afb-887b-fe09039484ec": "Starting \nin 2010, as part of the Pascal Visual Object Challenge, an annual \ncompetition called the ImageNet Large- Scale Visual \nRecognition Challenge (ILSVRC) has been held. ILSVRC uses a \nsubset of ImageNet with roughly 1000 images in each of 1000 \ncategories. In all, there are roughly 1.2 million training images, \n50,000 validation images, and 150,000 testing images.\nILSVRC-2010 is the only version of ILSVRC for which the \ntest set labels are available, so this is the version on which we performed most of our experiments. Since we also entered \nour model in the ILSVRC-2012 competition, in Section 7 we \nreport our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is cus-\ntomary to report two error rates: top-1 and top-5, where the \ntop-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most \nprobable by the model.\nImageNet consists of variable-resolution images, while \nour system requires a constant input dimensionality. Therefore, we down-sampled the images to a fixed resolution \nof 256 \u00d7 256. Given a rectangular image, we first rescaled the \nimage such that the shorter side was of length 256, and then \ncropped out the central 256 \u00d7 256 patch from the resulting image. We did not pre process the images in any other way, \nexcept for subtracting the mean activity over the training set \nfrom each pixel. So we trained our network on the (centered) raw RGB values of the pixels.\n4. THE ARCHITECTURE\nThe architecture of our network is summarized in Figure 2. It \ncontains eight learned layers\u2014five convolutional and three fully connected. Below, we describe some of the novel or \nunusual features of our network\u2019s architecture. Sections 4.1\u2013\n4.4 are sorted according to our estimation of their impor-tance, with the most important first.\n4.1. Rectified Linear Unit nonlinearity\nThe standard way to model a neuron\u2019s output f  as a function \nof its input x  is with f (x) = tanh(x ) or f (x) = (1 + e\n\u2212x)\u22121. In terms \nof training time with gradient descent, these saturating \nnonlinearities are much slower than the non-saturating \nnonlinearity f(x) = max(0, x ). Following Nair and Hinton,24 \nwe refer to neurons with this non linearity as Rectified \nLinear Units (ReLUs). Deep CNNs with ReLUs train several \ntimes faster than their equivalents with tanh units. This is demonstrated in Figure 1, which shows the number of itera-\ntions required to reach 25% training error on the CIFAR-10 \ndataset for a particular four-layer convolutional network. \nThis plot shows that we would not have been able to experi-\nment with such large neural networks for this work if we had used traditional saturating neuron models.\nWe are not the first to consider alternatives to traditional \nneuron models in CNNs. For example, Jarrett et al.\n13 claim \nthat the nonlinearity f(x) = |tanh( x)| works particularly well \nwith their type of contrast normalization followed by local a http://code.google.com/p/cuda-convnet/.",
        "06ec0d92-c795-4a23-9db4-04317e569e2a": "research highlights  \n \n86    COMMUNICATIONS OF THE ACM   |  JUNE 2017  |   VOL. 60  |   NO. 6average pooling on the Caltech-101 dataset. However, on this \ndataset the primary concern is preventing overfitting, so the effect they are observing is different from the accelerated ability to fit the training set which we report when using ReLUs. Faster learning has a great influence on the perfor-mance of large models trained on large datasets.\n4.2. Training on multiple GPUs\nA single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs. Current GPUs are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to one another\u2019s mem-ory directly, without going through host machine memory. The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one addi-tional trick: the GPUs communicate only in certain layers. This means that, for example, the kernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of con-nectivity is a problem for cross-validation, but this allows us to precisely tune the amount of communication until it is an acceptable fraction of the amount of computation.\nThe resultant architecture is somewhat similar to that of \nthe \u201ccolumnar\u201d CNN employed by Cires \u00b8 an et al.,\n4 except that \nour columns are not independent (see Figure 2). This scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many kernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time to train than the one-GPU net.\nb\n4.3. Local response normalizationReLUs have the desirable property that they do not require input normalization to prevent them from saturating. If at least some training examples produce a positive input to a ReLU, learning will happen in that neuron. However, we still find that the following local normalization scheme aids gen-eralization. Denoting by a\ni\nx, y the activity of a neuron com-\nputed by applying kernel i at position (x, y) and then applying \nthe ReLU nonlinearity, the response-normalized activity bi\nx, y \nis given by the expression\nwhere the sum runs over n  \u201cadjacent\u201d kernel maps at the \nsame spatial position, and N is the total number of kernels in the layer. The ordering of the kernel maps is of course arbi-trary and determined before training begins. This sort of response normalization implements a form of lateral inhibi-tion inspired by the type found in real neurons, creating com-petition for big activities among neuron outputs computed using different kernels. The constants k, n, \u03b1, and \u03b2 are hyper-\nparameters whose values are determined using a validation set; we used k = 2, n = 5, \u03b1 = 10\n\u22124, and \u03b2 = 0.75. We applied this \nnormalization after applying the ReLU nonlinearity in cer-tain layers (see Section 4.5).\nThis scheme bears some resemblance to the local contrast \nnormalization scheme of Jarrett et al.,\n13 but ours would be \nmore correctly termed \u201cbrightness normalization,\u201d since we do not subtract the mean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with  normalization.\nc\n4.4. Overlapping poolingPooling layers in CNNs summarize the outputs of neighbor-ing groups of neurons in the same kernel map. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., Refs.\n5, 13, 20). To be more precise, a pooling \nlayer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of \nsize z \u00d7 z centered at the location of the pooling unit.",
        "de33c8c8-d33d-4eda-858e-3fe227f0621f": "Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with  normalization.\nc\n4.4. Overlapping poolingPooling layers in CNNs summarize the outputs of neighbor-ing groups of neurons in the same kernel map. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., Refs.\n5, 13, 20). To be more precise, a pooling \nlayer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of \nsize z \u00d7 z centered at the location of the pooling unit. If we set \ns = z , we obtain traditional local pooling as commonly \nemployed in CNNs. If we set s  < z , we obtain overlapping 000.25Training error rate0.50.75\n51 01 52 0\nEpochs25 30 35 40Figure 1. A four-layer convolutional neural network with ReLUs \n(solid line) reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line). The learning rates for each network were chosen independently to make training as fast as possible. No regularization of any kind was employed. The magnitude of the effect demonstrated here varies with network architecture, but networks with ReLUs consistently learn several times faster than equivalents with saturating neurons.\nb The one-GPU net actually has the same number of kernels as the two-GPU \nnet in the final convolutional layer. This is because most of the net\u2019s param-eters are in the first fully connected layer, which takes the last convolutional layer as input. So to make the two nets have approximately the same num-ber of parameters, we did not halve the size of the final convolutional layer (nor the fully connected layers which follow). Therefore this comparison is biased in favor of the one-GPU net, since it is bigger than \u201chalf the size\u201d of the two-GPU net.\nc W e cannot describe this network in detail due to space constraints, but it \nis specified precisely by the code and parameter files provided here: http://code.google.com/p/cuda-convnet/.",
        "2a05d9a6-1fab-445c-a9ef-794817645db8": "JUNE 2017  |   VOL. 60  |   NO. 6  |   COMMUNICATIONS OF THE ACM    87of the second convolutional layer. The fourth convolu-\ntional layer has 384 kernels of size 3 \u00d7 3 \u00d7 192, and the fifth \nconvolutional layer has 256 kernels of size 3 \u00d7 3 \u00d7 192. The \nfully connected layers have 4096 neurons each.\n5. REDUCING OVERFITTING\nOur neural network architecture has 60 million parameters. \nAlthough the 1000 classes of ILSVRC make each training \nexample impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so \nmany parameters without considerable overfitting. Below, \nwe describe the two primary ways in which we combat overfitting.\n5.1. Data augmentation\nThe easiest and most common method to reduce overfitting \non image data is to artificially enlarge the dataset using label-preserving transformations (e.g., Refs.\n4, 5, 30). We employ two \ndistinct forms of data augmentation, both of which allow \ntransformed images to be produced from the original images \nwith very little computation, so the transformed images do not need to be stored on disk. In our implementation, the \ntransformed images are generated in Python code on the CPU \nwhile the GPU is training on the previous batch of images. So \nthese data augmentation schemes are, in effect, computa-\ntionally free.\nThe first form of data augmentation consists of generating \nimage translations and horizontal reflections. We do this by extracting random 224 \u00d7 224 patches (and their horizontal reflections) from the 256 \u00d7 256 images and training our net-\nwork on these extracted patches.\nd This increases the size of \nour training set by a factor of 2048, though the resulting train-ing examples are, of course, highly inter dependent. Without \nthis scheme, our network suffers from substantial overfit-ting, which would have forced us to use much smaller net-\nworks. At test time, the network makes a prediction by \nextracting five 224 \u00d7 224 patches (the four corner patches and \nthe center patch) as well as their horizontal reflections (hence \n10 patches in all), and averaging the predictions made by the network\u2019s softmax layer on the ten patches.pooling. This is what we use throughout our network, with s = 2 and z  = 3. This scheme reduces the top-1 and top-5 error \nrates by 0.4% and 0.3%, respectively, as compared with the \nnon overlapping scheme s = 2, z = 2, which produces output of \nequivalent dimensions. We generally observe during training that models with overlapping pooling find it slightly more dif-\nficult to overfit.\n4.5. Overall architecture\nNow we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net contains eight \nlayers with weights; the first five are convolutional and the \nremaining three are fully connected. The output of the last fully connected layer is fed to a 1000-way softmax which \nproduces a distribution over the 1000 class labels. Our net-\nwork maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average \nacross training cases of the log-probability of the correct \nlabel under the prediction distribution.\nThe kernels of the second, fourth, and fifth convolutional \nlayers are connected only to those kernel maps in the previous layer which reside on the same GPU (see Figure 2). The ker-\nnels of the third convolutional layer are connected to all ker-\nnel maps in the second layer. The neurons in the fully-connected layers are connected to all neurons in the \nprevious layer. Response-normalization layers follow the first \nand second convolutional layers. Max-pooling layers, of the \nkind described in Section 4.4, follow both response-normal-\nization layers as well as the fifth convolutional layer. The ReLU non linearity is applied to the output of every convolu-\ntional and fully connected layer.\nThe first convolutional layer filters the 224 \u00d7 224 \u00d7 3 \ninput image with 96 kernels of size 11 \u00d7 11 \u00d7 3 with a stride of 4 pixels (this is the distance between the receptive field \ncenters of neighboring neurons in a kernel map).",
        "446dc478-b255-4c65-ac05-a2bbc79c2e77": "The ker-\nnels of the third convolutional layer are connected to all ker-\nnel maps in the second layer. The neurons in the fully-connected layers are connected to all neurons in the \nprevious layer. Response-normalization layers follow the first \nand second convolutional layers. Max-pooling layers, of the \nkind described in Section 4.4, follow both response-normal-\nization layers as well as the fifth convolutional layer. The ReLU non linearity is applied to the output of every convolu-\ntional and fully connected layer.\nThe first convolutional layer filters the 224 \u00d7 224 \u00d7 3 \ninput image with 96 kernels of size 11 \u00d7 11 \u00d7 3 with a stride of 4 pixels (this is the distance between the receptive field \ncenters of neighboring neurons in a kernel map). The sec-ond convolutional layer takes as input the (response-nor-\nmalized and pooled) output of the first convolutional layer \nand filters it with 256 kernels of size 5 \u00d7 5 \u00d7 48. The third, \nfourth, and fifth convolutional layers are connected to one \nanother without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size \n3 \u00d7 3 \u00d7 256 connected to the (normalized, pooled) outputs \nd This is the reason why the input images in Figure 2 are 224 \u00d7 224 \u00d7 3 dimensional.48 3Stride\nof 4Max\npoolingMax\npoolingMax\npooling55\n11\n224224\n1155\n5485\n27\n27\n128128\n3\n1921921 92\n19233\n3333\n3333\n33\n13 13\n13 13\n128128 2048\n2048 204810002048dense\ndense dense13\n1333\n5\n51111Figure 2. An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU \nruns the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. \nThe network\u2019s input is 150,528-dimensional, and the number of neurons in the network\u2019s remaining layers is given by 290,400\u2013186,624\u2013\n64,896\u201364,896\u201343,264\u20134096\u20134096\u20131000.",
        "7bbb8d9e-cd05-4ff9-acbd-ebd77241acb4": "research highlights  \n \n88    COMMUNICATIONS OF THE ACM   |  JUNE 2017  |   VOL. 60  |   NO. 6The second form of data augmentation consists of altering \nthe intensities of the RGB channels in training images. \nSpecifically, we perform PCA on the set of RGB pixel values \nthroughout the ImageNet training set. To each training image, we add multiples of the found principal components, \nwith magnitudes proportional to the corresponding eigen \nvalues times a random variable drawn from a Gaussian with \nmean 0 and standard deviation 0.1. Therefore to each RGB \nimage pixel I\nxy = [IR\nxy, IG\nxy, IB\nxy]T we add the following quantity:\n[p1, p2, p3] [\u03b11\u03bb1, \u03b12\u03bb2, \u03b13\u03bb3]T,\nwhere pi and \u03bbi are ith eigenvector and eigenvalue of the 3 \u00d7 3 \ncovariance matrix of RGB pixel values, respectively, and \u03b1i is \nthe aforementioned random variable. Each \u03b1i is drawn only \nonce for all the pixels of a particular training image until that \nimage is used for training again, at which point it is re drawn. \nThis scheme approximately captures an important property of natural images, namely, that object identity is invariant to \nchanges in the intensity and color of the illumination. This \nscheme reduces the top-1 error rate by over 1%.\n5.2. Dropout\nCombining the predictions of many different models is a very \nsuccessful way to reduce test errors,\n1, 3 but it appears to be too \nexpensive for big neural networks that already take several days to train. There is, however, a very efficient version of \nmodel combination that only costs about a factor of two dur-ing training. The recently introduced technique, called \n\u201cdropout\u201d,\n12 consists of setting to zero the output of each hid-\nden neuron with probability 0.5. The neurons which are \u201cdropped out\u201d in this way do not contribute to the forward \npass and do not participate in back propagation. So every time an input is presented, the neural network samples a dif-\nferent architecture, but all these architectures share weights. \nThis technique reduces complex co adaptations of neurons, \nsince a neuron cannot rely on the presence of particular other \nneurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random \nsubsets of the other neurons. At test time, we use all the neu-\nrons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predic-\ntive distributions produced by the exponentially-many drop-\nout networks.\nWe use dropout in the first two fully connected layers of \nFigure 2. Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of itera-\ntions required to converge.\n6. DETAILS OF LEARNING\nWe trained our models using stochastic gradient descent \nwith a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. We found that this small amount of \nweight decay was important for the model to learn. In other \nwords, weight decay here is not merely a regularizer: it \nreduces the model\u2019s training error. The update rule for weight \nw waswhere i is the iteration index, u is the momentum variable, \u03b5 is \nthe learning rate, and \u2329wi\u232aDi is the average over the ith batch \nDi of the derivative of the objective with respect to w, evaluated \nat wi.\nWe initialized the weights in each layer from a zero-mean \nGaussian distribution with standard deviation 0.01. We ini-tialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully connected hidden \nlayers, with the constant 1. This initialization accelerates the \nearly stages of learning by providing the ReLUs with positive \ninputs. We initialized the neuron biases in the remaining lay-\ners with the constant 0.\nWe used an equal learning rate for all layers, which we \nadjusted manually throughout training. The heuristic which we followed was to divide the learning rate by 10 when the vali-dation error rate stopped improving with the current learning \nrate. The learning rate was initialized at 0.01 and reduced \nthree times prior to termination. We trained the network for \nroughly 90 cycles through the training set of 1.2 million \nimages, which took 5\u20136 days on two NVIDIA GTX 580 3GB GPUs.\n7.",
        "a9149a8f-694b-4cf8-b18d-a4ccff1b6957": "We ini-tialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully connected hidden \nlayers, with the constant 1. This initialization accelerates the \nearly stages of learning by providing the ReLUs with positive \ninputs. We initialized the neuron biases in the remaining lay-\ners with the constant 0.\nWe used an equal learning rate for all layers, which we \nadjusted manually throughout training. The heuristic which we followed was to divide the learning rate by 10 when the vali-dation error rate stopped improving with the current learning \nrate. The learning rate was initialized at 0.01 and reduced \nthree times prior to termination. We trained the network for \nroughly 90 cycles through the training set of 1.2 million \nimages, which took 5\u20136 days on two NVIDIA GTX 580 3GB GPUs.\n7. RESULTS\nOur results on ILSVRC-2010 are summarized in Table 1. Our \nnetwork achieves top-1 and top-5 test set error rates of 37.5% \nand 17.0%, respectively.\ne The best performance achieved dur-\ning the ILSVRC-2010 competition was 47.1% and 28.2% with \nan approach that averages the predictions produced from six \nsparse-coding models trained on different features,2 and \nsince then the best published results are 45.7% and 25.7% \nwith an approach that averages the predictions of two classi-\nfiers trained on Fisher Vectors (FVs) computed from two types of densely sampled features.\n29\nWe also entered our model in the ILSVRC-2012 competi-\ntion and report our results in Table 2. Since the ILSVRC-2012 \ntest set labels are not publicly available, we cannot report test \nModel Top-1 (%) Top-5 (%)\nSparse coding247.1 28.2\nSIFT + FVs2945.7 25.7\nCNN 37.5 17.0Table 1. Comparison of results on ILSVRC-2010 test set.\nIn italics are best results achieved by others.e The error rates without averaging predictions over 10 patches as described \nin Section 5.1 are 39.0% and 18.3%.\nModel Top-1 (val, %) Top-5 (val, %) Top-5 (test, %)\nSIFT + FVs6\u2013 \u2013 26.2\n1 CNN 40.7 18.2 \u2013\n5 CNNs 38.1 16.4 16.4\n1 CNN* 39.0 16.6 \u2013\n7 CNNs* 36.7 15.4 15.3Table 2. Comparison of error rates on ILSVRC-2012 validation and test sets.\nIn italics are best results achieved by others. Models with an \u201c*\u201d were \u201cpre-trained\u201d to classify \nthe entire ImageNet 2011 Fall release (see Section 7 for details).",
        "69fcb657-09eb-42b5-9147-372584004e59": "JUNE 2017  |   VOL. 60  |   NO. 6  |   COMMUNICATIONS OF THE ACM    89color-specific. This kind of specialization occurs during every \nrun and is independent of any particular random weight ini-\ntialization (modulo a renumbering of the GPUs).\nIn the left panel of Figure 4 we qualitatively assess what the \nnetwork has learned by computing its top-5 predictions on eight test images. Notice that even off-center objects, such as \nthe mite in the top-left, can be recognized by the net. Most of \nthe top-5 labels appear reasonable. For example, only other \ntypes of cat are considered plausible labels for the leopard. In some cases (grille, cherry) there is genuine ambiguity about \nthe intended focus of the photograph.\nAnother way to probe the network\u2019s visual knowledge is to \nconsider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce \nfeature activation vectors with a small Euclidean separation, \nwe can say that the higher levels of the neural network con -\nsider them to be similar. Figure 4 shows five images from the \ntest set and the six images from the training set that are most \nsimilar to each of them according to this measure. Notice that \nat the pixel level, the retrieved training images are generally not close in L2 to the query images in the first column. For \nexample, the retrieved dogs and elephants appear in a variety error rates for all the models that we tried. In the remainder of \nthis paragraph, we use validation and test error rates inter-\nchangeably because in our experience they do not differ by \nmore than 0.1% (see Table 2). The CNN described in this paper achieves a top-5 error rate of 18.2%. Averaging the pre-\ndictions of five similar CNNs gives an error rate of 16.4%. \nTraining one CNN, with an extra sixth convolutional layer \nover the last pooling layer, to classify the entire ImageNet Fall \n2011 release (15M images, 22K categories), and then \u201cfine-tuning\u201d it on ILSVRC-2012 gives an error rate of 16.6%. \nAveraging the predictions of two CNNs that were pre-trained \non the entire Fall 2011 release with the afore mentioned five CNNs gives an error rate of 15.3%. The second-best contest \nentry achieved an error rate of 26.2% with an approach that \naverages the predictions of several classifiers trained on FVs \ncomputed from different types of densely sampled features.\n6\nFinally, we also report our error rates on the Fall 2009 ver-\nsion of ImageNet with 10,184 categories and 8.9 million images. On this dataset we follow the convention in the litera-\nture of using half of the images for training and half for test-ing. Since there is no established test set, our split necessarily \ndiffers from the splits used by previous authors, but this does \nnot affect the results appreciably. Our top-1 and top-5 error \nrates on this dataset are 67.4% and 40.9%, attained by the net \ndescribed above but with an additional, sixth convolutional layer over the last pooling layer. The best published results on \nthis dataset are 78.1% and 60.9%.\n23\n7.1. Qualitative evaluations\nFigure 3 shows the convolutional kernels learned by the net-\nwork\u2019s two data-connected layers. The network has learned a \nvariety of frequency- and orientation-selective kernels, as well as various colored blobs. Notice the specialization exhibited \nby the two GPUs, a result of the restricted connectivity \ndescribed in Section 4.5. The kernels on GPU 1 are largely \ncolor-agnostic, while the kernels on on GPU 2 are largely \nFigure 3. Ninety-six convolutional kernels of size 11 \u00d7 11 \u00d7 3 learned \nby the first convolutional layer on the 224 \u00d7 224 \u00d7 3 input images. \nThe top 48 kernels were learned on GPU 1 while the bottom 48 \nkernels were learned on GPU 2 (see Section 7.1 for details).\nFigure 4. (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5).",
        "d43a81ad-1760-4022-80fd-41a6d534f673": "Notice the specialization exhibited \nby the two GPUs, a result of the restricted connectivity \ndescribed in Section 4.5. The kernels on GPU 1 are largely \ncolor-agnostic, while the kernels on on GPU 2 are largely \nFigure 3. Ninety-six convolutional kernels of size 11 \u00d7 11 \u00d7 3 learned \nby the first convolutional layer on the 224 \u00d7 224 \u00d7 3 input images. \nThe top 48 kernels were learned on GPU 1 while the bottom 48 \nkernels were learned on GPU 2 (see Section 7.1 for details).\nFigure 4. (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5). (Right) Five \nILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last \nhidden layer with the smallest Euclidean distance from the feature vector for the test image.",
        "1e8956e5-a52e-4568-a4c0-a9e95184e913": "research highlights  \n \n90    COMMUNICATIONS OF THE ACM   |  JUNE 2017  |   VOL. 60  |   NO. 6of poses. We present the results for many more test images in \nthe supplementary material.\nComputing similarity by using Euclidean distance \nbetween two 4096-dimensional, real-valued vectors is ineffi-cient, but it could be made efficient by training an auto encoder to compress these vectors to short binary codes. This should produce a much better image retrieval method than applying auto encoders to the raw pixels,\n16 which does not \nmake use of image labels and hence has a tendency to retrieve images with similar patterns of edges, whether or not they are semantically similar.\n8. DISCUSSION\nOur results show that a large, deep CNN is capable of achieving record-breaking results on a highly challenging dataset using purely supervised learning. It is notable that our network\u2019s per-formance degrades if a single convolutional layer is removed. For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results.\nTo simplify our experiments, we did not use any unsuper -\nvised pre-training even though we expect that it will help, espe-cially if we obtain enough computational power to significantly increase the size of the network without obtaining a corre-sponding increase in the amount of labeled data. Thus far, our results have improved as we have made our network larger and trained it longer but we still have many orders of magnitude to go in order to match the infero temporal pathway of the human visual system. Ultimately we would like to use very large and deep convolutional nets on video sequences where the tempo-ral structure provides very helpful information, that is, missing or far less obvious in static images.\n9. EPILOGUE\nThe response of the computer vision community to the suc-cess of SuperVision was impressive. Over the next year or two, they switched to using deep neural networks and these are now widely deployed by Google, Facebook, Microsoft, Baidu and many other companies. By 2015, better hardware, more hidden layers, and a host of technical advances reduced the error rate of deep convolutional neural nets by a further factor of three so that they are now quite close to human perfor-mance for static images.\n11, 31 Much of the credit for this revolu-\ntion should go to the pioneers who spent many years developing the technology of CNNs, but the essential missing ingredient was supplied by FeiFei et al.\n7 who put a huge effort \ninto producing a labeled dataset that was finally large enough to show what neural networks could really do.\n \nReferences\n 1.\n Bell, R., Koren, Y. Lessons from the \nnetflix prize challenge. \nACM SIGKDD \nExplor. Newsl. 9, 2 (2007), 75\u201379.\n 2.\n Berg, A., Deng, J., Fei-Fei, L. Large scale visual recognition challenge 2010. www.image-net.org/challenges. 2010.\n 3.\n Breiman, L. Random forests. \nMach. \nLearn. 45, 1 (2001), 5\u201332.\n 4.\n Cires\n\u00b8an, D., Meier, U., Masci, J., \nGambardella, L., Schmidhuber, J. High-performance neural networks for visual object classification. Arxiv preprint arXiv:1102.0183, 2011.\n 5.\n Cires\n\u00b8an, D., Meier, U., Schmidhuber, J. \nMulti-column deep neural networks for image classification. Arxiv preprint arXiv:1202.2745, 2012.\n 6.\n Deng, J., Berg, A., Satheesh, S., Su, H., Khosla, A., Fei-Fei, L. In \nILSVRC-2012 \n(2012).\n 7.\n Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L. ImageNet: A large-scale hierarchical image database. In\u00a0 CVPR09\n (2009). 8.\n Fei-Fei, L., Fergus, R., Perona, P. Learning generative visual models from few training examples: An\u00a0\nincremental Bayesian approach \ntested on 101 object categories. Comput.",
        "7567e824-cd5e-411c-b218-0ded5cdfecef": "Arxiv preprint arXiv:1202.2745, 2012.\n 6.\n Deng, J., Berg, A., Satheesh, S., Su, H., Khosla, A., Fei-Fei, L. In \nILSVRC-2012 \n(2012).\n 7.\n Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L. ImageNet: A large-scale hierarchical image database. In\u00a0 CVPR09\n (2009). 8.\n Fei-Fei, L., Fergus, R., Perona, P. Learning generative visual models from few training examples: An\u00a0\nincremental Bayesian approach \ntested on 101 object categories. Comput. Vision Image Understanding 106, 1 (2007), 59\u201370.\n 9.\n Fukushima, K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. \nBiol. \nCybern. 36, 4 (1980), 193\u2013202.\n 10.\n Griffin, G., Holub, A., Perona, P. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007.\n 11.\n He, K., Zhang, X., Ren, S., Sun,\u00a0\nJ. Deep \nresidual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.\n 12.\n Hinton, G., Srivastava, N., Krizhevsky,\u00a0\nA., Sutskever, I., \nSalakhutdinov,\u00a0R. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012).\n 13.\n Jarrett, K., Kavukcuoglu, K., Ranzato,\u00a0\nM.A., LeCun, Y. What is the \nbest multi-stage architecture for object recognition? In International Conference on Computer Vision (2009). IEEE, 2146\u20132153.\n 14.\n Krizhevsky, A. Learning multiple layers of features from tiny images. Master\u2019s thesis, Department of Computer Science, University of Toronto, 2009.\n 15.\n Krizhevsky, A. Convolutional deep belief networks on cifar-10. Unpublished manuscript\n, 2010.\n 16.\n Krizhevsky, A., Hinton, G. Using very deep autoencoders for content-based image retrieval. In \nESANN (2011).\n 17.\n LeCun, Y., Boser, B., Denker, J.,\n \nHenderson, D., Howard, R., Hubbard, W., Jackel, L., et al. Handwritten digit recognition with a back-propagation network. In Advances in Neural Information Processing Systems (1990).\n 18.\n LeCun, Y. Une procedure\n \nd\u2019apprentissage pour reseau a seuil asymmetrique (a learning scheme for asymmetric threshold networks). 1985.\n 19.\n LeCun, Y., Huang, F., Bottou, L. Learning methods for generic object recognition with invariance to pose and lighting. In \nProceedings of the 2004 \nIEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004, CVPR 2004. Volume 2 (2004). IEEE, II\u201397.\n 20.\n LeCun, Y., Kavukcuoglu, K., Farabet, C. Convolutional networks and applications in vision. In \nProceedings \nof 2010 IEEE International Symposium on Circuits and Systems (ISCAS) (2010). IEEE, 253\u2013256.\n 21.\n Lee, H., Grosse, R., Ranganath, R., Ng,\n \nA. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In\u00a0 Proceedings of \nthe 26th Annual International Conference on Machine Learning  \n(2009). ACM, 609\u2013616.\n 22.\n Linnainmaa, S. Taylor expansion of the accumulated rounding error. \nBIT \nNumer. Math. 16, 2 (1976), 146\u2013160.\n 23.",
        "821d0fb8-34c9-4bee-9335-fd79c846c3c5": "Volume 2 (2004). IEEE, II\u201397.\n 20.\n LeCun, Y., Kavukcuoglu, K., Farabet, C. Convolutional networks and applications in vision. In \nProceedings \nof 2010 IEEE International Symposium on Circuits and Systems (ISCAS) (2010). IEEE, 253\u2013256.\n 21.\n Lee, H., Grosse, R., Ranganath, R., Ng,\n \nA. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In\u00a0 Proceedings of \nthe 26th Annual International Conference on Machine Learning  \n(2009). ACM, 609\u2013616.\n 22.\n Linnainmaa, S. Taylor expansion of the accumulated rounding error. \nBIT \nNumer. Math. 16, 2 (1976), 146\u2013160.\n 23.\n Mensink, T., Verbeek, J., Perronnin, F.,\n \n Csurka, G. Metric learning for large scale image classification: Generalizing to new classes at near-zero cost. In ECCV \u2013 European Conference on Computer Vision (Florence, Italy, Oct. 2012).\n 24.\n Nair, V., Hinton, G.E. Rectified linear units improve restricted Boltzmann machines. In \nProceedings of the 27th \nInternational Conference on Machine Learning (2010).\n 25.\n Pinto, N., Cox, D., DiCarlo, J. Why is real-world visual object recognition hard? PLoS Comput. Biol. 4\n, 1 (2008), \ne27.\n 26.\n Pinto, N., Doukhan, D., DiCarlo, J., Cox, D. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS Comput. Biol. 5\n, \n11 (2009), e1000579.\n 27.\n Rumelhart, D.E., Hinton, G.E., Williams, R.J. Learning internal representations by error propagation. Technical report, DTIC\u00a0\nDocument, 1985.\n 28.\n Russell, BC, Torralba, A., Murphy, K., Freeman, W. Labelme: A database and web-based tool for image annotation. Int. J. Comput Vis. 77\n, 1 (2008), \n157\u2013173.\n 29.\n S\u00e1nchez, J., Perronnin, F. High-dimensional signature compression for large-scale image classification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011\n \n(2011). IEEE, 1665\u20131672.\n 30.\n Simard, P., Steinkraus, D., Platt, J. Best practices for convolutional neural networks applied to visual document analysis. In \nProceedings of the \nSeventh International Conference on Document Analysis and Recognition. Volume 2 (2003), 958\u2013962.\n 31.\n Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan,\u00a0\nD., \nVanhoucke, V., Rabinovich, A.  Going deeper with convolutions. In\u00a0Proceedings of the IEEE Conference \non Computer Vision and Pattern Recognition (2015), 1\u20139.\n 32.\n Turaga, S., Murray, J., Jain, V., Roth, F., Helmstaedter, M., Briggman, K., Denk, W., Seung, H. Convolutional networks can learn to generate affinity graphs for image segmentation. \nNeural \nComput. 22, 2 (2010), 511\u2013538.\n 33.\n Werbos, P. Beyond regression: New tools for prediction and analysis in the\u00a0\nbehavioral sciences, 1974.\nAlex Krizhevsky and Geoffrey E. Hinton ({akrizhevsky, geoffhinton}@google.com), Google Inc.Ilya Sutskever (ilyasu@openai.com), OpenAI.\nCopyright held by Authors/Owners.",
        "53108509-33f0-4f9c-b63b-a9d2052d66e9": "Fast R-CNN\nRoss Girshick\nMicrosoft Research\nrbg@microsoft.com\nAbstract\nThis paper proposes a Fast Region-based Convolutional\nNetwork method (Fast R-CNN) for object detection. Fast\nR-CNN builds on previous work to ef\ufb01ciently classify ob-\nject proposals using deep convolutional networks. Com-\npared to previous work, Fast R-CNN employs several in-\nnovations to improve training and testing speed while also\nincreasing detection accuracy. Fast R-CNN trains the very\ndeep VGG16 network 9 \u00d7faster than R-CNN, is 213 \u00d7faster\nat test-time, and achieves a higher mAP on PASCAL VOC\n2012. Compared to SPPnet, Fast R-CNN trains VGG16 3 \u00d7\nfaster, tests 10 \u00d7faster, and is more accurate. Fast R-CNN\nis implemented in Python and C++ (using Caffe) and is\navailable under the open-source MIT License at https:\n//github.com/rbgirshick/fast-rcnn .\n1. Introduction\nRecently, deep ConvNets [ 14,16] have signi\ufb01cantly im-\nproved image classi\ufb01cation [ 14] and object detection [ 9,19]\naccuracy. Compared to image classi\ufb01cation, object detec-\ntion is a more challenging task that requires more com-\nplex methods to solve. Due to this complexity, current ap-\nproaches ( e.g., [9,11,19,25]) train models in multi-stage\npipelines that are slow and inelegant.\nComplexity arises because detection requires the ac-\ncurate localization of objects, creating two primary chal-\nlenges. First, numerous candidate object locations (often\ncalled \u201cproposals\u201d) must be processed. Second, these can-\ndidates provide only rough localization that must be re\ufb01ned\nto achieve precise localization. Solutions to these problems\noften compromise speed, accuracy, or simplicity.\nIn this paper, we streamline the training process for state-\nof-the-art ConvNet-based object detectors [ 9,11]. We pro-\npose a single-stage training algorithm that jointly learns to\nclassify object proposals and re\ufb01ne their spatial locations.\nThe resulting method can train a very deep detection\nnetwork (VGG16 [ 20]) 9\u00d7faster than R-CNN [ 9] and 3\u00d7\nfaster than SPPnet [ 11]. At runtime, the detection network\nprocesses images in 0.3s (excluding object proposal time)while achieving top accuracy on PASCAL VOC 2012 [ 7]\nwith a mAP of 66% (vs. 62% for R-CNN).1\n1.1. R\u00adCNN and SPPnet\nThe Region-based Convolutional Network method (R-\nCNN) [ 9] achieves excellent object detection accuracy by\nusing a deep ConvNet to classify object proposals. R-CNN,\nhowever, has notable drawbacks:\n1.Training is a multi-stage pipeline. R-CNN \ufb01rst \ufb01ne-\ntunes a ConvNet on object proposals using log loss.\nThen, it \ufb01ts SVMs to ConvNet features. These SVMs\nact as object detectors, replacing the softmax classi-\n\ufb01er learnt by \ufb01ne-tuning. In the third training stage,\nbounding-box regressors are learned.\n2.Training is expensive in space and time. For SVM\nand bounding-box regressor training, features are ex-\ntracted from each object proposal in each image and\nwritten to disk. With very deep networks, such as\nVGG16, this process takes 2.5 GPU-days for the 5k\nimages of the VOC07 trainval set. These features re-\nquire hundreds of gigabytes of storage.\n3.Object detection is slow. At test-time, features are\nextracted from each object proposal in each test image.\nDetection with VGG16 takes 47s / image (on a GPU).\nR-CNN is slow because it performs a ConvNet forward\npass for each object proposal, without sharing computation.\nSpatial pyramid pooling networks (SPPnets) [ 11] were pro-\nposed to speed up R-CNN by sharing computation. The\nSPPnet method computes a convolutional feature map for\nthe entire input image and then classi\ufb01es each object pro-\nposal using a feature vector extracted from the shared fea-\nture map.",
        "7da59371-5fbb-41e6-b179-b782a26a4804": "With very deep networks, such as\nVGG16, this process takes 2.5 GPU-days for the 5k\nimages of the VOC07 trainval set. These features re-\nquire hundreds of gigabytes of storage.\n3.Object detection is slow. At test-time, features are\nextracted from each object proposal in each test image.\nDetection with VGG16 takes 47s / image (on a GPU).\nR-CNN is slow because it performs a ConvNet forward\npass for each object proposal, without sharing computation.\nSpatial pyramid pooling networks (SPPnets) [ 11] were pro-\nposed to speed up R-CNN by sharing computation. The\nSPPnet method computes a convolutional feature map for\nthe entire input image and then classi\ufb01es each object pro-\nposal using a feature vector extracted from the shared fea-\nture map. Features are extracted for a proposal by max-\npooling the portion of the feature map inside the proposal\ninto a \ufb01xed-size output ( e.g.,6\u00d76). Multiple output sizes\nare pooled and then concatenated as in spatial pyramid pool-\ning [ 15]. SPPnet accelerates R-CNN by 10 to 100 \u00d7at test\ntime. Training time is also reduced by 3 \u00d7due to faster pro-\nposal feature extraction.\n1All timings use one Nvidia K40 GPU overclocked to 875 MHz.\n1440",
        "73f4df85-088e-4332-93e0-0a1319006cca": "SPPnet also has notable drawbacks. Like R-CNN, train-\ning is a multi-stage pipeline that involves extracting fea-\ntures, \ufb01ne-tuning a network with log loss, training SVMs,\nand \ufb01nally \ufb01tting bounding-box regressors. Features are\nalso written to disk. But unlike R-CNN, the \ufb01ne-tuning al-\ngorithm proposed in [ 11] cannot update the convolutional\nlayers that precede the spatial pyramid pooling. Unsurpris-\ningly, this limitation (\ufb01xed convolutional layers) limits the\naccuracy of very deep networks.\n1.2. Contributions\nWe propose a new training algorithm that \ufb01xes the disad-\nvantages of R-CNN and SPPnet, while improving on their\nspeed and accuracy. We call this method Fast R-CNN be-\ncause it\u2019s comparatively fast to train and test. The Fast R-\nCNN method has several advantages:\n1. Higher detection quality (mAP) than R-CNN, SPPnet\n2. Training is single-stage, using a multi-task loss\n3. Training can update all network layers\n4. No disk storage is required for feature caching\nFast R-CNN is written in Python and C++ (Caffe\n[13]) and is available under the open-source MIT Li-\ncense at https://github.com/rbgirshick/\nfast-rcnn .\n2. Fast R-CNN architecture and training\nFig. 1illustrates the Fast R-CNN architecture. A Fast\nR-CNN network takes as input an entire image and a set\nof object proposals. The network \ufb01rst processes the whole\nimage with several convolutional ( conv) and max pooling\nlayers to produce a conv feature map. Then, for each ob-\nject proposal a region of interest ( RoI) pooling layer ex-\ntracts a \ufb01xed-length feature vector from the feature map.\nEach feature vector is fed into a sequence of fully connected\n(fc) layers that \ufb01nally branch into two sibling output lay-\ners: one that produces softmax probability estimates over\nKobject classes plus a catch-all \u201cbackground\u201d class and\nanother layer that outputs four real-valued numbers for each\nof theKobject classes. Each set of 4values encodes re\ufb01ned\nbounding-box positions for one of the Kclasses.\n2.1. The RoI pooling layer\nThe RoI pooling layer uses max pooling to convert the\nfeatures inside any valid region of interest into a small fea-\nture map with a \ufb01xed spatial extent of H\u00d7W(e.g.,7\u00d77),\nwhereHandWare layer hyper-parameters that are inde-\npendent of any particular RoI. In this paper, an RoI is a\nrectangular window into a conv feature map. Each RoI is\nde\ufb01ned by a four-tuple (r,c,h,w)that speci\ufb01es its top-left\ncorner(r,c)and its height and width (h,w).Deep\nConvNet\nConv\nfeature mapRoI\nprojectionRoI\npooling\nlayerFCs\nRoI feature\nvectorsoftmaxbbox\nregressorOutputs :\nFC FC\nFor each RoI\nFigure 1. Fast R-CNN architecture. An input image and multi-\nple regions of interest (RoIs) are input into a fully convolutional\nnetwork. Each RoI is pooled into a \ufb01xed-size feature map and\nthen mapped to a feature vector by fully connected layers (FCs).\nThe network has two output vectors per RoI: softmax probabilities\nand per-class bounding-box regression offsets. The architecture is\ntrained end-to-end with a multi-task loss.\nRoI max pooling works by dividing the h\u00d7wRoI win-\ndow into an H\u00d7Wgrid of sub-windows of approximate\nsizeh/H\u00d7w/W and then max-pooling the values in each\nsub-window into the corresponding output grid cell. Pool-\ning is applied independently to each feature map channel,\nas in standard max pooling. The RoI layer is simply the\nspecial-case of the spatial pyramid pooling layer used in\nSPPnets [ 11] in which there is only one pyramid level. We\nuse the pooling sub-window calculation given in [ 11].\n2.2. Initializing from pre\u00adtrained networks\nWe experiment with three pre-trained ImageNet [ 4] net-\nworks, each with \ufb01ve max pooling layers and between \ufb01ve\nand thirteen conv layers (see Section 4.1for network de-\ntails).",
        "14cd157d-302f-4369-82b4-1e1931357725": "The architecture is\ntrained end-to-end with a multi-task loss.\nRoI max pooling works by dividing the h\u00d7wRoI win-\ndow into an H\u00d7Wgrid of sub-windows of approximate\nsizeh/H\u00d7w/W and then max-pooling the values in each\nsub-window into the corresponding output grid cell. Pool-\ning is applied independently to each feature map channel,\nas in standard max pooling. The RoI layer is simply the\nspecial-case of the spatial pyramid pooling layer used in\nSPPnets [ 11] in which there is only one pyramid level. We\nuse the pooling sub-window calculation given in [ 11].\n2.2. Initializing from pre\u00adtrained networks\nWe experiment with three pre-trained ImageNet [ 4] net-\nworks, each with \ufb01ve max pooling layers and between \ufb01ve\nand thirteen conv layers (see Section 4.1for network de-\ntails). When a pre-trained network initializes a Fast R-CNN\nnetwork, it undergoes three transformations.\nFirst, the last max pooling layer is replaced by a RoI\npooling layer that is con\ufb01gured by setting HandWto be\ncompatible with the net\u2019s \ufb01rst fully connected layer ( e.g.,\nH=W= 7for VGG16).\nSecond, the network\u2019s last fully connected layer and soft-\nmax (which were trained for 1000-way ImageNet classi\ufb01-\ncation) are replaced with the two sibling layers described\nearlier (a fully connected layer and softmax over K+1cat-\negories and category-speci\ufb01c bounding-box regressors).\nThird, the network is modi\ufb01ed to take two data inputs: a\nlist of images and a list of RoIs in those images.\n2.3. Fine\u00adtuning for detection\nTraining all network weights with back-propagation is an\nimportant capability of Fast R-CNN. First, let\u2019s elucidate\nwhy SPPnet is unable to update weights below the spatial\npyramid pooling layer.\nThe root cause is that back-propagation through the SPP\nlayer is highly inef\ufb01cient when each training sample ( i.e.\nRoI) comes from a different image, which is exactly how\nR-CNN and SPPnet networks are trained. The inef\ufb01ciency\n1441",
        "fbb1ab2b-3d84-45be-b39e-68f48d584514": "stems from the fact that each RoI may have a very large\nreceptive \ufb01eld, often spanning the entire input image. Since\nthe forward pass must process the entire receptive \ufb01eld, the\ntraining inputs are large (often the entire image).\nWe propose a more ef\ufb01cient training method that takes\nadvantage of feature sharing during training. In Fast R-\nCNN training, stochastic gradient descent (SGD) mini-\nbatches are sampled hierarchically, \ufb01rst by sampling Nim-\nages and then by sampling R/N RoIs from each image.\nCritically, RoIs from the same image share computation\nand memory in the forward and backward passes. Making\nNsmall decreases mini-batch computation. For example,\nwhen using N= 2 andR= 128 , the proposed training\nscheme is roughly 64 \u00d7faster than sampling one RoI from\n128different images ( i.e., the R-CNN and SPPnet strategy).\nOne concern over this strategy is it may cause slow train-\ning convergence because RoIs from the same image are cor-\nrelated. This concern does not appear to be a practical issue\nand we achieve good results with N= 2 andR= 128\nusing fewer SGD iterations than R-CNN.\nIn addition to hierarchical sampling, Fast R-CNN uses a\nstreamlined training process with one \ufb01ne-tuning stage that\njointly optimizes a softmax classi\ufb01er and bounding-box re-\ngressors, rather than training a softmax classi\ufb01er, SVMs,\nand regressors in three separate stages [ 9,11]. The compo-\nnents of this procedure (the loss, mini-batch sampling strat-\negy, back-propagation through RoI pooling layers, and SGD\nhyper-parameters) are described below.\nMulti-task loss. A Fast R-CNN network has two sibling\noutput layers. The \ufb01rst outputs a discrete probability distri-\nbution (per RoI), p= (p0,...,p K), overK+1categories.\nAs usual, pis computed by a softmax over the K+1outputs\nof a fully connected layer. The second sibling layer outputs\nbounding-box regression offsets, tk=(\ntk\nx,tk\ny,tk\nw,tk\nh)\n, for\neach of the Kobject classes, indexed by k. We use the pa-\nrameterization for tkgiven in [ 9], in which tkspeci\ufb01es a\nscale-invariant translation and log-space height/width shift\nrelative to an object proposal.\nEach training RoI is labeled with a ground-truth class u\nand a ground-truth bounding-box regression target v. We\nuse a multi-task loss Lon each labeled RoI to jointly train\nfor classi\ufb01cation and bounding-box regression:\nL(p,u,tu,v) =Lcls(p,u)+\u03bb[u\u22651]Lloc(tu,v),(1)\nin whichLcls(p,u) =\u2212logpuis log loss for true class u.\nThe second task loss, Lloc, is de\ufb01ned over a tuple of\ntrue bounding-box regression targets for class u,v=\n(vx,vy,vw,vh), and a predicted tuple tu= (tu\nx,tu\ny,tu\nw,tu\nh),\nagain for class u. The Iverson bracket indicator function\n[u\u22651]evaluates to 1 when u\u22651and 0 otherwise. By\nconvention the catch-all background class is labeled u= 0.\nFor background RoIs there is no notion of a ground-truthbounding box and hence Llocis ignored. For bounding-box\nregression, we use the loss\nLloc(tu,v) =\u2211\ni\u2208{x,y,w,h}smooth L1(tu\ni\u2212vi), (2)\nin which\nsmooth L1(x) ={\n0.5x2if|x|<1\n|x|\u22120.5otherwise ,(3)\nis a robust L1loss that is less sensitive to outliers than the\nL2loss used in R-CNN and SPPnet. When the regression\ntargets are unbounded, training with L2loss can require\ncareful tuning of learning rates in order to prevent exploding\ngradients. Eq. 3eliminates this sensitivity.\nThe hyper-parameter \u03bbin Eq. 1controls the balance be-\ntween the two task losses. We normalize the ground-truth\nregression targets vito have zero mean and unit variance.\nAll experiments use \u03bb= 1.",
        "8ee2a4d7-1a8a-479c-a6a0-42b084d864be": "For bounding-box\nregression, we use the loss\nLloc(tu,v) =\u2211\ni\u2208{x,y,w,h}smooth L1(tu\ni\u2212vi), (2)\nin which\nsmooth L1(x) ={\n0.5x2if|x|<1\n|x|\u22120.5otherwise ,(3)\nis a robust L1loss that is less sensitive to outliers than the\nL2loss used in R-CNN and SPPnet. When the regression\ntargets are unbounded, training with L2loss can require\ncareful tuning of learning rates in order to prevent exploding\ngradients. Eq. 3eliminates this sensitivity.\nThe hyper-parameter \u03bbin Eq. 1controls the balance be-\ntween the two task losses. We normalize the ground-truth\nregression targets vito have zero mean and unit variance.\nAll experiments use \u03bb= 1.\nWe note that [ 6] uses a related loss to train a class-\nagnostic object proposal network. Different from our ap-\nproach, [ 6] advocates for a two-network system that sepa-\nrates localization and classi\ufb01cation. OverFeat [ 19], R-CNN\n[9], and SPPnet [ 11] also train classi\ufb01ers and bounding-box\nlocalizers, however these methods use stage-wise training,\nwhich we show is suboptimal for Fast R-CNN (Section 5.1).\nMini-batch sampling. During \ufb01ne-tuning, each SGD\nmini-batch is constructed from N= 2images, chosen uni-\nformly at random (as is common practice, we actually iter-\nate over permutations of the dataset). We use mini-batches\nof sizeR= 128 , sampling 64RoIs from each image. As\nin [9], we take 25% of the RoIs from object proposals that\nhave intersection over union (IoU) overlap with a ground-\ntruth bounding box of at least 0.5. These RoIs comprise\nthe examples labeled with a foreground object class, i.e.\nu\u22651. The remaining RoIs are sampled from object pro-\nposals that have a maximum IoU with ground truth in the in-\nterval[0.1,0.5), following [ 11]. These are the background\nexamples and are labeled with u= 0. The lower threshold\nof0.1appears to act as a heuristic for hard example mining\n[8]. During training, images are horizontally \ufb02ipped with\nprobability 0.5. No other data augmentation is used.\nBack-propagation through RoI pooling layers. Back-\npropagation routes derivatives through the RoI pooling\nlayer. For clarity, we assume only one image per mini-batch\n(N= 1), though the extension to N >1is straightforward\nbecause the forward pass treats all images independently.\nLetxi\u2208Rbe thei-th activation input into the RoI pool-\ning layer and let yrjbe the layer\u2019s j-th output from the r-\nth RoI. The RoI pooling layer computes yrj=xi\u2217(r,j), in\nwhichi\u2217(r,j) = argmaxi\u2032\u2208R(r,j)xi\u2032.R(r,j)is the index\n1442",
        "c08d39f5-078b-4d21-91f1-8d80cc6b5d73": "set of inputs in the sub-window over which the output unit\nyrjmax pools. A single ximay be assigned to several dif-\nferent outputs yrj.\nThe RoI pooling layer\u2019s backwards function computes\npartial derivative of the loss function with respect to each\ninput variable xiby following the argmax switches:\n\u2202L\n\u2202xi=\u2211\nr\u2211\nj[i=i\u2217(r,j)]\u2202L\n\u2202yrj. (4)\nIn words, for each mini-batch RoI rand for each pooling\noutput unit yrj, the partial derivative \u2202L/\u2202y rjis accumu-\nlated ifiis the argmax selected for yrjby max pooling.\nIn back-propagation, the partial derivatives \u2202L/\u2202y rjare al-\nready computed by the backwards function of the layer\non top of the RoI pooling layer.\nSGD hyper-parameters. The fully connected layers used\nfor softmax classi\ufb01cation and bounding-box regression are\ninitialized from zero-mean Gaussian distributions with stan-\ndard deviations 0.01and0.001, respectively. Biases are ini-\ntialized to 0. All layers use a per-layer learning rate of 1 for\nweights and 2 for biases and a global learning rate of 0.001.\nWhen training on VOC07 or VOC12 trainval we run SGD\nfor 30k mini-batch iterations, and then lower the learning\nrate to0.0001 and train for another 10k iterations. When\nwe train on larger datasets, we run SGD for more iterations,\nas described later. A momentum of 0.9and parameter decay\nof0.0005 (on weights and biases) are used.\n2.4. Scale invariance\nWe explore two ways of achieving scale invariant ob-\nject detection: (1) via \u201cbrute force\u201d learning and (2) by us-\ning image pyramids. These strategies follow the two ap-\nproaches in [ 11]. In the brute-force approach, each image\nis processed at a pre-de\ufb01ned pixel size during both training\nand testing. The network must directly learn scale-invariant\nobject detection from the training data.\nThe multi-scale approach, in contrast, provides approx-\nimate scale-invariance to the network through an image\npyramid. At test-time, the image pyramid is used to ap-\nproximately scale-normalize each object proposal. During\nmulti-scale training, we randomly sample a pyramid scale\neach time an image is sampled, following [ 11], as a form of\ndata augmentation. We experiment with multi-scale train-\ning for smaller networks only, due to GPU memory limits.\n3. Fast R-CNN detection\nOnce a Fast R-CNN network is \ufb01ne-tuned, detection\namounts to little more than running a forward pass (assum-\ning object proposals are pre-computed). The network takes\nas input an image (or an image pyramid, encoded as a list\nof images) and a list of Robject proposals to score. Attest-time, Ris typically around 2000 , although we will con-\nsider cases in which it is larger ( \u224845k). When using an\nimage pyramid, each RoI is assigned to the scale such that\nthe scaled RoI is closest to 2242pixels in area [ 11].\nFor each test RoI r, the forward pass outputs a class\nposterior probability distribution pand a set of predicted\nbounding-box offsets relative to r(each of the Kclasses\ngets its own re\ufb01ned bounding-box prediction). We assign a\ndetection con\ufb01dence to rfor each object class kusing the\nestimated probability Pr (class=k|r)\u2206=pk. We then\nperform non-maximum suppression independently for each\nclass using the algorithm and settings from R-CNN [ 9].\n3.1. Truncated SVD for faster detection\nFor whole-image classi\ufb01cation, the time spent comput-\ning the fully connected layers is small compared to the conv\nlayers. On the contrary, for detection the number of RoIs\nto process is large and nearly half of the forward pass time\nis spent computing the fully connected layers (see Fig. 2).\nLarge fully connected layers are easily accelerated by com-\npressing them with truncated SVD [ 5,23].\nIn this technique, a layer parameterized by the u\u00d7v\nweight matrix Wis approximately factorized as\nW\u2248U\u03a3tVT(5)\nusing SVD.",
        "ec38cd34-aa8a-4dd8-9dce-d80c6cdb3d61": "We then\nperform non-maximum suppression independently for each\nclass using the algorithm and settings from R-CNN [ 9].\n3.1. Truncated SVD for faster detection\nFor whole-image classi\ufb01cation, the time spent comput-\ning the fully connected layers is small compared to the conv\nlayers. On the contrary, for detection the number of RoIs\nto process is large and nearly half of the forward pass time\nis spent computing the fully connected layers (see Fig. 2).\nLarge fully connected layers are easily accelerated by com-\npressing them with truncated SVD [ 5,23].\nIn this technique, a layer parameterized by the u\u00d7v\nweight matrix Wis approximately factorized as\nW\u2248U\u03a3tVT(5)\nusing SVD. In this factorization, Uis au\u00d7tmatrix com-\nprising the \ufb01rst tleft-singular vectors of W,\u03a3tis at\u00d7t\ndiagonal matrix containing the top tsingular values of W,\nandVisv\u00d7tmatrix comprising the \ufb01rst tright-singular\nvectors of W. Truncated SVD reduces the parameter count\nfromuvtot(u+v), which can be signi\ufb01cant if tis much\nsmaller than min(u,v). To compress a network, the single\nfully connected layer corresponding to Wis replaced by\ntwo fully connected layers, without a non-linearity between\nthem. The \ufb01rst of these layers uses the weight matrix \u03a3tVT\n(and no biases) and the second uses U(with the original bi-\nases associated with W). This simple compression method\ngives good speedups when the number of RoIs is large.\n4. Main results\nThree main results support this paper\u2019s contributions:\n1. State-of-the-art mAP on VOC07, 2010, and 2012\n2. Fast training and testing compared to R-CNN, SPPnet\n3. Fine-tuning conv layers in VGG16 improves mAP\n4.1. Experimental setup\nOur experiments use three pre-trained ImageNet models\nthat are available online.2The \ufb01rst is the CaffeNet (essen-\ntially AlexNet [ 14]) from R-CNN [ 9]. We alternatively refer\n2https://github.com/BVLC/caffe/wiki/Model-Zoo\n1443",
        "5a3877dd-9170-48bb-8c58-1641195a7c1e": "method train set aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP\nSPPnet BB [ 11]\u202007\\diff 73.9 72.3 62.5 51.5 44.4 74.4 73.0 74.4 42.3 73.6 57.7 70.3 74.6 74.3 54.2 34.0 56.4 56.4 67.9 73.5 63.1\nR-CNN BB [ 10]07 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\nFRCN [ours] 07 74.5 78.3 69.2 53.2 36.6 77.3 78.2 82.0 40.7 72.7 67.9 79.6 79.2 73.0 69.0 30.1 65.4 70.2 75.8 65.8 66.9\nFRCN [ours] 07\\diff 74.6 79.0 68.6 57.0 39.3 79.5 78.6 81.9 48.0 74.0 67.4 80.5 80.7 74.1 69.6 31.8 67.1 68.4 75.3 65.5 68.1\nFRCN [ours] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0\nTable 1. VOC 2007 test detection average precision (%). All methods use VGG16. Training set key: 07: VOC07 trainval, 07\\diff:07\nwithout \u201cdif\ufb01cult\u201d examples, 07+12 : union of 07and VOC12 trainval.\u2020SPPnet results were prepared by the authors of [ 11].\nmethod train set aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP\nBabyLearning Prop.",
        "dd5890fd-7926-4d1e-84b8-bc3f3e91990b": "VOC 2007 test detection average precision (%). All methods use VGG16. Training set key: 07: VOC07 trainval, 07\\diff:07\nwithout \u201cdif\ufb01cult\u201d examples, 07+12 : union of 07and VOC12 trainval.\u2020SPPnet results were prepared by the authors of [ 11].\nmethod train set aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP\nBabyLearning Prop. 77.7 73.8 62.3 48.8 45.4 67.3 67.0 80.3 41.3 70.8 49.7 79.5 74.7 78.6 64.5 36.0 69.9 55.7 70.4 61.7 63.8\nR-CNN BB [ 10]12 79.3 72.4 63.1 44.0 44.4 64.6 66.3 84.9 38.8 67.3 48.4 82.3 75.0 76.7 65.7 35.8 66.2 54.8 69.1 58.8 62.9\nSegDeepM 12+seg 82.3 75.2 67.1 50.7 49.8 71.1 69.6 88.2 42.5 71.2 50.0 85.7 76.6 81.8 69.3 41.5 71.9 62.2 73.2 64.6 67.2\nFRCN [ours] 12 80.1 74.4 67.7 49.4 41.4 74.2 68.8 87.8 41.9 70.1 50.2 86.1 77.3 81.1 70.4 33.3 67.0 63.3 77.2 60.0 66.1\nFRCN [ours] 07++12 82.0 77.8 71.6 55.3 42.4 77.3 71.7 89.3 44.5 72.1 53.7 87.7 80.0 82.5 72.7 36.6 68.7 65.4 81.1 62.7 68.8\nTable 2. VOC 2010 test detection average precision (%). BabyLearning uses a network based on [ 17]. All other methods use VGG16.\nTraining set key: 12: VOC12 trainval, Prop. : proprietary dataset, 12+seg :12with segmentation annotations, 07++12 : union of VOC07\ntrainval, VOC07 test, and VOC12 trainval.\nmethod train set aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP\nBabyLearning Prop. 78.0 74.2 61.3 45.7 42.7 68.2 66.8 80.2 40.6 70.0 49.8 79.0 74.5 77.9 64.0 35.3 67.9 55.7 68.7 62.6 63.2\nNUS NIN c2000 Unk.",
        "92955ec2-0c31-42d7-bbf7-51a8f246680a": "All other methods use VGG16.\nTraining set key: 12: VOC12 trainval, Prop. : proprietary dataset, 12+seg :12with segmentation annotations, 07++12 : union of VOC07\ntrainval, VOC07 test, and VOC12 trainval.\nmethod train set aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP\nBabyLearning Prop. 78.0 74.2 61.3 45.7 42.7 68.2 66.8 80.2 40.6 70.0 49.8 79.0 74.5 77.9 64.0 35.3 67.9 55.7 68.7 62.6 63.2\nNUS NIN c2000 Unk. 80.2 73.8 61.9 43.7 43.0 70.3 67.6 80.7 41.9 69.7 51.7 78.2 75.2 76.9 65.1 38.6 68.3 58.0 68.7 63.3 63.8\nR-CNN BB [ 10]12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4\nFRCN [ours] 12 80.3 74.7 66.9 46.9 37.7 73.9 68.6 87.7 41.7 71.1 51.1 86.0 77.8 79.8 69.8 32.1 65.5 63.8 76.4 61.7 65.7\nFRCN [ours] 07++12 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2 68.4\nTable 3. VOC 2012 test detection average precision (%). BabyLearning and NUS NIN c2000 use networks based on [ 17]. All other\nmethods use VGG16. Training set key: see Table 2,Unk. : unknown.\nto this CaffeNet as model S, for \u201csmall.\u201d The second net-\nwork is VGG CNN M1024 from [ 3], which has the same\ndepth as S, but is wider. We call this network model M,\nfor \u201cmedium.\u201d The \ufb01nal network is the very deep VGG16\nmodel from [ 20]. Since this model is the largest, we call\nit model L. In this section, all experiments use single-scale\ntraining and testing ( s= 600 ; see Section 5.2for details).\n4.2. VOC 2010 and 2012 results\nOn these datasets, we compare Fast R-CNN ( FRCN , for\nshort) against the top methods on the comp4 (outside data)\ntrack from the public leaderboard (Table 2, Table 3).3For\nthe NUS NIN c2000 and BabyLearning methods, there are\nno associated publications at this time and we could not\n\ufb01nd exact information on the ConvNet architectures used;\nthey are variants of the Network-in-Network design [ 17].\nAll other methods are initialized from the same pre-trained\nVGG16 network.\nFast R-CNN achieves the top result on VOC12 with a\nmAP of 65.7% (and 68.4% with extra data). It is also two\norders of magnitude faster than the other methods, which\nare all based on the \u201cslow\u201d R-CNN pipeline. On VOC10,\n3http://host.robots.ox.ac.uk:8080/leaderboard\n(accessed April 18, 2015)SegDeepM [ 25] achieves a higher mAP than Fast R-CNN\n(67.2% vs.",
        "eae273a8-806f-48df-9a2d-640426c698c3": "All other methods are initialized from the same pre-trained\nVGG16 network.\nFast R-CNN achieves the top result on VOC12 with a\nmAP of 65.7% (and 68.4% with extra data). It is also two\norders of magnitude faster than the other methods, which\nare all based on the \u201cslow\u201d R-CNN pipeline. On VOC10,\n3http://host.robots.ox.ac.uk:8080/leaderboard\n(accessed April 18, 2015)SegDeepM [ 25] achieves a higher mAP than Fast R-CNN\n(67.2% vs. 66.1%). SegDeepM is trained on VOC12 train-\nval plus segmentation annotations; it is designed to boost\nR-CNN accuracy by using a Markov random \ufb01eld to reason\nover R-CNN detections and segmentations from the O 2P\n[1] semantic-segmentation method. Fast R-CNN can be\nswapped into SegDeepM in place of R-CNN, which may\nlead to better results. When using the enlarged 07++12\ntraining set (see Table 2caption), Fast R-CNN\u2019s mAP in-\ncreases to 68.8%, surpassing SegDeepM.\n4.3. VOC 2007 results\nOn VOC07, we compare Fast R-CNN to R-CNN and\nSPPnet. All methods start from the same pre-trained\nVGG16 network and use bounding-box regression. The\nVGG16 SPPnet results were computed by the authors of\n[11]. SPPnet uses \ufb01ve scales during both training and test-\ning. The improvement of Fast R-CNN over SPPnet illus-\ntrates that even though Fast R-CNN uses single-scale train-\ning and testing, \ufb01ne-tuning the conv layers provides a large\nimprovement in mAP (from 63.1% to 66.9%). R-CNN\nachieves a mAP of 66.0%. As a minor point, SPPnet was\ntrained without examples marked as \u201cdif\ufb01cult\u201d in PASCAL.\nRemoving these examples improves Fast R-CNN mAP to\n68.1%. All other experiments use \u201cdif\ufb01cult\u201d examples.\n1444",
        "eed98ce0-39f5-468f-a6bc-35998f04278d": "4.4. Training and testing time\nFast training and testing times are our second main re-\nsult. Table 4compares training time (hours), testing rate\n(seconds per image), and mAP on VOC07 between Fast R-\nCNN, R-CNN, and SPPnet. For VGG16, Fast R-CNN pro-\ncesses images 146 \u00d7faster than R-CNN without truncated\nSVD and 213 \u00d7faster with it. Training time is reduced by\n9\u00d7, from 84 hours to 9.5. Compared to SPPnet, Fast R-\nCNN trains VGG16 2.7 \u00d7faster (in 9.5 vs. 25.5 hours) and\ntests 7\u00d7faster without truncated SVD or 10 \u00d7faster with it.\nFast R-CNN also eliminates hundreds of gigabytes of disk\nstorage, because it does not cache features.\nFast R-CNN R-CNN SPPnet\nS M L S M L\u2020L\ntrain time (h) 1.2 2.0 9.5 22 28 84 25\ntrain speedup 18.3\u00d714.0\u00d78.8\u00d7 1\u00d7 1\u00d7 1\u00d7 3.4\u00d7\ntest rate (s/im) 0.10 0.15 0.32 9.8 12.1 47.0 2.3\n\u22b2with SVD 0.06 0.08 0.22 - - - -\ntest speedup 98\u00d7 80\u00d7146\u00d7 1\u00d7 1\u00d7 1\u00d7 20\u00d7\n\u22b2with SVD 169\u00d7150\u00d7213\u00d7 - - - -\nVOC07 mAP 57.1 59.2 66.9 58.5 60.2 66.0 63.1\n\u22b2with SVD 56.5 58.7 66.6 - - - -\nTable 4. Runtime comparison between the same models in Fast R-\nCNN, R-CNN, and SPPnet. Fast R-CNN uses single-scale mode.\nSPPnet uses the \ufb01ve scales speci\ufb01ed in [ 11].\u2020Timing provided by\nthe authors of [ 11]. Times were measured on an Nvidia K40 GPU.\nTruncated SVD. Truncated SVD can reduce detection\ntime by more than 30% with only a small (0.3 percent-\nage point) drop in mAP and without needing to perform\nadditional \ufb01ne-tuning after model compression. Fig. 2il-\nlustrates how using the top 1024 singular values from the\n25088\u00d74096 matrix in VGG16\u2019s fc6 layer and the top 256\nsingular values from the 4096\u00d74096 fc7 layer reduces run-\ntime with little loss in mAP. Further speed-ups are possi-\nble with smaller drops in mAP if one \ufb01ne-tunes again after\ncompression.\nroi_pool55.4% (17ms)other\n3.5% (11ms)fc6\n38.7% (122ms)\nconv46.3% (146ms)fc76.2% (20ms)Forward pass timing\nmAP 66.9% @ 320ms / image\nroi_pool5\n7.9% (17ms)other\n5.1% (11ms)fc6\n17.5% (37ms)\nconv67.8% (143ms)fc7 1.7% (4ms)Forward pass timing (SVD)\nmAP 66.6% @ 223ms / image\nFigure 2. Timing for VGG16 before and after truncated SVD. Be-\nfore SVD, fully connected layers fc6 and fc7 take 45% of the time.4.5. Which layers to \ufb01ne\u00adtune?\nFor the less deep networks considered in the SPPnet pa-\nper [ 11], \ufb01ne-tuning only the fully connected layers ap-\npeared to be suf\ufb01cient for good accuracy. We hypothesized\nthat this result would not hold for very deep networks. To\nvalidate that \ufb01ne-tuning the conv layers is important for\nVGG16, we use Fast R-CNN to \ufb01ne-tune, but freeze the\nthirteen conv layers so that only the fully connected layers\nlearn.",
        "a7c9c245-3c5e-45f0-aebb-e54231f1a1b0": "Timing for VGG16 before and after truncated SVD. Be-\nfore SVD, fully connected layers fc6 and fc7 take 45% of the time.4.5. Which layers to \ufb01ne\u00adtune?\nFor the less deep networks considered in the SPPnet pa-\nper [ 11], \ufb01ne-tuning only the fully connected layers ap-\npeared to be suf\ufb01cient for good accuracy. We hypothesized\nthat this result would not hold for very deep networks. To\nvalidate that \ufb01ne-tuning the conv layers is important for\nVGG16, we use Fast R-CNN to \ufb01ne-tune, but freeze the\nthirteen conv layers so that only the fully connected layers\nlearn. This ablation emulates single-scale SPPnet training\nanddecreases mAP from 66.9% to 61.4% (Table 5). This\nexperiment veri\ufb01es our hypothesis: training through the RoI\npooling layer is important for very deep nets.\nlayers that are \ufb01ne-tuned in model LSPPnet L\n\u2265fc6\u2265conv3 1\u2265conv2 1\u2265fc6\nVOC07 mAP 61.4 66.9 67.2 63.1\ntest rate (s/im) 0.32 0.32 0.32 2.3\nTable 5. Effect of restricting which layers are \ufb01ne-tuned for\nVGG16. Fine-tuning \u2265fc6 emulates the SPPnet training algo-\nrithm [ 11], but using a single scale. SPPnet Lresults were ob-\ntained using \ufb01ve scales, at a signi\ufb01cant (7 \u00d7) speed cost.\nDoes this mean that allconv layers should be \ufb01ne-tuned?\nIn short, no. In the smaller networks ( SandM) we \ufb01nd\nthat conv1 is generic and task independent (a well-known\nfact [ 14]). Allowing conv1 to learn, or not, has no mean-\ningful effect on mAP. For VGG16, we found it only nec-\nessary to update layers from conv3 1 and up (9 of the 13\nconv layers). This observation is pragmatic: (1) updating\nfrom conv2 1 slows training by 1.3 \u00d7(12.5 vs. 9.5 hours)\ncompared to learning from conv3 1; and (2) updating from\nconv1 1 over-runs GPU memory. The difference in mAP\nwhen learning from conv2 1 up was only +0.3points (Ta-\nble5, last column). All Fast R-CNN results in this paper\nusing VGG16 \ufb01ne-tune layers conv3 1 and up; all experi-\nments with models SandM\ufb01ne-tune layers conv2 and up.\n5. Design evaluation\nWe conducted experiments to understand how Fast R-\nCNN compares to R-CNN and SPPnet, as well as to eval-\nuate design decisions. Following best practices, we per-\nformed these experiments on the PASCAL VOC07 dataset.\n5.1. Does multi\u00adtask training help?\nMulti-task training is convenient because it avoids man-\naging a pipeline of sequentially-trained tasks. But it also has\nthe potential to improve results because the tasks in\ufb02uence\neach other through a shared representation (the ConvNet)\n[2]. Does multi-task training improve object detection ac-\ncuracy in Fast R-CNN?\nTo test this question, we train baseline networks that\nuse only the classi\ufb01cation loss, Lcls, in Eq. 1(i.e., setting\n1445",
        "c17d7649-d2c3-412a-96bb-1e8e982bf78a": "S M L\nmulti-task training? \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nstage-wise training? \u2713 \u2713 \u2713\ntest-time bbox reg? \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nVOC07 mAP 52.2 53.3 54.6 57.1 54.7 55.5 56.6 59.2 62.6 63.4 64.0 66.9\nTable 6. Multi-task training (forth column per group) improves mAP over piecewise training (third column per group).\n\u03bb= 0). These baselines are printed for models S,M, and L\nin the \ufb01rst column of each group in Table 6. Note that these\nmodels do not have bounding-box regressors. Next (second\ncolumn per group), we take networks that were trained with\nthe multi-task loss (Eq. 1,\u03bb= 1), but we disable bounding-\nbox regression at test time. This isolates the networks\u2019 clas-\nsi\ufb01cation accuracy and allows an apples-to-apples compar-\nison with the baseline networks.\nAcross all three networks we observe that multi-task\ntraining improves pure classi\ufb01cation accuracy relative to\ntraining for classi\ufb01cation alone. The improvement ranges\nfrom+0.8to+1.1mAP points, showing a consistent posi-\ntive effect from multi-task learning.\nFinally, we take the baseline models (trained with only\nthe classi\ufb01cation loss), tack on the bounding-box regression\nlayer, and train them with Llocwhile keeping all other net-\nwork parameters frozen. The third column in each group\nshows the results of this stage-wise training scheme: mAP\nimproves over column one, but stage-wise training under-\nperforms multi-task training (forth column per group).\n5.2. Scale invariance: to brute force or \ufb01nesse?\nWe compare two strategies for achieving scale-invariant\nobject detection: brute-force learning (single scale) and im-\nage pyramids (multi-scale). In either case, we de\ufb01ne the\nscalesof an image to be the length of its shortest side.\nAll single-scale experiments use s= 600 pixels;smay\nbe less than 600for some images as we cap the longest im-\nage side at 1000 pixels and maintain the image\u2019s aspect ra-\ntio. These values were selected so that VGG16 \ufb01ts in GPU\nmemory during \ufb01ne-tuning. The smaller models are not\nmemory bound and can bene\ufb01t from larger values of s; how-\never, optimizing sfor each model is not our main concern.\nWe note that PASCAL images are 384\u00d7473pixels on av-\nerage and thus the single-scale setting typically upsamples\nimages by a factor of 1.6. The average effective stride at the\nRoI pooling layer is thus \u224810pixels.\nIn the multi-scale setting, we use the same \ufb01ve scales\nspeci\ufb01ed in [ 11] (s\u2208 {480,576,688,864,1200}) to facili-\ntate comparison with SPPnet. However, we cap the longest\nside at2000 pixels to avoid exceeding GPU memory.\nTable 7shows models SandMwhen trained and tested\nwith either one or \ufb01ve scales. Perhaps the most surpris-\ning result in [ 11] was that single-scale detection performs\nalmost as well as multi-scale detection. Our \ufb01ndings con-SPPnet ZF S M L\nscales 1 5 1 5 1 5 1\ntest rate (s/im) 0.14 0.38 0.10 0.39 0.15 0.64 0.32\nVOC07 mAP 58.0 59.2 57.1 58.4 59.2 60.7 66.9\nTable 7. Multi-scale vs. single scale. SPPnet ZF(similar to model\nS) results are from [ 11]. Larger networks with a single-scale offer\nthe best speed / accuracy tradeoff. ( Lcannot use multi-scale in our\nimplementation due to GPU memory constraints.)\n\ufb01rm their result: deep ConvNets are adept at directly learn-\ning scale invariance. The multi-scale approach offers only\na small increase in mAP at a large cost in compute time\n(Table 7).",
        "9f059ff5-2eb3-44ac-a4a0-fce64579f2e0": "Multi-scale vs. single scale. SPPnet ZF(similar to model\nS) results are from [ 11]. Larger networks with a single-scale offer\nthe best speed / accuracy tradeoff. ( Lcannot use multi-scale in our\nimplementation due to GPU memory constraints.)\n\ufb01rm their result: deep ConvNets are adept at directly learn-\ning scale invariance. The multi-scale approach offers only\na small increase in mAP at a large cost in compute time\n(Table 7). In the case of VGG16 (model L), we are lim-\nited to using a single scale by implementation details. Yet it\nachieves a mAP of 66.9%, which is slightly higher than the\n66.0% reported for R-CNN [ 10], even though R-CNN uses\n\u201cin\ufb01nite\u201d scales in the sense that each proposal is warped to\na canonical size.\nSince single-scale processing offers the best tradeoff be-\ntween speed and accuracy, especially for very deep models,\nall experiments outside of this sub-section use single-scale\ntraining and testing with s= 600 pixels.\n5.3. Do we need more training data?\nA good object detector should improve when supplied\nwith more training data. Zhu et al. [24] found that DPM [ 8]\nmAP saturates after only a few hundred to thousand train-\ning examples. Here we augment the VOC07 trainval set\nwith the VOC12 trainval set, roughly tripling the number\nof images to 16.5k, to evaluate Fast R-CNN. Enlarging the\ntraining set improves mAP on VOC07 test from 66.9% to\n70.0% (Table 1). When training on this dataset we use 60k\nmini-batch iterations instead of 40k.\nWe perform similar experiments for VOC10 and 2012,\nfor which we construct a dataset of 21.5k images from the\nunion of VOC07 trainval, test, and VOC12 trainval. When\ntraining on this dataset, we use 100k SGD iterations and\nlower the learning rate by 0.1\u00d7each 40k iterations (instead\nof each 30k). For VOC10 and 2012, mAP improves from\n66.1% to 68.8% and from 65.7% to 68.4%, respectively.\n5.4. Do SVMs outperform softmax?\nFast R-CNN uses the softmax classi\ufb01er learnt during\n\ufb01ne-tuning instead of training one-vs-rest linear SVMs\n1446",
        "6babadfb-176b-494f-afc1-5d097325dc55": "post-hoc, as was done in R-CNN and SPPnet. To under-\nstand the impact of this choice, we implemented post-hoc\nSVM training with hard negative mining in Fast R-CNN.\nWe use the same training algorithm and hyper-parameters\nas in R-CNN.\nmethod classi\ufb01er S M L\nR-CNN [ 9,10]SVM 58.5 60.2 66.0\nFRCN [ours] SVM 56.3 58.7 66.8\nFRCN [ours] softmax 57.1 59.2 66.9\nTable 8. Fast R-CNN with softmax vs. SVM (VOC07 mAP).\nTable 8shows softmax slightly outperforming SVM for\nall three networks, by +0.1to+0.8mAP points. This ef-\nfect is small, but it demonstrates that \u201cone-shot\u201d \ufb01ne-tuning\nis suf\ufb01cient compared to previous multi-stage training ap-\nproaches. We note that softmax, unlike one-vs-rest SVMs,\nintroduces competition between classes when scoring a RoI.\n5.5. Are more proposals always better?\nThere are (broadly) two types of object detectors: those\nthat use a sparse set of object proposals ( e.g., selective\nsearch [ 21]) and those that use a dense set (e.g., DPM [ 8]).\nClassifying sparse proposals is a type of cascade [22] in\nwhich the proposal mechanism \ufb01rst rejects a vast number of\ncandidates leaving the classi\ufb01er with a small set to evaluate.\nThis cascade improves detection accuracy when applied to\nDPM detections [ 21]. We \ufb01nd evidence that the proposal-\nclassi\ufb01er cascade also improves Fast R-CNN accuracy.\nUsing selective search\u2019s quality mode , we sweep from 1k\nto 10k proposals per image, each time re-training andre-\ntesting model M. If proposals serve a purely computational\nrole, increasing the number of proposals per image should\nnot harm mAP.\n103\n104\nNumber of object proposals\n49\n51\n53\n56\n58\n61\n63\n66mAP\nSel. Search (SS)\nSS (2k) + Rand Dense\nSS replace Dense\n45k Dense Softmax\n45k Dense SVM\n49\n51\n53\n56\n58\n61\n63\n66\nAverage Recall\nSS Avg. Recall\nFigure 3. VOC07 test mAP and AR for various proposal schemes.\nWe \ufb01nd that mAP rises and then falls slightly as the pro-\nposal count increases (Fig. 3, solid blue line). This exper-\niment shows that swamping the deep classi\ufb01er with more\nproposals does not help, and even slightly hurts, accuracy.This result is dif\ufb01cult to predict without actually running\nthe experiment. The state-of-the-art for measuring object\nproposal quality is Average Recall (AR) [ 12]. AR correlates\nwell with mAP for several proposal methods using R-CNN,\nwhen using a \ufb01xed number of proposals per image . Fig. 3\nshows that AR (solid red line) does not correlate well with\nmAP as the number of proposals per image is varied. AR\nmust be used with care; higher AR due to more proposals\ndoes not imply that mAP will increase. Fortunately, training\nand testing with model Mtakes less than 2.5 hours. Fast\nR-CNN thus enables ef\ufb01cient, direct evaluation of object\nproposal mAP, which is preferable to proxy metrics.\nWe also investigate Fast R-CNN when using densely\ngenerated boxes (over scale, position, and aspect ratio), at\na rate of about 45k boxes / image. This dense set is rich\nenough that when each selective search box is replaced by\nits closest (in IoU) dense box, mAP drops only 1 point (to\n57.7%, Fig. 3, blue triangle).\nThe statistics of the dense boxes differ from those of\nselective search boxes. Starting with 2k selective search\nboxes, we test mAP when adding a random sample of\n1000\u00d7{2,4,6,8,10,32,45}dense boxes. For each exper-\niment we re-train and re-test model M. When these dense\nboxes are added, mAP falls more strongly than when adding\nmore selective search boxes, eventually reaching 53.0%.",
        "31805a36-aabc-4885-9aac-ad6b1a460135": "We also investigate Fast R-CNN when using densely\ngenerated boxes (over scale, position, and aspect ratio), at\na rate of about 45k boxes / image. This dense set is rich\nenough that when each selective search box is replaced by\nits closest (in IoU) dense box, mAP drops only 1 point (to\n57.7%, Fig. 3, blue triangle).\nThe statistics of the dense boxes differ from those of\nselective search boxes. Starting with 2k selective search\nboxes, we test mAP when adding a random sample of\n1000\u00d7{2,4,6,8,10,32,45}dense boxes. For each exper-\niment we re-train and re-test model M. When these dense\nboxes are added, mAP falls more strongly than when adding\nmore selective search boxes, eventually reaching 53.0%.\nWe also train and test Fast R-CNN using only dense\nboxes (45k / image). This setting yields a mAP of 52.9%\n(blue diamond). Finally, we check if SVMs with hard nega-\ntive mining are needed to cope with the dense box distribu-\ntion. SVMs do even worse: 49.3% (blue circle).\n5.6. Preliminary MS COCO results\nWe applied Fast R-CNN (with VGG16) to the MS\nCOCO dataset [ 18] to establish a preliminary baseline. We\ntrained on the 80k image training set for 240k iterations and\nevaluated on the \u201ctest-dev\u201d set using the evaluation server.\nThe PASCAL-style mAP is 35.9%; the new COCO-style\nAP, which also averages over IoU thresholds, is 19.7%.\n6. Conclusion\nThis paper proposes Fast R-CNN, a clean and fast update\nto R-CNN and SPPnet. In addition to reporting state-of-the-\nart detection results, we present detailed experiments that\nwe hope provide new insights. Of particular note, sparse\nobject proposals appear to improve detector quality. This\nissue was too costly (in time) to probe in the past, but be-\ncomes practical with Fast R-CNN. Of course, there may ex-\nist yet undiscovered techniques that allow dense boxes to\nperform as well as sparse proposals. Such methods, if de-\nveloped, may help further accelerate object detection.\nAcknowledgements. I thank Kaiming He, Larry Zitnick,\nand Piotr Doll \u00b4ar for helpful discussions and encouragement.\n1447",
        "6a9f2c26-4833-4c6f-abb8-c51b5f569137": "References\n[1] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\nmantic segmentation with second-order pooling. In ECCV ,\n2012. 5\n[2] R. Caruana. Multitask learning. Machine learning , 28(1),\n1997. 6\n[3] K. Chat\ufb01eld, K. Simonyan, A. Vedaldi, and A. Zisserman.\nReturn of the devil in the details: Delving deep into convo-\nlutional nets. In BMVC , 2014. 5\n[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. ImageNet: A large-scale hierarchical image database.\nInCVPR , 2009. 2\n[5] E. Denton, W. Zaremba, J. Bruna, Y . LeCun, and R. Fergus.\nExploiting linear structure within convolutional networks for\nef\ufb01cient evaluation. In NIPS , 2014. 4\n[6] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\nobject detection using deep neural networks. In CVPR , 2014.\n3\n[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\nA. Zisserman. The PASCAL Visual Object Classes (VOC)\nChallenge. IJCV , 2010. 1\n[8] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-\nmanan. Object detection with discriminatively trained part\nbased models. TPAMI , 2010. 3,7,8\n[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In CVPR , 2014. 1,3,4,8\n[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Region-\nbased convolutional networks for accurate object detection\nand segmentation. TPAMI , 2015. 5,7,8\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\nin deep convolutional networks for visual recognition. In\nECCV , 2014. 1,2,3,4,5,6,7\n[12] J. H. Hosang, R. Benenson, P. Doll \u00b4ar, and B. Schiele. What\nmakes for effective detection proposals? arXiv preprint\narXiv:1502.05082 , 2015. 8\n[13] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\narchitecture for fast feature embedding. In Proc. of the ACM\nInternational Conf. on Multimedia , 2014. 2\n[14] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-\nsi\ufb01cation with deep convolutional neural networks. In NIPS ,\n2012. 1,4,6\n[15] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of\nfeatures: Spatial pyramid matching for recognizing natural\nscene categories. In CVPR , 2006. 1\n[16] Y . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard,\nW. Hubbard, and L. Jackel. Backpropagation applied to\nhandwritten zip code recognition. Neural Comp. , 1989. 1\n[17] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR ,\n2014.",
        "baef78d2-6922-4d4c-818b-1c8f8335a857": "2\n[14] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-\nsi\ufb01cation with deep convolutional neural networks. In NIPS ,\n2012. 1,4,6\n[15] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of\nfeatures: Spatial pyramid matching for recognizing natural\nscene categories. In CVPR , 2006. 1\n[16] Y . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard,\nW. Hubbard, and L. Jackel. Backpropagation applied to\nhandwritten zip code recognition. Neural Comp. , 1989. 1\n[17] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR ,\n2014. 5\n[18] T. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick,\nJ. Hays, P. Perona, D. Ramanan, P. Doll \u00b4ar, and C. L. Zit-\nnick. Microsoft COCO: common objects in context. arXiv\ne-prints , arXiv:1405.0312 [cs.CV], 2014. 8[19] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y . LeCun. OverFeat: Integrated Recognition, Localiza-\ntion and Detection using Convolutional Networks. In ICLR ,\n2014. 1,3\n[20] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. In ICLR , 2015.\n1,5\n[21] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.\nSelective search for object recognition. IJCV , 2013. 8\n[22] P. Viola and M. Jones. Rapid object detection using a boosted\ncascade of simple features. In CVPR , 2001. 8\n[23] J. Xue, J. Li, and Y . Gong. Restructuring of deep neural\nnetwork acoustic models with singular value decomposition.\nInInterspeech , 2013. 4\n[24] X. Zhu, C. V ondrick, D. Ramanan, and C. Fowlkes. Do we\nneed more training data or better models for object detec-\ntion? In BMVC , 2012. 7\n[25] Y . Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler.\nsegDeepM: Exploiting segmentation and context in deep\nneural networks for object detection. In CVPR , 2015. 1,\n5\n1448",
        "07c07394-394b-4712-ba71-e7bb6693dbab": "You Only Look Once:\nUni\ufb01ed, Real-Time Object Detection\nJoseph Redmon\u2217, Santosh Divvala\u2217\u2020, Ross Girshick\u00b6, Ali Farhadi\u2217\u2020\nUniversity of Washington\u2217, Allen Institute for AI\u2020, Facebook AI Research\u00b6\nhttp://pjreddie.com/yolo/\nAbstract\nWe present YOLO, a new approach to object detection.\nPrior work on object detection repurposes classi\ufb01ers to per-\nform detection. Instead, we frame object detection as a re-\ngression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network pre-\ndicts bounding boxes and class probabilities directly from\nfull images in one evaluation. Since the whole detection\npipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.\nOur uni\ufb01ed architecture is extremely fast. Our base\nYOLO model processes images in real-time at 45 frames\nper second. A smaller version of the network, Fast YOLO,\nprocesses an astounding 155 frames per second while\nstill achieving double the mAP of other real-time detec-\ntors. Compared to state-of-the-art detection systems, YOLO\nmakes more localization errors but is less likely to predict\nfalse positives on background. Finally, YOLO learns very\ngeneral representations of objects. It outperforms other de-\ntection methods, including DPM and R-CNN, when gener-\nalizing from natural images to other domains like artwork.\n1. Introduction\nHumans glance at an image and instantly know what ob-\njects are in the image, where they are, and how they inter-\nact. The human visual system is fast and accurate, allow-\ning us to perform complex tasks like driving with little con-\nscious thought. Fast, accurate algorithms for object detec-\ntion would allow computers to drive cars without special-\nized sensors, enable assistive devices to convey real-time\nscene information to human users, and unlock the potential\nfor general purpose, responsive robotic systems.\nCurrent detection systems repurpose classi\ufb01ers to per-\nform detection. To detect an object, these systems take a\nclassi\ufb01er for that object and evaluate it at various locations\nand scales in a test image. Systems like deformable parts\nmodels (DPM) use a sliding window approach where the\nclassi\ufb01er is run at evenly spaced locations over the entire\nimage [10].\nMore recent approaches like R-CNN use region proposal\n1. Resize image.\n2. Run convolutional network.3. Non-max suppression.\nDog: 0.30Person: 0.64Horse: 0.28Figure 1: The YOLO Detection System. Processing images\nwith YOLO is simple and straightforward. Our system (1) resizes\nthe input image to 448\u00d7448, (2) runs a single convolutional net-\nwork on the image, and (3) thresholds the resulting detections by\nthe model\u2019s con\ufb01dence.\nmethods to \ufb01rst generate potential bounding boxes in an im-\nage and then run a classi\ufb01er on these proposed boxes. After\nclassi\ufb01cation, post-processing is used to re\ufb01ne the bound-\ning boxes, eliminate duplicate detections, and rescore the\nboxes based on other objects in the scene [13]. These com-\nplex pipelines are slow and hard to optimize because each\nindividual component must be trained separately.\nWe reframe object detection as a single regression prob-\nlem, straight from image pixels to bounding box coordi-\nnates and class probabilities. Using our system, you only\nlook once (YOLO) at an image to predict what objects are\npresent and where they are.\nYOLO is refreshingly simple: see Figure 1. A sin-\ngle convolutional network simultaneously predicts multi-\nple bounding boxes and class probabilities for those boxes.\nYOLO trains on full images and directly optimizes detec-\ntion performance. This uni\ufb01ed model has several bene\ufb01ts\nover traditional methods of object detection.\nFirst, YOLO is extremely fast. Since we frame detection\nas a regression problem we don\u2019t need a complex pipeline.\nWe simply run our neural network on a new image at test\ntime to predict detections. Our base network runs at 45\nframes per second with no batch processing on a Titan X\nGPU and a fast version runs at more than 150 fps. This\nmeans we can process streaming video in real-time with\nless than 25 milliseconds of latency.",
        "68c134dc-a062-48b4-9fc6-2c4219499010": "YOLO is refreshingly simple: see Figure 1. A sin-\ngle convolutional network simultaneously predicts multi-\nple bounding boxes and class probabilities for those boxes.\nYOLO trains on full images and directly optimizes detec-\ntion performance. This uni\ufb01ed model has several bene\ufb01ts\nover traditional methods of object detection.\nFirst, YOLO is extremely fast. Since we frame detection\nas a regression problem we don\u2019t need a complex pipeline.\nWe simply run our neural network on a new image at test\ntime to predict detections. Our base network runs at 45\nframes per second with no batch processing on a Titan X\nGPU and a fast version runs at more than 150 fps. This\nmeans we can process streaming video in real-time with\nless than 25 milliseconds of latency. Furthermore, YOLO\nachieves more than twice the mean average precision of\nother real-time systems. For a demo of our system running\nin real-time on a webcam please see our project webpage:\nhttp://pjreddie.com/yolo/ .\nSecond, YOLO reasons globally about the image when\n1arXiv:1506.02640v5  [cs.CV]  9 May 2016",
        "22720652-1ff3-48b9-876d-17e10a79b424": "making predictions. Unlike sliding window and region\nproposal-based techniques, YOLO sees the entire image\nduring training and test time so it implicitly encodes contex-\ntual information about classes as well as their appearance.\nFast R-CNN, a top detection method [14], mistakes back-\nground patches in an image for objects because it can\u2019t see\nthe larger context. YOLO makes less than half the number\nof background errors compared to Fast R-CNN.\nThird, YOLO learns generalizable representations of ob-\njects. When trained on natural images and tested on art-\nwork, YOLO outperforms top detection methods like DPM\nand R-CNN by a wide margin. Since YOLO is highly gen-\neralizable it is less likely to break down when applied to\nnew domains or unexpected inputs.\nYOLO still lags behind state-of-the-art detection systems\nin accuracy. While it can quickly identify objects in im-\nages it struggles to precisely localize some objects, espe-\ncially small ones. We examine these tradeoffs further in our\nexperiments.\nAll of our training and testing code is open source. A\nvariety of pretrained models are also available to download.\n2. Uni\ufb01ed Detection\nWe unify the separate components of object detection\ninto a single neural network. Our network uses features\nfrom the entire image to predict each bounding box. It also\npredicts all bounding boxes across all classes for an im-\nage simultaneously. This means our network reasons glob-\nally about the full image and all the objects in the image.\nThe YOLO design enables end-to-end training and real-\ntime speeds while maintaining high average precision.\nOur system divides the input image into an S\u00d7Sgrid.\nIf the center of an object falls into a grid cell, that grid cell\nis responsible for detecting that object.\nEach grid cell predicts Bbounding boxes and con\ufb01dence\nscores for those boxes. These con\ufb01dence scores re\ufb02ect how\ncon\ufb01dent the model is that the box contains an object and\nalso how accurate it thinks the box is that it predicts. For-\nmally we de\ufb01ne con\ufb01dence as Pr(Object )\u2217IOUtruth\npred. If no\nobject exists in that cell, the con\ufb01dence scores should be\nzero. Otherwise we want the con\ufb01dence score to equal the\nintersection over union (IOU) between the predicted box\nand the ground truth.\nEach bounding box consists of 5 predictions: x,y,w,h,\nand con\ufb01dence. The (x,y)coordinates represent the center\nof the box relative to the bounds of the grid cell. The width\nand height are predicted relative to the whole image. Finally\nthe con\ufb01dence prediction represents the IOU between the\npredicted box and any ground truth box.\nEach grid cell also predicts Cconditional class proba-\nbilities, Pr(Classi|Object ). These probabilities are condi-\ntioned on the grid cell containing an object. We only predictone set of class probabilities per grid cell, regardless of the\nnumber of boxes B.\nAt test time we multiply the conditional class probabili-\nties and the individual box con\ufb01dence predictions,\nPr(Classi|Object )\u2217Pr(Object )\u2217IOUtruth\npred= Pr( Classi)\u2217IOUtruth\npred(1)\nwhich gives us class-speci\ufb01c con\ufb01dence scores for each\nbox. These scores encode both the probability of that class\nappearing in the box and how well the predicted box \ufb01ts the\nobject.\nS \u00d7 S grid on inputBounding boxes + confidence\nClass probability mapFinal detections\nFigure 2: The Model. Our system models detection as a regres-\nsion problem. It divides the image into an S\u00d7Sgrid and for each\ngrid cell predicts Bbounding boxes, con\ufb01dence for those boxes,\nandCclass probabilities. These predictions are encoded as an\nS\u00d7S\u00d7(B\u22175 +C)tensor.\nFor evaluating YOLO on P ASCAL VOC, we use S= 7,\nB= 2. PASCAL VOC has 20 labelled classes so C= 20 .\nOur \ufb01nal prediction is a 7\u00d77\u00d730tensor.\n2.1. Network Design\nWe implement this model as a convolutional neural net-\nwork and evaluate it on the P ASCAL VOC detection dataset\n[9].",
        "ba245602-906f-4066-894d-2ed1734fab1e": "S \u00d7 S grid on inputBounding boxes + confidence\nClass probability mapFinal detections\nFigure 2: The Model. Our system models detection as a regres-\nsion problem. It divides the image into an S\u00d7Sgrid and for each\ngrid cell predicts Bbounding boxes, con\ufb01dence for those boxes,\nandCclass probabilities. These predictions are encoded as an\nS\u00d7S\u00d7(B\u22175 +C)tensor.\nFor evaluating YOLO on P ASCAL VOC, we use S= 7,\nB= 2. PASCAL VOC has 20 labelled classes so C= 20 .\nOur \ufb01nal prediction is a 7\u00d77\u00d730tensor.\n2.1. Network Design\nWe implement this model as a convolutional neural net-\nwork and evaluate it on the P ASCAL VOC detection dataset\n[9]. The initial convolutional layers of the network extract\nfeatures from the image while the fully connected layers\npredict the output probabilities and coordinates.\nOur network architecture is inspired by the GoogLeNet\nmodel for image classi\ufb01cation [34]. Our network has 24\nconvolutional layers followed by 2 fully connected layers.\nInstead of the inception modules used by GoogLeNet, we\nsimply use 1\u00d71reduction layers followed by 3\u00d73convo-\nlutional layers, similar to Lin et al [22]. The full network is\nshown in Figure 3.\nWe also train a fast version of YOLO designed to push\nthe boundaries of fast object detection. Fast YOLO uses a\nneural network with fewer convolutional layers (9 instead\nof 24) and fewer \ufb01lters in those layers. Other than the size\nof the network, all training and testing parameters are the\nsame between YOLO and Fast YOLO.",
        "1194c0a3-4a7f-46b2-9836-122e2bcc3e7c": "448\n448\n3\n7\n7\nConv. Layer\n7x7x64-s-2\nMaxpool Layer\n2x2-s-2\n3\n3112\n112\n192\n3\n356\n56\n256\nConn. Layer4096\nConn. Layer Conv. Layer\n3x3x192\nMaxpool Layer\n2x2-s-2Conv. Layers\n1x1x128\n3x3x256\n1x1x256\n3x3x512\nMaxpool Layer\n2x2-s-2\n3\n328\n28\n512\nConv. Layers\n1x1x256\n3x3x5121x1x512\n3x3x1024\nMaxpool Layer\n2x2-s-2\n3\n314\n14\n1024\nConv. Layers\n1x1x512\n3x3x10243x3x1024\n3x3x1024-s-2\n3\n37\n7\n10247\n7\n10247\n7\n30\n} \u00d74 } \u00d72Conv. Layers\n3x3x1024\n3x3x1024Figure 3: The Architecture. Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating 1\u00d71\nconvolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classi\ufb01cation\ntask at half the resolution ( 224\u00d7224input image) and then double the resolution for detection.\nThe \ufb01nal output of our network is the 7\u00d77\u00d730tensor\nof predictions.\n2.2. Training\nWe pretrain our convolutional layers on the ImageNet\n1000-class competition dataset [30]. For pretraining we use\nthe \ufb01rst 20 convolutional layers from Figure 3 followed by a\naverage-pooling layer and a fully connected layer. We train\nthis network for approximately a week and achieve a single\ncrop top-5 accuracy of 88% on the ImageNet 2012 valida-\ntion set, comparable to the GoogLeNet models in Caffe\u2019s\nModel Zoo [24]. We use the Darknet framework for all\ntraining and inference [26].\nWe then convert the model to perform detection. Ren et\nal. show that adding both convolutional and connected lay-\ners to pretrained networks can improve performance [29].\nFollowing their example, we add four convolutional lay-\ners and two fully connected layers with randomly initialized\nweights. Detection often requires \ufb01ne-grained visual infor-\nmation so we increase the input resolution of the network\nfrom 224\u00d7224to448\u00d7448.\nOur \ufb01nal layer predicts both class probabilities and\nbounding box coordinates. We normalize the bounding box\nwidth and height by the image width and height so that they\nfall between 0 and 1. We parametrize the bounding box x\nandycoordinates to be offsets of a particular grid cell loca-\ntion so they are also bounded between 0 and 1.\nWe use a linear activation function for the \ufb01nal layer and\nall other layers use the following leaky recti\ufb01ed linear acti-\nvation:\n\u03c6(x) ={\nx, ifx>0\n0.1x,otherwise(2)\nWe optimize for sum-squared error in the output of ourmodel. We use sum-squared error because it is easy to op-\ntimize, however it does not perfectly align with our goal of\nmaximizing average precision. It weights localization er-\nror equally with classi\ufb01cation error which may not be ideal.\nAlso, in every image many grid cells do not contain any\nobject. This pushes the \u201ccon\ufb01dence\u201d scores of those cells\ntowards zero, often overpowering the gradient from cells\nthat do contain objects. This can lead to model instability,\ncausing training to diverge early on.\nTo remedy this, we increase the loss from bounding box\ncoordinate predictions and decrease the loss from con\ufb01-\ndence predictions for boxes that don\u2019t contain objects. We\nuse two parameters, \u03bbcoordand\u03bbnoobjto accomplish this. We\nset\u03bbcoord= 5and\u03bbnoobj=.5.\nSum-squared error also equally weights errors in large\nboxes and small boxes. Our error metric should re\ufb02ect that\nsmall deviations in large boxes matter less than in small\nboxes. To partially address this we predict the square root\nof the bounding box width and height instead of the width\nand height directly.",
        "1c68d4e4-fb97-415d-9898-2d4ac517ddc0": "Also, in every image many grid cells do not contain any\nobject. This pushes the \u201ccon\ufb01dence\u201d scores of those cells\ntowards zero, often overpowering the gradient from cells\nthat do contain objects. This can lead to model instability,\ncausing training to diverge early on.\nTo remedy this, we increase the loss from bounding box\ncoordinate predictions and decrease the loss from con\ufb01-\ndence predictions for boxes that don\u2019t contain objects. We\nuse two parameters, \u03bbcoordand\u03bbnoobjto accomplish this. We\nset\u03bbcoord= 5and\u03bbnoobj=.5.\nSum-squared error also equally weights errors in large\nboxes and small boxes. Our error metric should re\ufb02ect that\nsmall deviations in large boxes matter less than in small\nboxes. To partially address this we predict the square root\nof the bounding box width and height instead of the width\nand height directly.\nYOLO predicts multiple bounding boxes per grid cell.\nAt training time we only want one bounding box predictor\nto be responsible for each object. We assign one predictor\nto be \u201cresponsible\u201d for predicting an object based on which\nprediction has the highest current IOU with the ground\ntruth. This leads to specialization between the bounding box\npredictors. Each predictor gets better at predicting certain\nsizes, aspect ratios, or classes of object, improving overall\nrecall.\nDuring training we optimize the following, multi-part",
        "d28fad82-10e6-44dd-943d-7681f6baa4c0": "loss function:\n\u03bbcoordS2\u2211\ni=0B\u2211\nj=01obj\nij[\n(xi\u2212\u02c6xi)2+ (yi\u2212\u02c6yi)2]\n+\u03bbcoordS2\u2211\ni=0B\u2211\nj=01obj\nij[(\u221awi\u2212\u221a\n\u02c6wi)2+(\u221a\nhi\u2212\u221a\n\u02c6hi)2]\n+S2\u2211\ni=0B\u2211\nj=01obj\nij(\nCi\u2212\u02c6Ci)2\n+\u03bbnoobjS2\u2211\ni=0B\u2211\nj=01noobj\nij(\nCi\u2212\u02c6Ci)2\n+S2\u2211\ni=01obj\ni\u2211\nc\u2208classes(pi(c)\u2212\u02c6pi(c))2(3)\nwhere 1obj\nidenotes if object appears in cell iand 1obj\nijde-\nnotes that the jth bounding box predictor in cell iis \u201cre-\nsponsible\u201d for that prediction.\nNote that the loss function only penalizes classi\ufb01cation\nerror if an object is present in that grid cell (hence the con-\nditional class probability discussed earlier). It also only pe-\nnalizes bounding box coordinate error if that predictor is\n\u201cresponsible\u201d for the ground truth box (i.e. has the highest\nIOU of any predictor in that grid cell).\nWe train the network for about 135 epochs on the train-\ning and validation data sets from P ASCAL VOC 2007 and\n2012. When testing on 2012 we also include the VOC 2007\ntest data for training. Throughout training we use a batch\nsize of 64, a momentum of 0.9and a decay of 0.0005 .\nOur learning rate schedule is as follows: For the \ufb01rst\nepochs we slowly raise the learning rate from 10\u22123to10\u22122.\nIf we start at a high learning rate our model often diverges\ndue to unstable gradients. We continue training with 10\u22122\nfor 75 epochs, then 10\u22123for 30 epochs, and \ufb01nally 10\u22124\nfor 30 epochs.\nTo avoid over\ufb01tting we use dropout and extensive data\naugmentation. A dropout layer with rate = .5 after the \ufb01rst\nconnected layer prevents co-adaptation between layers [18].\nFor data augmentation we introduce random scaling and\ntranslations of up to 20% of the original image size. We\nalso randomly adjust the exposure and saturation of the im-\nage by up to a factor of 1.5in the HSV color space.\n2.3. Inference\nJust like in training, predicting detections for a test image\nonly requires one network evaluation. On P ASCAL VOC the\nnetwork predicts 98 bounding boxes per image and class\nprobabilities for each box. YOLO is extremely fast at test\ntime since it only requires a single network evaluation, un-\nlike classi\ufb01er-based methods.\nThe grid design enforces spatial diversity in the bound-\ning box predictions. Often it is clear which grid cell an\nobject falls in to and the network only predicts one box for\neach object. However, some large objects or objects nearthe border of multiple cells can be well localized by multi-\nple cells. Non-maximal suppression can be used to \ufb01x these\nmultiple detections. While not critical to performance as it\nis for R-CNN or DPM, non-maximal suppression adds 2-\n3% in mAP.\n2.4. Limitations of YOLO\nYOLO imposes strong spatial constraints on bounding\nbox predictions since each grid cell only predicts two boxes\nand can only have one class. This spatial constraint lim-\nits the number of nearby objects that our model can pre-\ndict. Our model struggles with small objects that appear in\ngroups, such as \ufb02ocks of birds.\nSince our model learns to predict bounding boxes from\ndata, it struggles to generalize to objects in new or unusual\naspect ratios or con\ufb01gurations. Our model also uses rela-\ntively coarse features for predicting bounding boxes since\nour architecture has multiple downsampling layers from the\ninput image.\nFinally, while we train on a loss function that approxi-\nmates detection performance, our loss function treats errors\nthe same in small bounding boxes versus large bounding\nboxes. A small error in a large box is generally benign but a\nsmall error in a small box has a much greater effect on IOU.\nOur main source of error is incorrect localizations.\n3.",
        "af2a3a00-f6ee-4f8e-8493-de49e367015e": "This spatial constraint lim-\nits the number of nearby objects that our model can pre-\ndict. Our model struggles with small objects that appear in\ngroups, such as \ufb02ocks of birds.\nSince our model learns to predict bounding boxes from\ndata, it struggles to generalize to objects in new or unusual\naspect ratios or con\ufb01gurations. Our model also uses rela-\ntively coarse features for predicting bounding boxes since\nour architecture has multiple downsampling layers from the\ninput image.\nFinally, while we train on a loss function that approxi-\nmates detection performance, our loss function treats errors\nthe same in small bounding boxes versus large bounding\nboxes. A small error in a large box is generally benign but a\nsmall error in a small box has a much greater effect on IOU.\nOur main source of error is incorrect localizations.\n3. Comparison to Other Detection Systems\nObject detection is a core problem in computer vision.\nDetection pipelines generally start by extracting a set of\nrobust features from input images (Haar [25], SIFT [23],\nHOG [4], convolutional features [6]). Then, classi\ufb01ers\n[36, 21, 13, 10] or localizers [1, 32] are used to identify\nobjects in the feature space. These classi\ufb01ers or localizers\nare run either in sliding window fashion over the whole im-\nage or on some subset of regions in the image [35, 15, 39].\nWe compare the YOLO detection system to several top de-\ntection frameworks, highlighting key similarities and differ-\nences.\nDeformable parts models. Deformable parts models\n(DPM) use a sliding window approach to object detection\n[10]. DPM uses a disjoint pipeline to extract static features,\nclassify regions, predict bounding boxes for high scoring\nregions, etc. Our system replaces all of these disparate parts\nwith a single convolutional neural network. The network\nperforms feature extraction, bounding box prediction, non-\nmaximal suppression, and contextual reasoning all concur-\nrently. Instead of static features, the network trains the fea-\ntures in-line and optimizes them for the detection task. Our\nuni\ufb01ed architecture leads to a faster, more accurate model\nthan DPM.\nR-CNN. R-CNN and its variants use region proposals in-\nstead of sliding windows to \ufb01nd objects in images. Selective",
        "e5bd64bd-e8e1-4cb2-9f54-798afede9a4a": "Search [35] generates potential bounding boxes, a convolu-\ntional network extracts features, an SVM scores the boxes, a\nlinear model adjusts the bounding boxes, and non-max sup-\npression eliminates duplicate detections. Each stage of this\ncomplex pipeline must be precisely tuned independently\nand the resulting system is very slow, taking more than 40\nseconds per image at test time [14].\nYOLO shares some similarities with R-CNN. Each grid\ncell proposes potential bounding boxes and scores those\nboxes using convolutional features. However, our system\nputs spatial constraints on the grid cell proposals which\nhelps mitigate multiple detections of the same object. Our\nsystem also proposes far fewer bounding boxes, only 98\nper image compared to about 2000 from Selective Search.\nFinally, our system combines these individual components\ninto a single, jointly optimized model.\nOther Fast Detectors Fast and Faster R-CNN focus on\nspeeding up the R-CNN framework by sharing computa-\ntion and using neural networks to propose regions instead\nof Selective Search [14] [28]. While they offer speed and\naccuracy improvements over R-CNN, both still fall short of\nreal-time performance.\nMany research efforts focus on speeding up the DPM\npipeline [31] [38] [5]. They speed up HOG computation,\nuse cascades, and push computation to GPUs. However,\nonly 30Hz DPM [31] actually runs in real-time.\nInstead of trying to optimize individual components of\na large detection pipeline, YOLO throws out the pipeline\nentirely and is fast by design.\nDetectors for single classes like faces or people can be\nhighly optimized since they have to deal with much less\nvariation [37]. YOLO is a general purpose detector that\nlearns to detect a variety of objects simultaneously.\nDeep MultiBox. Unlike R-CNN, Szegedy et al. train a\nconvolutional neural network to predict regions of interest\n[8] instead of using Selective Search. MultiBox can also\nperform single object detection by replacing the con\ufb01dence\nprediction with a single class prediction. However, Multi-\nBox cannot perform general object detection and is still just\na piece in a larger detection pipeline, requiring further im-\nage patch classi\ufb01cation. Both YOLO and MultiBox use a\nconvolutional network to predict bounding boxes in an im-\nage but YOLO is a complete detection system.\nOverFeat. Sermanet et al. train a convolutional neural\nnetwork to perform localization and adapt that localizer to\nperform detection [32]. OverFeat ef\ufb01ciently performs slid-\ning window detection but it is still a disjoint system. Over-\nFeat optimizes for localization, not detection performance.\nLike DPM, the localizer only sees local information when\nmaking a prediction. OverFeat cannot reason about global\ncontext and thus requires signi\ufb01cant post-processing to pro-\nduce coherent detections.\nMultiGrasp. Our work is similar in design to work ongrasp detection by Redmon et al [27]. Our grid approach to\nbounding box prediction is based on the MultiGrasp system\nfor regression to grasps. However, grasp detection is a much\nsimpler task than object detection. MultiGrasp only needs\nto predict a single graspable region for an image containing\none object. It doesn\u2019t have to estimate the size, location,\nor boundaries of the object or predict it\u2019s class, only \ufb01nd a\nregion suitable for grasping. YOLO predicts both bounding\nboxes and class probabilities for multiple objects of multi-\nple classes in an image.\n4. Experiments\nFirst we compare YOLO with other real-time detection\nsystems on P ASCAL VOC 2007. To understand the differ-\nences between YOLO and R-CNN variants we explore the\nerrors on VOC 2007 made by YOLO and Fast R-CNN, one\nof the highest performing versions of R-CNN [14]. Based\non the different error pro\ufb01les we show that YOLO can be\nused to rescore Fast R-CNN detections and reduce the er-\nrors from background false positives, giving a signi\ufb01cant\nperformance boost. We also present VOC 2012 results and\ncompare mAP to current state-of-the-art methods. Finally,\nwe show that YOLO generalizes to new domains better than\nother detectors on two artwork datasets.\n4.1. Comparison to Other Real-Time Systems\nMany research efforts in object detection focus on mak-\ning standard detection pipelines fast.",
        "346aa2e5-5fc2-4933-9ab1-0b2ecfed1e96": "To understand the differ-\nences between YOLO and R-CNN variants we explore the\nerrors on VOC 2007 made by YOLO and Fast R-CNN, one\nof the highest performing versions of R-CNN [14]. Based\non the different error pro\ufb01les we show that YOLO can be\nused to rescore Fast R-CNN detections and reduce the er-\nrors from background false positives, giving a signi\ufb01cant\nperformance boost. We also present VOC 2012 results and\ncompare mAP to current state-of-the-art methods. Finally,\nwe show that YOLO generalizes to new domains better than\nother detectors on two artwork datasets.\n4.1. Comparison to Other Real-Time Systems\nMany research efforts in object detection focus on mak-\ning standard detection pipelines fast. [5] [38] [31] [14] [17]\n[28] However, only Sadeghi et al. actually produce a de-\ntection system that runs in real-time (30 frames per second\nor better) [31]. We compare YOLO to their GPU imple-\nmentation of DPM which runs either at 30Hz or 100Hz.\nWhile the other efforts don\u2019t reach the real-time milestone\nwe also compare their relative mAP and speed to examine\nthe accuracy-performance tradeoffs available in object de-\ntection systems.\nFast YOLO is the fastest object detection method on\nPASCAL ; as far as we know, it is the fastest extant object\ndetector. With 52.7%mAP, it is more than twice as accurate\nas prior work on real-time detection. YOLO pushes mAP to\n63.4%while still maintaining real-time performance.\nWe also train YOLO using VGG-16. This model is more\naccurate but also signi\ufb01cantly slower than YOLO. It is use-\nful for comparison to other detection systems that rely on\nVGG-16 but since it is slower than real-time the rest of the\npaper focuses on our faster models.\nFastest DPM effectively speeds up DPM without sacri-\n\ufb01cing much mAP but it still misses real-time performance\nby a factor of 2 [38]. It also is limited by DPM\u2019s relatively\nlow accuracy on detection compared to neural network ap-\nproaches.\nR-CNN minus R replaces Selective Search with static\nbounding box proposals [20]. While it is much faster than",
        "5c927126-fec3-4773-b67c-a3d503c46bd7": "Real-Time Detectors Train mAP FPS\n100Hz DPM [31] 2007 16.0 100\n30Hz DPM [31] 2007 26.1 30\nFast YOLO 2007+2012 52.7 155\nYOLO 2007+2012 63.4 45\nLess Than Real-Time\nFastest DPM [38] 2007 30.4 15\nR-CNN Minus R [20] 2007 53.5 6\nFast R-CNN [14] 2007+2012 70.0 0.5\nFaster R-CNN VGG-16[28] 2007+2012 73.2 7\nFaster R-CNN ZF [28] 2007+2012 62.1 18\nYOLO VGG-16 2007+2012 66.4 21\nTable 1: Real-Time Systems on P ASCAL VOC 2007. Compar-\ning the performance and speed of fast detectors. Fast YOLO is\nthe fastest detector on record for P ASCAL VOC detection and is\nstill twice as accurate as any other real-time detector. YOLO is\n10 mAP more accurate than the fast version while still well above\nreal-time in speed.\nR-CNN, it still falls short of real-time and takes a signi\ufb01cant\naccuracy hit from not having good proposals.\nFast R-CNN speeds up the classi\ufb01cation stage of R-CNN\nbut it still relies on selective search which can take around\n2 seconds per image to generate bounding box proposals.\nThus it has high mAP but at 0.5fps it is still far from real-\ntime.\nThe recent Faster R-CNN replaces selective search with\na neural network to propose bounding boxes, similar to\nSzegedy et al. [8] In our tests, their most accurate model\nachieves 7 fps while a smaller, less accurate one runs at\n18 fps. The VGG-16 version of Faster R-CNN is 10 mAP\nhigher but is also 6 times slower than YOLO. The Zeiler-\nFergus Faster R-CNN is only 2.5 times slower than YOLO\nbut is also less accurate.\n4.2. VOC 2007 Error Analysis\nTo further examine the differences between YOLO and\nstate-of-the-art detectors, we look at a detailed breakdown\nof results on VOC 2007. We compare YOLO to Fast R-\nCNN since Fast R-CNN is one of the highest performing\ndetectors on P ASCAL and it\u2019s detections are publicly avail-\nable.\nWe use the methodology and tools of Hoiem et al. [19]\nFor each category at test time we look at the top N predic-\ntions for that category. Each prediction is either correct or\nit is classi\ufb01ed based on the type of error:\n\u2022Correct: correct class and IOU >.5\n\u2022Localization: correct class, .1<IOU<.5\n\u2022Similar: class is similar, IOU >.1\nCorrect: 71.6% Correct: 65.5%Loc: 8.6%Sim: 4.3%Other: 1.9%Background: 13.6%\nLoc: 19.0%Sim: 6.75%Other: 4.0%Background: 4.75%Fast R-CNN YOLOFigure 4: Error Analysis: Fast R-CNN vs. YOLO These\ncharts show the percentage of localization and background errors\nin the top N detections for various categories (N = # objects in that\ncategory).\n\u2022Other: class is wrong, IOU >.1\n\u2022Background: IOU <.1for any object\nFigure 4 shows the breakdown of each error type aver-\naged across all 20 classes.\nYOLO struggles to localize objects correctly. Localiza-\ntion errors account for more of YOLO\u2019s errors than all other\nsources combined. Fast R-CNN makes much fewer local-\nization errors but far more background errors. 13.6% of\nit\u2019s top detections are false positives that don\u2019t contain any\nobjects. Fast R-CNN is almost 3x more likely to predict\nbackground detections than YOLO.\n4.3. Combining Fast R-CNN and YOLO\nYOLO makes far fewer background mistakes than Fast\nR-CNN.",
        "e8d12c06-3d34-477e-b3ff-cfad4c4c7206": "YOLO These\ncharts show the percentage of localization and background errors\nin the top N detections for various categories (N = # objects in that\ncategory).\n\u2022Other: class is wrong, IOU >.1\n\u2022Background: IOU <.1for any object\nFigure 4 shows the breakdown of each error type aver-\naged across all 20 classes.\nYOLO struggles to localize objects correctly. Localiza-\ntion errors account for more of YOLO\u2019s errors than all other\nsources combined. Fast R-CNN makes much fewer local-\nization errors but far more background errors. 13.6% of\nit\u2019s top detections are false positives that don\u2019t contain any\nobjects. Fast R-CNN is almost 3x more likely to predict\nbackground detections than YOLO.\n4.3. Combining Fast R-CNN and YOLO\nYOLO makes far fewer background mistakes than Fast\nR-CNN. By using YOLO to eliminate background detec-\ntions from Fast R-CNN we get a signi\ufb01cant boost in perfor-\nmance. For every bounding box that R-CNN predicts we\ncheck to see if YOLO predicts a similar box. If it does, we\ngive that prediction a boost based on the probability pre-\ndicted by YOLO and the overlap between the two boxes.\nThe best Fast R-CNN model achieves a mAP of 71.8%\non the VOC 2007 test set. When combined with YOLO, its\nmAP Combined Gain\nFast R-CNN 71.8 - -\nFast R-CNN (2007 data) 66.9 72.4 .6\nFast R-CNN (VGG-M) 59.2 72.4 .6\nFast R-CNN (CaffeNet) 57.1 72.1 .3\nYOLO 63.4 75.0 3.2\nTable 2: Model combination experiments on VOC 2007. We\nexamine the effect of combining various models with the best ver-\nsion of Fast R-CNN. Other versions of Fast R-CNN provide only\na small bene\ufb01t while YOLO provides a signi\ufb01cant performance\nboost.",
        "c6e3a5ac-5f12-48ae-b5ef-113bea634d25": "VOC 2012 test mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike personplant sheep sofa train tv\nMR CNN MORE DATA [11] 73.9 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0\nHyperNet VGG 71.4 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7\nHyperNet SP 71.3 84.1 78.3 73.3 55.5 53.6 78.6 79.6 87.5 49.5 74.9 52.1 85.6 81.6 83.2 81.6 48.4 73.2 59.3 79.7 65.6\nFast R-CNN + YOLO 70.7 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2\nMR CNN SCNN [11] 70.7 85.0 79.6 71.5 55.3 57.7 76.0 73.9 84.6 50.5 74.3 61.7 85.5 79.9 81.7 76.4 41.0 69.0 61.2 77.7 72.1\nFaster R-CNN [28] 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nDEEP ENS COCO 70.1 84.0 79.4 71.6 51.9 51.1 74.1 72.1 88.6 48.3 73.4 57.8 86.1 80.0 80.7 70.4 46.6 69.6 68.8 75.9 71.4\nNoC [29] 68.8 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1\nFast R-CNN [14] 68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2\nUMICH FGS STRUCT 66.4 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2\nNUS NIN C2000 [7] 63.8 80.2 73.8 61.9 43.7 43.0 70.3 67.6 80.7 41.",
        "b183149e-359e-4921-a5ac-9830603947dc": "8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2\nUMICH FGS STRUCT 66.4 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2\nNUS NIN C2000 [7] 63.8 80.2 73.8 61.9 43.7 43.0 70.3 67.6 80.7 41.9 69.7 51.7 78.2 75.2 76.9 65.1 38.6 68.3 58.0 68.7 63.3\nBabyLearning [7] 63.2 78.0 74.2 61.3 45.7 42.7 68.2 66.8 80.2 40.6 70.0 49.8 79.0 74.5 77.9 64.0 35.3 67.9 55.7 68.7 62.6\nNUS NIN 62.4 77.9 73.1 62.6 39.5 43.3 69.1 66.4 78.9 39.1 68.1 50.0 77.2 71.3 76.1 64.7 38.4 66.9 56.2 66.9 62.7\nR-CNN VGG BB [13] 62.4 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3\nR-CNN VGG [13] 59.2 76.8 70.9 56.6 37.5 36.9 62.9 63.6 81.1 35.7 64.3 43.9 80.4 71.6 74.0 60.0 30.8 63.4 52.0 63.5 58.7\nYOLO 57.9 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8\nFeature Edit [33] 56.3 74.6 69.1 54.4 39.1 33.1 65.2 62.7 69.7 30.8 56.0 44.6 70.0 64.4 71.1 60.2 33.3 61.3 46.4 61.7 57.8\nR-CNN BB [13] 53.3 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1\nSDS [16] 50.7 69.7 58.4 48.5 28.3 28.8 61.3 57.5 70.8 24.1 50.7 35.9 64.9 59.1 65.8 57.1 26.0 58.8 38.6 58.9 50.7\nR-CNN [13] 49.6 68.1 63.8 46.1 29.",
        "d69d94ba-381d-4c74-8ec7-0a80bf0135f2": "3 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1\nSDS [16] 50.7 69.7 58.4 48.5 28.3 28.8 61.3 57.5 70.8 24.1 50.7 35.9 64.9 59.1 65.8 57.1 26.0 58.8 38.6 58.9 50.7\nR-CNN [13] 49.6 68.1 63.8 46.1 29.4 27.9 56.6 57.0 65.9 26.5 48.7 39.5 66.2 57.3 65.4 53.2 26.2 54.5 38.1 50.6 51.6\nTable 3: PASCAL VOC 2012 Leaderboard. YOLO compared with the full comp4 (outside data allowed) public leaderboard as of\nNovember 6th, 2015. Mean average precision and per-class average precision are shown for a variety of detection methods. YOLO is the\nonly real-time detector. Fast R-CNN + YOLO is the forth highest scoring method, with a 2.3% boost over Fast R-CNN.\nmAP increases by 3.2% to 75.0%. We also tried combining\nthe top Fast R-CNN model with several other versions of\nFast R-CNN. Those ensembles produced small increases in\nmAP between .3 and .6%, see Table 2 for details.\nThe boost from YOLO is not simply a byproduct of\nmodel ensembling since there is little bene\ufb01t from combin-\ning different versions of Fast R-CNN. Rather, it is precisely\nbecause YOLO makes different kinds of mistakes at test\ntime that it is so effective at boosting Fast R-CNN\u2019s per-\nformance.\nUnfortunately, this combination doesn\u2019t bene\ufb01t from the\nspeed of YOLO since we run each model seperately and\nthen combine the results. However, since YOLO is so fast\nit doesn\u2019t add any signi\ufb01cant computational time compared\nto Fast R-CNN.\n4.4. VOC 2012 Results\nOn the VOC 2012 test set, YOLO scores 57.9% mAP.\nThis is lower than the current state of the art, closer to\nthe original R-CNN using VGG-16, see Table 3. Our sys-\ntem struggles with small objects compared to its closest\ncompetitors. On categories like bottle ,sheep , and\ntv/monitor YOLO scores 8-10% lower than R-CNN or\nFeature Edit. However, on other categories like cat and\ntrain YOLO achieves higher performance.\nOur combined Fast R-CNN + YOLO model is one of the\nhighest performing detection methods. Fast R-CNN gets\na 2.3% improvement from the combination with YOLO,\nboosting it 5 spots up on the public leaderboard.\n4.5. Generalizability: Person Detection in Artwork\nAcademic datasets for object detection draw the training\nand testing data from the same distribution. In real-world\napplications it is hard to predict all possible use cases andthe test data can diverge from what the system has seen be-\nfore [3]. We compare YOLO to other detection systems on\nthe Picasso Dataset [12] and the People-Art Dataset [3], two\ndatasets for testing person detection on artwork.\nFigure 5 shows comparative performance between\nYOLO and other detection methods. For reference, we give\nVOC 2007 detection AP on person where all models are\ntrained only on VOC 2007 data. On Picasso models are\ntrained on VOC 2012 while on People-Art they are trained\non VOC 2010.\nR-CNN has high AP on VOC 2007. However, R-CNN\ndrops off considerably when applied to artwork. R-CNN\nuses Selective Search for bounding box proposals which is\ntuned for natural images.",
        "9149fb6f-b6a0-43d9-a7d1-e153021afd76": "In real-world\napplications it is hard to predict all possible use cases andthe test data can diverge from what the system has seen be-\nfore [3]. We compare YOLO to other detection systems on\nthe Picasso Dataset [12] and the People-Art Dataset [3], two\ndatasets for testing person detection on artwork.\nFigure 5 shows comparative performance between\nYOLO and other detection methods. For reference, we give\nVOC 2007 detection AP on person where all models are\ntrained only on VOC 2007 data. On Picasso models are\ntrained on VOC 2012 while on People-Art they are trained\non VOC 2010.\nR-CNN has high AP on VOC 2007. However, R-CNN\ndrops off considerably when applied to artwork. R-CNN\nuses Selective Search for bounding box proposals which is\ntuned for natural images. The classi\ufb01er step in R-CNN only\nsees small regions and needs good proposals.\nDPM maintains its AP well when applied to artwork.\nPrior work theorizes that DPM performs well because it has\nstrong spatial models of the shape and layout of objects.\nThough DPM doesn\u2019t degrade as much as R-CNN, it starts\nfrom a lower AP.\nYOLO has good performance on VOC 2007 and its AP\ndegrades less than other methods when applied to artwork.\nLike DPM, YOLO models the size and shape of objects,\nas well as relationships between objects and where objects\ncommonly appear. Artwork and natural images are very\ndifferent on a pixel level but they are similar in terms of\nthe size and shape of objects, thus YOLO can still predict\ngood bounding boxes and detections.\n5. Real-Time Detection In The Wild\nYOLO is a fast, accurate object detector, making it ideal\nfor computer vision applications. We connect YOLO to a\nwebcam and verify that it maintains real-time performance,",
        "95a0c2e5-2407-4614-94aa-c600a79229a3": "Poselets\nRCNN\nD&THumans\nDPMYOLO\n(a)Picasso Dataset precision-recall curves.VOC 2007 Picasso People-Art\nAP AP BestF1 AP\nYOLO 59.2 53.3 0.590 45\nR-CNN 54.2 10.4 0.226 26\nDPM 43.2 37.8 0.458 32\nPoselets [2] 36.5 17.8 0.271\nD&T [4] - 1.9 0.051\n(b)Quantitative results on the VOC 2007, Picasso, and People-Art Datasets.\nThe Picasso Dataset evaluates on both AP and best F1score.\nFigure 5: Generalization results on Picasso and People-Art datasets.\nFigure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it\ndoes think one person is an airplane.\nincluding the time to fetch images from the camera and dis-\nplay the detections.\nThe resulting system is interactive and engaging. While\nYOLO processes images individually, when attached to a\nwebcam it functions like a tracking system, detecting ob-\njects as they move around and change in appearance. A\ndemo of the system and the source code can be found on\nour project website: http://pjreddie.com/yolo/ .\n6. Conclusion\nWe introduce YOLO, a uni\ufb01ed model for object detec-\ntion. Our model is simple to construct and can be traineddirectly on full images. Unlike classi\ufb01er-based approaches,\nYOLO is trained on a loss function that directly corresponds\nto detection performance and the entire model is trained\njointly.\nFast YOLO is the fastest general-purpose object detec-\ntor in the literature and YOLO pushes the state-of-the-art in\nreal-time object detection. YOLO also generalizes well to\nnew domains making it ideal for applications that rely on\nfast, robust object detection.\nAcknowledgements: This work is partially supported by\nONR N00014-13-1-0720, NSF IIS-1338054, and The Allen\nDistinguished Investigator Award.",
        "644cccac-1820-4a01-b1fe-cbd81d74e355": "References\n[1] M. B. Blaschko and C. H. Lampert. Learning to localize ob-\njects with structured output regression. In Computer Vision\u2013\nECCV 2008 , pages 2\u201315. Springer, 2008. 4\n[2] L. Bourdev and J. Malik. Poselets: Body part detectors\ntrained using 3d human pose annotations. In International\nConference on Computer Vision (ICCV) , 2009. 8\n[3] H. Cai, Q. Wu, T. Corradi, and P. Hall. The cross-\ndepiction problem: Computer vision algorithms for recog-\nnising objects in artwork and in photographs. arXiv preprint\narXiv:1505.00110 , 2015. 7\n[4] N. Dalal and B. Triggs. Histograms of oriented gradients for\nhuman detection. In Computer Vision and Pattern Recogni-\ntion, 2005. CVPR 2005. IEEE Computer Society Conference\non, volume 1, pages 886\u2013893. IEEE, 2005. 4, 8\n[5] T. Dean, M. Ruzon, M. Segal, J. Shlens, S. Vijaya-\nnarasimhan, J. Yagnik, et al. Fast, accurate detection of\n100,000 object classes on a single machine. In Computer\nVision and Pattern Recognition (CVPR), 2013 IEEE Confer-\nence on , pages 1814\u20131821. IEEE, 2013. 5\n[6] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-\nvation feature for generic visual recognition. arXiv preprint\narXiv:1310.1531 , 2013. 4\n[7] J. Dong, Q. Chen, S. Yan, and A. Yuille. Towards uni\ufb01ed\nobject detection and semantic segmentation. In Computer\nVision\u2013ECCV 2014 , pages 299\u2013314. Springer, 2014. 7\n[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\nobject detection using deep neural networks. In Computer\nVision and Pattern Recognition (CVPR), 2014 IEEE Confer-\nence on , pages 2155\u20132162. IEEE, 2014. 5, 6\n[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\nWilliams, J. Winn, and A. Zisserman. The pascal visual ob-\nject classes challenge: A retrospective. International Journal\nof Computer Vision , 111(1):98\u2013136, Jan. 2015. 2\n[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\nmanan. Object detection with discriminatively trained part\nbased models. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 32(9):1627\u20131645, 2010. 1, 4\n[11] S. Gidaris and N. Komodakis. Object detection via a multi-\nregion & semantic segmentation-aware CNN model. CoRR ,\nabs/1505.01749, 2015. 7\n[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting peo-\nple in cubist art. In Computer Vision-ECCV 2014 Workshops ,\npages 101\u2013116. Springer, 2014. 7\n[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In Computer Vision and Pattern Recognition\n(CVPR), 2014 IEEE Conference on , pages 580\u2013587. IEEE,\n2014. 1, 4, 7\n[14] R. B. Girshick. Fast R-CNN. CoRR , abs/1504.08083, 2015.",
        "48461d0d-28f5-4431-9e93-17fffdba3368": "CoRR ,\nabs/1505.01749, 2015. 7\n[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting peo-\nple in cubist art. In Computer Vision-ECCV 2014 Workshops ,\npages 101\u2013116. Springer, 2014. 7\n[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In Computer Vision and Pattern Recognition\n(CVPR), 2014 IEEE Conference on , pages 580\u2013587. IEEE,\n2014. 1, 4, 7\n[14] R. B. Girshick. Fast R-CNN. CoRR , abs/1504.08083, 2015.\n2, 5, 6, 7\n[15] S. Gould, T. Gao, and D. Koller. Region-based segmenta-\ntion and object detection. In Advances in neural information\nprocessing systems , pages 655\u2013663, 2009. 4[16] B. Hariharan, P. Arbel \u00b4aez, R. Girshick, and J. Malik. Simul-\ntaneous detection and segmentation. In Computer Vision\u2013\nECCV 2014 , pages 297\u2013312. Springer, 2014. 7\n[17] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\nin deep convolutional networks for visual recognition. arXiv\npreprint arXiv:1406.4729 , 2014. 5\n[18] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. Improving neural networks by pre-\nventing co-adaptation of feature detectors. arXiv preprint\narXiv:1207.0580 , 2012. 4\n[19] D. Hoiem, Y . Chodpathumwan, and Q. Dai. Diagnosing error\nin object detectors. In Computer Vision\u2013ECCV 2012 , pages\n340\u2013353. Springer, 2012. 6\n[20] K. Lenc and A. Vedaldi. R-cnn minus r. arXiv preprint\narXiv:1506.06981 , 2015. 5, 6\n[21] R. Lienhart and J. Maydt. An extended set of haar-like fea-\ntures for rapid object detection. In Image Processing. 2002.\nProceedings. 2002 International Conference on , volume 1,\npages I\u2013900. IEEE, 2002. 4\n[22] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR ,\nabs/1312.4400, 2013. 2\n[23] D. G. Lowe. Object recognition from local scale-invariant\nfeatures. In Computer vision, 1999. The proceedings of the\nseventh IEEE international conference on , volume 2, pages\n1150\u20131157. Ieee, 1999. 4\n[24] D. Mishkin. Models accuracy on imagenet 2012\nval. https://github.com/BVLC/caffe/wiki/\nModels-accuracy-on-ImageNet-2012-val . Ac-\ncessed: 2015-10-2. 3\n[25] C. P. Papageorgiou, M. Oren, and T. Poggio. A general\nframework for object detection. In Computer vision, 1998.\nsixth international conference on , pages 555\u2013562. IEEE,\n1998. 4\n[26] J. Redmon. Darknet: Open source neural networks in c.\nhttp://pjreddie.com/darknet/ , 2013\u20132016. 3\n[27] J. Redmon and A. Angelova. Real-time grasp detection using\nconvolutional neural networks. CoRR , abs/1412.3128, 2014.\n5\n[28] S. Ren, K. He, R. Girshick, and J. Sun.",
        "446177f8-bab6-494f-aefc-7af550fba5b9": "Ac-\ncessed: 2015-10-2. 3\n[25] C. P. Papageorgiou, M. Oren, and T. Poggio. A general\nframework for object detection. In Computer vision, 1998.\nsixth international conference on , pages 555\u2013562. IEEE,\n1998. 4\n[26] J. Redmon. Darknet: Open source neural networks in c.\nhttp://pjreddie.com/darknet/ , 2013\u20132016. 3\n[27] J. Redmon and A. Angelova. Real-time grasp detection using\nconvolutional neural networks. CoRR , abs/1412.3128, 2014.\n5\n[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-\nwards real-time object detection with region proposal net-\nworks. arXiv preprint arXiv:1506.01497 , 2015. 5, 6, 7\n[29] S. Ren, K. He, R. B. Girshick, X. Zhang, and J. Sun. Object\ndetection networks on convolutional feature maps. CoRR ,\nabs/1504.06066, 2015. 3, 7\n[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\nRecognition Challenge. International Journal of Computer\nVision (IJCV) , 2015. 3\n[31] M. A. Sadeghi and D. Forsyth. 30hz object detection with\ndpm v5. In Computer Vision\u2013ECCV 2014 , pages 65\u201379.\nSpringer, 2014. 5, 6\n[32] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y . LeCun. Overfeat: Integrated recognition, localiza-\ntion and detection using convolutional networks. CoRR ,\nabs/1312.6229, 2013. 4, 5",
        "1cd0cdf3-d656-4de2-9455-41c2279b42fa": "[33] Z. Shen and X. Xue. Do more dropouts in pool5 feature maps\nfor better object detection. arXiv preprint arXiv:1409.6911 ,\n2014. 7\n[34] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. CoRR , abs/1409.4842,\n2014. 2\n[35] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.\nSmeulders. Selective search for object recognition. Inter-\nnational journal of computer vision , 104(2):154\u2013171, 2013.\n4\n[36] P. Viola and M. Jones. Robust real-time object detection.\nInternational Journal of Computer Vision , 4:34\u201347, 2001. 4\n[37] P. Viola and M. J. Jones. Robust real-time face detection.\nInternational journal of computer vision , 57(2):137\u2013154,\n2004. 5\n[38] J. Yan, Z. Lei, L. Wen, and S. Z. Li. The fastest deformable\npart model for object detection. In Computer Vision and Pat-\ntern Recognition (CVPR), 2014 IEEE Conference on , pages\n2497\u20132504. IEEE, 2014. 5, 6\n[39] C. L. Zitnick and P. Doll \u00b4ar. Edge boxes: Locating object pro-\nposals from edges. In Computer Vision\u2013ECCV 2014 , pages\n391\u2013405. Springer, 2014. 4",
        "c9b6dd7b-2c3e-43cf-9f4d-2a96b9f631ca": "U-Net: Convolutional Networks for Biomedical\nImage Segmentation\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox\nComputer Science Department and BIOSS Centre for Biological Signalling Studies,\nUniversity of Freiburg, Germany\nronneber@informatik.uni-freiburg.de ,\nWWW home page: http://lmb.informatik.uni-freiburg.de/\nAbstract. There is large consent that successful training of deep net-\nworks requires many thousand annotated training samples. In this pa-\nper, we present a network and training strategy that relies on the strong\nuse of data augmentation to use the available annotated samples more\ne\ufb03ciently. The architecture consists of a contracting path to capture\ncontext and a symmetric expanding path that enables precise localiza-\ntion. We show that such a network can be trained end-to-end from very\nfew images and outperforms the prior best method (a sliding-window\nconvolutional network) on the ISBI challenge for segmentation of neu-\nronal structures in electron microscopic stacks. Using the same net-\nwork trained on transmitted light microscopy images (phase contrast\nand DIC) we won the ISBI cell tracking challenge 2015 in these cate-\ngories by a large margin. Moreover, the network is fast. Segmentation\nof a 512x512 image takes less than a second on a recent GPU. The full\nimplementation (based on Ca\ufb00e) and the trained networks are available\nat http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.\n1 Introduction\nIn the last two years, deep convolutional networks have outperformed the state of\nthe art in many visual recognition tasks, e.g. [7,3]. While convolutional networks\nhave already existed for a long time [8], their success was limited due to the\nsize of the available training sets and the size of the considered networks. The\nbreakthrough by Krizhevsky et al. [7] was due to supervised training of a large\nnetwork with 8 layers and millions of parameters on the ImageNet dataset with\n1 million training images. Since then, even larger and deeper networks have been\ntrained [12].\nThe typical use of convolutional networks is on classi\ufb01cation tasks, where\nthe output to an image is a single class label. However, in many visual tasks,\nespecially in biomedical image processing, the desired output should include\nlocalization, i.e., a class label is supposed to be assigned to each pixel. More-\nover, thousands of training images are usually beyond reach in biomedical tasks.\nHence, Ciresan et al. [1] trained a network in a sliding-window setup to predict\nthe class label of each pixel by providing a local region (patch) around that pixelarXiv:1505.04597v1  [cs.CV]  18 May 2015",
        "79b4fcb0-bfdf-4e2b-98ba-a28ad747843f": "2\ncopy and cropinput\nimage\ntileoutput \nsegmentation \nmap641\n128\n256\n512\n1024max pool 2x2\nup-conv 2x2conv 3x3, ReLU572 x 572\n284\u00b264\n128\n256\n512570 x 570\n568 x 568\n282\u00b2\n280\u00b2140\u00b2\n138\u00b2\n136\u00b268\u00b2\n66\u00b2\n64\u00b232\u00b2\n28\u00b256\u00b2\n54\u00b2\n52\u00b2512\n104\u00b2\n102\u00b2\n100\u00b2200\u00b230\u00b2\n198\u00b2\n196\u00b2392 x 392\n390 x 390\n388 x 388\n388 x 388\n102451225625612864128642\nconv 1x1\nFig. 1. U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue\nbox corresponds to a multi-channel feature map. The number of channels is denoted\non top of the box. The x-y-size is provided at the lower left edge of the box. White\nboxes represent copied feature maps. The arrows denote the di\ufb00erent operations.\nas input. First, this network can localize. Secondly, the training data in terms\nof patches is much larger than the number of training images. The resulting\nnetwork won the EM segmentation challenge at ISBI 2012 by a large margin.\nObviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it\nis quite slow because the network must be run separately for each patch, and\nthere is a lot of redundancy due to overlapping patches. Secondly, there is a\ntrade-o\ufb00 between localization accuracy and the use of context. Larger patches\nrequire more max-pooling layers that reduce the localization accuracy, while\nsmall patches allow the network to see only little context. More recent approaches\n[11,4] proposed a classi\ufb01er output that takes into account the features from\nmultiple layers. Good localization and the use of context are possible at the\nsame time.\nIn this paper, we build upon a more elegant architecture, the so-called \u201cfully\nconvolutional network\u201d [9]. We modify and extend this architecture such that it\nworks with very few training images and yields more precise segmentations; see\nFigure 1. The main idea in [9] is to supplement a usual contracting network by\nsuccessive layers, where pooling operators are replaced by upsampling operators.\nHence, these layers increase the resolution of the output. In order to localize, high\nresolution features from the contracting path are combined with the upsampled",
        "04aa873f-faea-49eb-9d7c-e2f925aa1f89": "3\nFig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here\nsegmentation of neuronal structures in EM stacks). Prediction of the segmentation in\nthe yellow area, requires image data within the blue area as input. Missing input data\nis extrapolated by mirroring\noutput. A successive convolution layer can then learn to assemble a more precise\noutput based on this information.\nOne important modi\ufb01cation in our architecture is that in the upsampling\npart we have also a large number of feature channels, which allow the network\nto propagate context information to higher resolution layers. As a consequence,\nthe expansive path is more or less symmetric to the contracting path, and yields\na u-shaped architecture. The network does not have any fully connected layers\nand only uses the valid part of each convolution, i.e., the segmentation map only\ncontains the pixels, for which the full context is available in the input image.\nThis strategy allows the seamless segmentation of arbitrarily large images by an\noverlap-tile strategy (see Figure 2). To predict the pixels in the border region\nof the image, the missing context is extrapolated by mirroring the input image.\nThis tiling strategy is important to apply the network to large images, since\notherwise the resolution would be limited by the GPU memory.\nAs for our tasks there is very little training data available, we use excessive\ndata augmentation by applying elastic deformations to the available training im-\nages. This allows the network to learn invariance to such deformations, without\nthe need to see these transformations in the annotated image corpus. This is\nparticularly important in biomedical segmentation, since deformation used to\nbe the most common variation in tissue and realistic deformations can be simu-\nlated e\ufb03ciently. The value of data augmentation for learning invariance has been\nshown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning.\nAnother challenge in many cell segmentation tasks is the separation of touch-\ning objects of the same class; see Figure 3. To this end, we propose the use of\na weighted loss, where the separating background labels between touching cells\nobtain a large weight in the loss function.\nThe resulting network is applicable to various biomedical segmentation prob-\nlems. In this paper, we show results on the segmentation of neuronal structures\nin EM stacks (an ongoing competition started at ISBI 2012), where we out-",
        "0c3dd57f-8d01-4409-9310-3d27291e63cc": "4\nperformed the network of Ciresan et al. [1]. Furthermore, we show results for\ncell segmentation in light microscopy images from the ISBI cell tracking chal-\nlenge 2015. Here we won with a large margin on the two most challenging 2D\ntransmitted light datasets.\n2 Network Architecture\nThe network architecture is illustrated in Figure 1. It consists of a contracting\npath (left side) and an expansive path (right side). The contracting path follows\nthe typical architecture of a convolutional network. It consists of the repeated\napplication of two 3x3 convolutions (unpadded convolutions), each followed by\na recti\ufb01ed linear unit (ReLU) and a 2x2 max pooling operation with stride 2\nfor downsampling. At each downsampling step we double the number of feature\nchannels. Every step in the expansive path consists of an upsampling of the\nfeature map followed by a 2x2 convolution (\u201cup-convolution\u201d) that halves the\nnumber of feature channels, a concatenation with the correspondingly cropped\nfeature map from the contracting path, and two 3x3 convolutions, each fol-\nlowed by a ReLU. The cropping is necessary due to the loss of border pixels in\nevery convolution. At the \ufb01nal layer a 1x1 convolution is used to map each 64-\ncomponent feature vector to the desired number of classes. In total the network\nhas 23 convolutional layers.\nTo allow a seamless tiling of the output segmentation map (see Figure 2), it\nis important to select the input tile size such that all 2x2 max-pooling operations\nare applied to a layer with an even x- and y-size.\n3 Training\nThe input images and their corresponding segmentation maps are used to train\nthe network with the stochastic gradient descent implementation of Ca\ufb00e [6].\nDue to the unpadded convolutions, the output image is smaller than the input\nby a constant border width. To minimize the overhead and make maximum use\nof the GPU memory, we favor large input tiles over a large batch size and hence\nreduce the batch to a single image. Accordingly we use a high momentum (0.99)\nsuch that a large number of the previously seen training samples determine the\nupdate in the current optimization step.\nThe energy function is computed by a pixel-wise soft-max over the \ufb01nal\nfeature map combined with the cross entropy loss function. The soft-max is\nde\ufb01ned aspk(x) = exp(ak(x))/(\u2211K\nk\u2032=1exp(ak\u2032(x)))\nwhereak(x) denotes the\nactivation in feature channel kat the pixel position x\u2208\u2126with\u2126\u2282Z2.K\nis the number of classes and pk(x) is the approximated maximum-function. I.e.\npk(x)\u22481 for thekthat has the maximum activation ak(x) andpk(x)\u22480 for\nall otherk. The cross entropy then penalizes at each position the deviation of\np\u2113(x)(x) from 1 using\nE=\u2211\nx\u2208\u2126w(x) log(p\u2113(x)(x)) (1)",
        "464c77d7-139d-4499-9f99-b62f19d9914d": "5\na\n b\n c\n d\nFig. 3. HeLa cells on glass recorded with DIC (di\ufb00erential interference contrast) mi-\ncroscopy. ( a) raw image. ( b) overlay with ground truth segmentation. Di\ufb00erent colors\nindicate di\ufb00erent instances of the HeLa cells. ( c) generated segmentation mask (white:\nforeground, black: background). ( d) map with a pixel-wise loss weight to force the\nnetwork to learn the border pixels.\nwhere\u2113:\u2126\u2192{1,...,K}is the true label of each pixel and w:\u2126\u2192Ris\na weight map that we introduced to give some pixels more importance in the\ntraining.\nWe pre-compute the weight map for each ground truth segmentation to com-\npensate the di\ufb00erent frequency of pixels from a certain class in the training\ndata set, and to force the network to learn the small separation borders that we\nintroduce between touching cells (See Figure 3c and d).\nThe separation border is computed using morphological operations. The\nweight map is then computed as\nw(x) =wc(x) +w0\u00b7exp(\n\u2212(d1(x) +d2(x))2\n2\u03c32)\n(2)\nwherewc:\u2126\u2192Ris the weight map to balance the class frequencies, d1:\u2126\u2192R\ndenotes the distance to the border of the nearest cell and d2:\u2126\u2192Rthe distance\nto the border of the second nearest cell. In our experiments we set w0= 10 and\n\u03c3\u22485 pixels.\nIn deep networks with many convolutional layers and di\ufb00erent paths through\nthe network, a good initialization of the weights is extremely important. Oth-\nerwise, parts of the network might give excessive activations, while other parts\nnever contribute. Ideally the initial weights should be adapted such that each\nfeature map in the network has approximately unit variance. For a network with\nour architecture (alternating convolution and ReLU layers) this can be achieved\nby drawing the initial weights from a Gaussian distribution with a standard\ndeviation of\u221a\n2/N, whereNdenotes the number of incoming nodes of one neu-\nron [5]. E.g. for a 3x3 convolution and 64 feature channels in the previous layer\nN= 9\u00b764 = 576.\n3.1 Data Augmentation\nData augmentation is essential to teach the network the desired invariance and\nrobustness properties, when only few training samples are available. In case of",
        "5f325b1a-5e61-41e6-9bcd-36b9499ad88d": "6\nmicroscopical images we primarily need shift and rotation invariance as well as\nrobustness to deformations and gray value variations. Especially random elas-\ntic deformations of the training samples seem to be the key concept to train\na segmentation network with very few annotated images. We generate smooth\ndeformations using random displacement vectors on a coarse 3 by 3 grid. The\ndisplacements are sampled from a Gaussian distribution with 10 pixels standard\ndeviation. Per-pixel displacements are then computed using bicubic interpola-\ntion. Drop-out layers at the end of the contracting path perform further implicit\ndata augmentation.\n4 Experiments\nWe demonstrate the application of the u-net to three di\ufb00erent segmentation\ntasks. The \ufb01rst task is the segmentation of neuronal structures in electron mi-\ncroscopic recordings. An example of the data set and our obtained segmentation\nis displayed in Figure 2. We provide the full result as Supplementary Material.\nThe data set is provided by the EM segmentation challenge [14] that was started\nat ISBI 2012 and is still open for new contributions. The training data is a set of\n30 images (512x512 pixels) from serial section transmission electron microscopy\nof the Drosophila \ufb01rst instar larva ventral nerve cord (VNC). Each image comes\nwith a corresponding fully annotated ground truth segmentation map for cells\n(white) and membranes (black). The test set is publicly available, but its seg-\nmentation maps are kept secret. An evaluation can be obtained by sending the\npredicted membrane probability map to the organizers. The evaluation is done\nby thresholding the map at 10 di\ufb00erent levels and computation of the \u201cwarping\nerror\u201d, the \u201cRand error\u201d and the \u201cpixel error\u201d [14].\nThe u-net (averaged over 7 rotated versions of the input data) achieves with-\nout any further pre- or postprocessing a warping error of 0.0003529 (the new\nbest score, see Table 1) and a rand-error of 0.0382.\nThis is signi\ufb01cantly better than the sliding-window convolutional network\nresult by Ciresan et al. [1], whose best submission had a warping error of 0.000420\nand a rand error of 0.0504. In terms of rand error the only better performing\nTable 1. Ranking on the EM segmentation challenge [14] (march 6th, 2015), sorted\nby warping error.\nRank Group name Warping Error Rand Error Pixel Error\n** human values ** 0.000005 0.0021 0.0010\n1. u-net 0.000353 0.0382 0.0611\n2. DIVE-SCI 0.000355 0.0305 0.0584\n3. IDSIA [1] 0.000420 0.0504 0.0613\n4. DIVE 0.000430 0.0545 0.0582\n...\n10. IDSIA-SCI 0.000653 0.0189 0.1027",
        "7ed7b0dd-bec9-4b6a-9a64-b1a4df0913c9": "7\na\n b\n c\n d\nFig. 4. Result on the ISBI cell tracking challenge. ( a) part of an input image of the\n\u201cPhC-U373\u201d data set. ( b) Segmentation result (cyan mask) with manual ground truth\n(yellow border) ( c) input image of the \u201cDIC-HeLa\u201d data set. ( d) Segmentation result\n(random colored masks) with manual ground truth (yellow border).\nTable 2. Segmentation results (IOU) on the ISBI cell tracking challenge 2015.\nName PhC-U373 DIC-HeLa\nIMCB-SG (2014) 0.2669 0.2935\nKTH-SE (2014) 0.7953 0.4607\nHOUS-US (2014) 0.5323 -\nsecond-best 2015 0.83 0.46\nu-net (2015) 0.9203 0.7756\nalgorithms on this data set use highly data set speci\ufb01c post-processing methods1\napplied to the probability map of Ciresan et al. [1].\nWe also applied the u-net to a cell segmentation task in light microscopic im-\nages. This segmenation task is part of the ISBI cell tracking challenge 2014 and\n2015 [10,13]. The \ufb01rst data set \u201cPhC-U373\u201d2contains Glioblastoma-astrocytoma\nU373 cells on a polyacrylimide substrate recorded by phase contrast microscopy\n(see Figure 4a,b and Supp. Material). It contains 35 partially annotated train-\ning images. Here we achieve an average IOU (\u201cintersection over union\u201d) of 92%,\nwhich is signi\ufb01cantly better than the second best algorithm with 83% (see Ta-\nble 2). The second data set \u201cDIC-HeLa\u201d3are HeLa cells on a \ufb02at glass recorded\nby di\ufb00erential interference contrast (DIC) microscopy (see Figure 3, Figure 4c,d\nand Supp. Material). It contains 20 partially annotated training images. Here we\nachieve an average IOU of 77.5% which is signi\ufb01cantly better than the second\nbest algorithm with 46%.\n5 Conclusion\nThe u-net architecture achieves very good performance on very di\ufb00erent biomed-\nical segmentation applications. Thanks to data augmentation with elastic defor-\n1The authors of this algorithm have submitted 78 di\ufb00erent solutions to achieve this\nresult.\n2Data set provided by Dr. Sanjay Kumar. Department of Bioengineering University\nof California at Berkeley. Berkeley CA (USA)\n3Data set provided by Dr. Gert van Cappellen Erasmus Medical Center. Rotterdam.\nThe Netherlands",
        "a70c55ea-c3c2-41fe-8f4b-457722afbd06": "8\nmations, it only needs very few annotated images and has a very reasonable\ntraining time of only 10 hours on a NVidia Titan GPU (6 GB). We provide the\nfull Ca\ufb00e[6]-based implementation and the trained networks4. We are sure that\nthe u-net architecture can be applied easily to many more tasks.\nAcknowlegements\nThis study was supported by the Excellence Initiative of the German Federal\nand State governments (EXC 294) and by the BMBF (Fkz 0316185B).\nReferences\n1. Ciresan, D.C., Gambardella, L.M., Giusti, A., Schmidhuber, J.: Deep neural net-\nworks segment neuronal membranes in electron microscopy images. In: NIPS. pp.\n2852\u20132860 (2012)\n2. Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative un-\nsupervised feature learning with convolutional neural networks. In: NIPS (2014)\n3. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for ac-\ncurate object detection and semantic segmentation. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) (2014)\n4. Hariharan, B., Arbelez, P., Girshick, R., Malik, J.: Hypercolumns for object seg-\nmentation and \ufb01ne-grained localization (2014), arXiv:1411.5752 [cs.CV]\n5. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into recti\ufb01ers: Surpassing human-\nlevel performance on imagenet classi\ufb01cation (2015), arXiv:1502.01852 [cs.CV]\n6. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-\nrama, S., Darrell, T.: Ca\ufb00e: Convolutional architecture for fast feature embedding\n(2014), arXiv:1408.5093 [cs.CV]\n7. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classi\ufb01cation with deep con-\nvolutional neural networks. In: NIPS. pp. 1106\u20131114 (2012)\n8. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.,\nJackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural\nComputation 1(4), 541\u2013551 (1989)\n9. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation (2014), arXiv:1411.4038 [cs.CV]\n10. Maska, M., (...), de Solorzano, C.O.: A benchmark for comparison of cell tracking\nalgorithms. Bioinformatics 30, 1609\u20131617 (2014)\n11. Seyedhosseini, M., Sajjadi, M., Tasdizen, T.: Image segmentation with cascaded\nhierarchical models and logistic disjunctive normal networks. In: Computer Vision\n(ICCV), 2013 IEEE International Conference on. pp. 2168\u20132175 (2013)\n12. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition (2014), arXiv:1409.1556 [cs.CV]\n13. WWW: Web page of the cell tracking challenge, http://www.codesolorzano.com/\ncelltrackingchallenge/Cell_Tracking_Challenge/Welcome.html\n14. WWW: Web page of the em segmentation challenge, http://brainiac2.mit.edu/\nisbi_challenge/\n4U-net implementation, trained networks and supplementary material available at\nhttp://lmb.informatik.uni-freiburg.de/people/ronneber/u-net",
        "21f137e3-9d64-4c4b-9a05-a4e20a086d73": "YOLOv3: An Incremental Improvement\nJoseph Redmon, Ali Farhadi\nUniversity of Washington\nAbstract\nWe present some updates to YOLO! We made a bunch\nof little design changes to make it better. We also trained\nthis new network that\u2019s pretty swell. It\u2019s a little bigger than\nlast time but more accurate. It\u2019s still fast though, don\u2019t\nworry. At 320\u00d7320YOLOv3 runs in 22 ms at 28.2 mAP ,\nas accurate as SSD but three times faster. When we look\nat the old .5 IOU mAP detection metric YOLOv3 is quite\ngood. It achieves 57.9AP50in 51 ms on a Titan X, com-\npared to 57.5AP50in 198 ms by RetinaNet, similar perfor-\nmance but 3.8 \u00d7faster. As always, all the code is online at\nhttps://pjreddie.com/yolo/ .\n1. Introduction\nSometimes you just kinda phone it in for a year, you\nknow? I didn\u2019t do a whole lot of research this year. Spent\na lot of time on Twitter. Played around with GANs a little.\nI had a little momentum left over from last year [12] [1]; I\nmanaged to make some improvements to YOLO. But, hon-\nestly, nothing like super interesting, just a bunch of small\nchanges that make it better. I also helped out with other\npeople\u2019s research a little.\nActually, that\u2019s what brings us here today. We have\na camera-ready deadline [4] and we need to cite some of\nthe random updates I made to YOLO but we don\u2019t have a\nsource. So get ready for a TECH REPORT!\nThe great thing about tech reports is that they don\u2019t need\nintros, y\u2019all know why we\u2019re here. So the end of this intro-\nduction will signpost for the rest of the paper. First we\u2019ll tell\nyou what the deal is with YOLOv3. Then we\u2019ll tell you how\nwe do. We\u2019ll also tell you about some things we tried that\ndidn\u2019t work. Finally we\u2019ll contemplate what this all means.\n2. The Deal\nSo here\u2019s the deal with YOLOv3: We mostly took good\nideas from other people. We also trained a new classi\ufb01er\nnetwork that\u2019s better than the other ones. We\u2019ll just take\nyou through the whole system from scratch so you can un-\nderstand it all.\n50 100 150 200 250\ninference time (ms)283032343638COCO AP\nB CDEFGRetinaNet-50\nRetinaNet-101YOLOv3\nMethod\n[B] SSD321\n[C] DSSD321[D] R-FCN\n[E] SSD513\n[F] DSSD513\n[G] FPN FRCN\nRetinaNet-50-500\nRetinaNet-101-500\nRetinaNet-101-800\nYOLOv3-320YOLOv3-416\nYOLOv3-608mAP\n28.0\n28.0\n29.9\n31.2\n33.2\n36.2\n32.5\n34.4\n37.8\n28.2\n31.0\n33.0time\n61\n8585\n125156\n172\n73\n90\n198\n22\n29\n51Figure 1. We adapt this \ufb01gure from the Focal Loss paper [9].\nYOLOv3 runs signi\ufb01cantly faster than other detection methods\nwith comparable performance. Times from either an M40 or Titan\nX, they are basically the same GPU.\n2.1. Bounding Box Prediction\nFollowing YOLO9000 our system predicts bounding\nboxes using dimension clusters as anchor boxes [15]. The\nnetwork predicts 4 coordinates for each bounding box, tx,\nty,tw,th. If the cell is offset from the top left corner of the\nimage by (cx,cy)and the bounding box prior has width and\nheightpw,ph, then the predictions correspond to:\nbx=\u03c3(tx) +cx\nby=\u03c3(ty) +cy\nbw=pwetw\nbh=pheth\nDuring training we use sum of squared error loss.",
        "37ee2ce5-7e29-4f74-9a0e-d88294bfc47e": "We adapt this \ufb01gure from the Focal Loss paper [9].\nYOLOv3 runs signi\ufb01cantly faster than other detection methods\nwith comparable performance. Times from either an M40 or Titan\nX, they are basically the same GPU.\n2.1. Bounding Box Prediction\nFollowing YOLO9000 our system predicts bounding\nboxes using dimension clusters as anchor boxes [15]. The\nnetwork predicts 4 coordinates for each bounding box, tx,\nty,tw,th. If the cell is offset from the top left corner of the\nimage by (cx,cy)and the bounding box prior has width and\nheightpw,ph, then the predictions correspond to:\nbx=\u03c3(tx) +cx\nby=\u03c3(ty) +cy\nbw=pwetw\nbh=pheth\nDuring training we use sum of squared error loss. If the\nground truth for some coordinate prediction is \u02c6t*our gra-\ndient is the ground truth value (computed from the ground\ntruth box) minus our prediction: \u02c6t*\u2212t*. This ground truth\nvalue can be easily computed by inverting the equations\nabove.\nYOLOv3 predicts an objectness score for each bounding\nbox using logistic regression. This should be 1 if the bound-\ning box prior overlaps a ground truth object by more than\nany other bounding box prior. If the bounding box prior\n1",
        "071cea47-f792-4cbf-ab2f-3d9d82901d62": "\u03c3(tx)\u03c3(ty)pw\nphbhbw\nbw=pwe\nbh=phecx\ncy\nbx=\u03c3(tx)+cx\nby=\u03c3(ty)+cy\ntw\nthFigure 2. Bounding boxes with dimension priors and location\nprediction. We predict the width and height of the box as offsets\nfrom cluster centroids. We predict the center coordinates of the\nbox relative to the location of \ufb01lter application using a sigmoid\nfunction. This \ufb01gure blatantly self-plagiarized from [15].\nis not the best but does overlap a ground truth object by\nmore than some threshold we ignore the prediction, follow-\ning [17]. We use the threshold of .5. Unlike [17] our system\nonly assigns one bounding box prior for each ground truth\nobject. If a bounding box prior is not assigned to a ground\ntruth object it incurs no loss for coordinate or class predic-\ntions, only objectness.\n2.2. Class Prediction\nEach box predicts the classes the bounding box may con-\ntain using multilabel classi\ufb01cation. We do not use a softmax\nas we have found it is unnecessary for good performance,\ninstead we simply use independent logistic classi\ufb01ers. Dur-\ning training we use binary cross-entropy loss for the class\npredictions.\nThis formulation helps when we move to more complex\ndomains like the Open Images Dataset [7]. In this dataset\nthere are many overlapping labels (i.e. Woman and Person).\nUsing a softmax imposes the assumption that each box has\nexactly one class which is often not the case. A multilabel\napproach better models the data.\n2.3. Predictions Across Scales\nYOLOv3 predicts boxes at 3 different scales. Our sys-\ntem extracts features from those scales using a similar con-\ncept to feature pyramid networks [8]. From our base fea-\nture extractor we add several convolutional layers. The last\nof these predicts a 3-d tensor encoding bounding box, ob-\njectness, and class predictions. In our experiments with\nCOCO [10] we predict 3 boxes at each scale so the tensor is\nN\u00d7N\u00d7[3\u2217(4 + 1 + 80)] for the 4 bounding box offsets,\n1 objectness prediction, and 80 class predictions.Next we take the feature map from 2 layers previous and\nupsample it by 2\u00d7. We also take a feature map from earlier\nin the network and merge it with our upsampled features\nusing concatenation. This method allows us to get more\nmeaningful semantic information from the upsampled fea-\ntures and \ufb01ner-grained information from the earlier feature\nmap. We then add a few more convolutional layers to pro-\ncess this combined feature map, and eventually predict a\nsimilar tensor, although now twice the size.\nWe perform the same design one more time to predict\nboxes for the \ufb01nal scale. Thus our predictions for the 3rd\nscale bene\ufb01t from all the prior computation as well as \ufb01ne-\ngrained features from early on in the network.\nWe still use k-means clustering to determine our bound-\ning box priors. We just sort of chose 9 clusters and 3\nscales arbitrarily and then divide up the clusters evenly\nacross scales. On the COCO dataset the 9 clusters were:\n(10\u00d713),(16\u00d730),(33\u00d723),(30\u00d761),(62\u00d745),(59\u00d7\n119),(116\u00d790),(156\u00d7198),(373\u00d7326) .\n2.4. Feature Extractor\nWe use a new network for performing feature extraction.\nOur new network is a hybrid approach between the network\nused in YOLOv2, Darknet-19, and that newfangled residual\nnetwork stuff. Our network uses successive 3\u00d73and1\u00d71\nconvolutional layers but now has some shortcut connections\nas well and is signi\ufb01cantly larger. It has 53 convolutional\nlayers so we call it.... wait for it..... Darknet-53!",
        "eb6e9722-96bb-4352-88b0-0f52c1c6aab0": "We just sort of chose 9 clusters and 3\nscales arbitrarily and then divide up the clusters evenly\nacross scales. On the COCO dataset the 9 clusters were:\n(10\u00d713),(16\u00d730),(33\u00d723),(30\u00d761),(62\u00d745),(59\u00d7\n119),(116\u00d790),(156\u00d7198),(373\u00d7326) .\n2.4. Feature Extractor\nWe use a new network for performing feature extraction.\nOur new network is a hybrid approach between the network\nused in YOLOv2, Darknet-19, and that newfangled residual\nnetwork stuff. Our network uses successive 3\u00d73and1\u00d71\nconvolutional layers but now has some shortcut connections\nas well and is signi\ufb01cantly larger. It has 53 convolutional\nlayers so we call it.... wait for it..... Darknet-53!\nType\nConvolutional\nConvolutional\nConvolutional\nConvolutional\nResidual\nConvolutional\nConvolutional\nConvolutional\nResidual\nConvolutionalConvolutional\nConvolutional\nResidualConvolutional\nConvolutional\nConvolutionalResidual\nConvolutional\nConvolutional\nConvolutional\nResidual\nAvgpoolConnected\nSoftmaxFilters\n32\n64\n32\n64\n128\n64\n128\n256\n128\n256\n512\n256\n512\n1024\n512\n1024Size\n3 \u00d7 3\n3 \u00d7 3 / 2\n1 \u00d7 1\n3 \u00d7 3\n3 \u00d7 3 / 2\n1 \u00d7 1\n3 \u00d7 3\n3 \u00d7 3 / 2\n1 \u00d7 1\n3 \u00d7 3\n3 \u00d7 3 / 2\n1 \u00d7 1\n3 \u00d7 3\n3 \u00d7 3 / 2\n1 \u00d7 1\n3 \u00d7 3\nGlobal\n1000Output\n256 \u00d7 256\n128 \u00d7 128\n128 \u00d7 128\n64 \u00d7 6464 \u00d7 64\n32 \u00d7 32\n32 \u00d7 32\n16 \u00d7 16\n16 \u00d7 16\n8 \u00d7 8\n8 \u00d7 81\u00d7\n2\u00d7\n8\u00d7\n8\u00d7\n4\u00d7\nTable 1. Darknet-53.",
        "42265957-6c1d-41fb-94b2-00b4ba81a6c1": "This new network is much more powerful than Darknet-\n19 but still more ef\ufb01cient than ResNet-101 or ResNet-152.\nHere are some ImageNet results:\nBackbone Top-1 Top-5 Bn Ops BFLOP/s FPS\nDarknet-19 [15] 74.1 91.8 7.29 1246 171\nResNet-101[5] 77.1 93.7 19.7 1039 53\nResNet-152 [5] 77.6 93.8 29.4 1090 37\nDarknet-53 77.2 93.8 18.7 1457 78\nTable 2. Comparison of backbones. Accuracy, billions of oper-\nations, billion \ufb02oating point operations per second, and FPS for\nvarious networks.\nEach network is trained with identical settings and tested\nat256\u00d7256, single crop accuracy. Run times are measured\non a Titan X at 256\u00d7256. Thus Darknet-53 performs on\npar with state-of-the-art classi\ufb01ers but with fewer \ufb02oating\npoint operations and more speed. Darknet-53 is better than\nResNet-101 and 1.5\u00d7faster. Darknet-53 has similar perfor-\nmance to ResNet-152 and is 2\u00d7faster.\nDarknet-53 also achieves the highest measured \ufb02oating\npoint operations per second. This means the network struc-\nture better utilizes the GPU, making it more ef\ufb01cient to eval-\nuate and thus faster. That\u2019s mostly because ResNets have\njust way too many layers and aren\u2019t very ef\ufb01cient.\n2.5. Training\nWe still train on full images with no hard negative mining\nor any of that stuff. We use multi-scale training, lots of data\naugmentation, batch normalization, all the standard stuff.\nWe use the Darknet neural network framework for training\nand testing [14].\n3. How We Do\nYOLOv3 is pretty good! See table 3. In terms of COCOs\nweird average mean AP metric it is on par with the SSD\nvariants but is 3\u00d7faster. It is still quite a bit behind other\nbackbone AP AP 50 AP75 APS APM APL\nTwo-stage methods\nFaster R-CNN+++ [5] ResNet-101-C4 34.9 55.7 37.4 15.6 38.7 50.9\nFaster R-CNN w FPN [8] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2\nFaster R-CNN by G-RMI [6] Inception-ResNet-v2 [21] 34.7 55.5 36.7 13.5 38.1 52.0\nFaster R-CNN w TDM [20] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1\nOne-stage methods\nYOLOv2 [15] DarkNet-19 [15] 21.6 44.0 19.2 5.0 22.4 35.5\nSSD513 [11, 3] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8\nDSSD513 [3] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1\nRetinaNet [9] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2\nRetinaNet [9] ResNeXt-101-FPN 40.8 61.1 44.1 24.1 44.2 51.2\nYOLOv3 608\u00d7608 Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9\nTable 3. I\u2019m seriously just stealing all these tables from [9] they take soooo long to make from scratch. Ok, YOLOv3 is doing alright.",
        "f62021c2-9c5e-42f7-986a-9b5528c9a149": "I\u2019m seriously just stealing all these tables from [9] they take soooo long to make from scratch. Ok, YOLOv3 is doing alright.\nKeep in mind that RetinaNet has like 3.8\u00d7longer to process an image. YOLOv3 is much better than SSD variants and comparable to\nstate-of-the-art models on the AP 50metric.models like RetinaNet in this metric though.\nHowever, when we look at the \u201cold\u201d detection metric of\nmAP at IOU =.5(or AP 50in the chart) YOLOv3 is very\nstrong. It is almost on par with RetinaNet and far above\nthe SSD variants. This indicates that YOLOv3 is a very\nstrong detector that excels at producing decent boxes for ob-\njects. However, performance drops signi\ufb01cantly as the IOU\nthreshold increases indicating YOLOv3 struggles to get the\nboxes perfectly aligned with the object.\nIn the past YOLO struggled with small objects. How-\never, now we see a reversal in that trend. With the new\nmulti-scale predictions we see YOLOv3 has relatively high\nAPSperformance. However, it has comparatively worse\nperformance on medium and larger size objects. More in-\nvestigation is needed to get to the bottom of this.\nWhen we plot accuracy vs speed on the AP 50metric (see\n\ufb01gure 5) we see YOLOv3 has signi\ufb01cant bene\ufb01ts over other\ndetection systems. Namely, it\u2019s faster and better.\n4. Things We Tried That Didn\u2019t Work\nWe tried lots of stuff while we were working on\nYOLOv3. A lot of it didn\u2019t work. Here\u2019s the stuff we can\nremember.\nAnchor box x,yoffset predictions. We tried using the\nnormal anchor box prediction mechanism where you pre-\ndict thex,yoffset as a multiple of the box width or height\nusing a linear activation. We found this formulation de-\ncreased model stability and didn\u2019t work very well.\nLinearx,ypredictions instead of logistic. We tried\nusing a linear activation to directly predict the x,y offset\ninstead of the logistic activation. This led to a couple point\ndrop in mAP.\nFocal loss. We tried using focal loss. It dropped our\nmAP about 2 points. YOLOv3 may already be robust to\nthe problem focal loss is trying to solve because it has sep-\narate objectness predictions and conditional class predic-\ntions. Thus for most examples there is no loss from the\nclass predictions? Or something? We aren\u2019t totally sure.",
        "16d7d6c6-d456-48a0-b2f3-d5f7c5db248b": "50 100 150 200 250\ninference time (ms)485052545658COCO mAP-50\nB CD\nEFG\nRetinaNet-50\nRetinaNet-101YOLOv3\nMethod\n[B] SSD321\n[C] DSSD321\n[D] R-FCN\n[E] SSD513\n[F] DSSD513\n[G] FPN FRCN\nRetinaNet-50-500\nRetinaNet-101-500\nRetinaNet-101-800\nYOLOv3-320YOLOv3-416\nYOLOv3-608mAP-50\n45.4\n46.1\n51.9\n50.4\n53.3\n59.1\n50.9\n53.1\n57.5\n51.5\n55.3\n57.9time\n61\n85\n85\n125156\n172\n73\n90\n198\n22\n29\n51Figure 3. Again adapted from the [9], this time displaying speed/accuracy tradeoff on the mAP at .5 IOU metric. You can tell YOLOv3 is\ngood because it\u2019s very high and far to the left. Can you cite your own paper? Guess who\u2019s going to try, this guy \u2192[16]. Oh, I forgot, we\nalso \ufb01x a data loading bug in YOLOv2, that helped by like 2 mAP. Just sneaking this in here to not throw off layout.\nDual IOU thresholds and truth assignment. Faster R-\nCNN uses two IOU thresholds during training. If a predic-\ntion overlaps the ground truth by .7 it is as a positive exam-\nple, by [.3\u2212.7]it is ignored, less than .3 for all ground truth\nobjects it is a negative example. We tried a similar strategy\nbut couldn\u2019t get good results.\nWe quite like our current formulation, it seems to be at\na local optima at least. It is possible that some of these\ntechniques could eventually produce good results, perhaps\nthey just need some tuning to stabilize the training.\n5. What This All Means\nYOLOv3 is a good detector. It\u2019s fast, it\u2019s accurate. It\u2019s\nnot as great on the COCO average AP between .5 and .95\nIOU metric. But it\u2019s very good on the old detection metric\nof .5 IOU.\nWhy did we switch metrics anyway? The original\nCOCO paper just has this cryptic sentence: \u201cA full discus-\nsion of evaluation metrics will be added once the evaluation\nserver is complete\u201d. Russakovsky et al report that that hu-\nmans have a hard time distinguishing an IOU of .3 from .5!\n\u201cTraining humans to visually inspect a bounding box with\nIOU of 0.3 and distinguish it from one with IOU 0.5 is sur-prisingly dif\ufb01cult.\u201d [18] If humans have a hard time telling\nthe difference, how much does it matter?\nBut maybe a better question is: \u201cWhat are we going to\ndo with these detectors now that we have them?\u201d A lot of\nthe people doing this research are at Google and Facebook.\nI guess at least we know the technology is in good hands\nand de\ufb01nitely won\u2019t be used to harvest your personal infor-\nmation and sell it to.... wait, you\u2019re saying that\u2019s exactly\nwhat it will be used for?? Oh.\nWell the other people heavily funding vision research are\nthe military and they\u2019ve never done anything horrible like\nkilling lots of people with new technology oh wait.....1\nI have a lot of hope that most of the people using com-\nputer vision are just doing happy, good stuff with it, like\ncounting the number of zebras in a national park [13], or\ntracking their cat as it wanders around their house [19]. But\ncomputer vision is already being put to questionable use and\nas researchers we have a responsibility to at least consider\nthe harm our work might be doing and think of ways to mit-\nigate it. We owe the world that much.\nIn closing, do not @ me. (Because I \ufb01nally quit Twitter).\n1The author is funded by the Of\ufb01ce of Naval Research and Google.",
        "b8d9bb19-327f-41a8-b71d-c37c9c518d63": "References\n[1] Analogy. Wikipedia , Mar 2018. 1\n[2] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and\nA. Zisserman. The pascal visual object classes (voc) chal-\nlenge. International journal of computer vision , 88(2):303\u2013\n338, 2010. 6\n[3] C.-Y . Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.\nDssd: Deconvolutional single shot detector. arXiv preprint\narXiv:1701.06659 , 2017. 3\n[4] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox,\nand A. Farhadi. Iqa: Visual question answering in interactive\nenvironments. arXiv preprint arXiv:1712.03316 , 2017. 1\n[5] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n770\u2013778, 2016. 3\n[6] J. Huang, V . Rathod, C. Sun, M. Zhu, A. Korattikara,\nA. Fathi, I. Fischer, Z. Wojna, Y . Song, S. Guadarrama, et al.\nSpeed/accuracy trade-offs for modern convolutional object\ndetectors. 3\n[7] I. Krasin, T. Duerig, N. Alldrin, V . Ferrari, S. Abu-El-Haija,\nA. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. Veit,\nS. Belongie, V . Gomes, A. Gupta, C. Sun, G. Chechik,\nD. Cai, Z. Feng, D. Narayanan, and K. Murphy. Open-\nimages: A public dataset for large-scale multi-label and\nmulti-class image classi\ufb01cation. Dataset available from\nhttps://github.com/openimages , 2017. 2\n[8] T.-Y . Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and\nS. Belongie. Feature pyramid networks for object detection.\nInProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 2117\u20132125, 2017. 2, 3\n[9] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll \u00b4ar.\nFocal loss for dense object detection. arXiv preprint\narXiv:1708.02002 , 2017. 1, 3, 4\n[10] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll \u00b4ar, and C. L. Zitnick. Microsoft coco: Com-\nmon objects in context. In European conference on computer\nvision , pages 740\u2013755. Springer, 2014. 2\n[11] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-\nY . Fu, and A. C. Berg. Ssd: Single shot multibox detector.\nInEuropean conference on computer vision , pages 21\u201337.\nSpringer, 2016. 3\n[12] I. Newton. Philosophiae naturalis principia mathematica .\nWilliam Dawson & Sons Ltd., London, 1687. 1\n[13] J. Parham, J. Crall, C. Stewart, T. Berger-Wolf, and\nD. Rubenstein. Animal population censusing at scale with\ncitizen science and photographic identi\ufb01cation. 2017. 4\n[14] J. Redmon. Darknet: Open source neural networks in c.\nhttp://pjreddie.com/darknet/ , 2013\u20132016. 3\n[15] J. Redmon and A. Farhadi.",
        "ea86d536-b052-45f7-91a0-ff61f0272f9f": "Fu, and A. C. Berg. Ssd: Single shot multibox detector.\nInEuropean conference on computer vision , pages 21\u201337.\nSpringer, 2016. 3\n[12] I. Newton. Philosophiae naturalis principia mathematica .\nWilliam Dawson & Sons Ltd., London, 1687. 1\n[13] J. Parham, J. Crall, C. Stewart, T. Berger-Wolf, and\nD. Rubenstein. Animal population censusing at scale with\ncitizen science and photographic identi\ufb01cation. 2017. 4\n[14] J. Redmon. Darknet: Open source neural networks in c.\nhttp://pjreddie.com/darknet/ , 2013\u20132016. 3\n[15] J. Redmon and A. Farhadi. Yolo9000: Better, faster, stronger.\nInComputer Vision and Pattern Recognition (CVPR), 2017\nIEEE Conference on , pages 6517\u20136525. IEEE, 2017. 1, 2, 3\n[16] J. Redmon and A. Farhadi. Yolov3: An incremental improve-\nment. arXiv , 2018. 4\n[17] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-\nwards real-time object detection with region proposal net-\nworks. arXiv preprint arXiv:1506.01497 , 2015. 2[18] O. Russakovsky, L.-J. Li, and L. Fei-Fei. Best of both\nworlds: human-machine collaboration for object annotation.\nInProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 2121\u20132131, 2015. 4\n[19] M. Scott. Smart camera gimbal bot scanlime:027, Dec 2017.\n4\n[20] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be-\nyond skip connections: Top-down modulation for object de-\ntection. arXiv preprint arXiv:1612.06851 , 2016. 3\n[21] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. A. Alemi.\nInception-v4, inception-resnet and the impact of residual\nconnections on learning. 2017. 3",
        "305779f3-f006-4656-805c-536242a7c040": "0255075100\n0 50 100 150 200YOLOv3 All the other slow onesmAP 50\nExecution time (ms)0255075100\n0 12.5 25 37.5 50YOLOv3All the other slow ones\nFPSmAP 50Figure 4. Zero-axis charts are probably more intellectually honest... and we can still screw with the variables to make ourselves look good!\nRebuttal\nWe would like to thank the Reddit commenters, labmates,\nemailers, and passing shouts in the hallway for their lovely, heart-\nfelt words. If you, like me, are reviewing for ICCV then we know\nyou probably have 37 other papers you could be reading that you\u2019ll\ninvariably put off until the last week and then have some legend in\nthe \ufb01eld email you about how you really should \ufb01nish those re-\nviews execept it won\u2019t entirely be clear what they\u2019re saying and\nmaybe they\u2019re from the future? Anyway, this paper won\u2019t have be-\ncome what it will in time be without all the work your past selves\nwill have done also in the past but only a little bit further forward,\nnot like all the way until now forward. And if you tweeted about\nit I wouldn\u2019t know. Just sayin.\nReviewer #2 AKA Dan Grossman (lol blinding who does that)\ninsists that I point out here that our graphs have not one but two\nnon-zero origins. You\u2019re absolutely right Dan, that\u2019s because it\nlooks way better than admitting to ourselves that we\u2019re all just\nhere battling over 2-3% mAP. But here are the requested graphs.\nI threw in one with FPS too because we look just like super good\nwhen we plot on FPS.\nReviewer #4 AKA JudasAdventus on Reddit writes \u201cEntertain-\ning read but the arguments against the MSCOCO metrics seem a\nbit weak\u201d. Well, I always knew you would be the one to turn on\nme Judas. You know how when you work on a project and it only\ncomes out alright so you have to \ufb01gure out some way to justify\nhow what you did actually was pretty cool? I was basically trying\nto do that and I lashed out at the COCO metrics a little bit. But\nnow that I\u2019ve staked out this hill I may as well die on it.\nSee here\u2019s the thing, mAP is already sort of broken so an up-\ndate to it should maybe address some of the issues with it or at least\njustify why the updated version is better in some way. And that\u2019s\nthe big thing I took issue with was the lack of justi\ufb01cation. For\nPASCAL VOC, the IOU threshold was \u201dset deliberately low to ac-\ncount for inaccuracies in bounding boxes in the ground truth data\u201c\n[2]. Does COCO have better labelling than VOC? This is de\ufb01-\nnitely possible since COCO has segmentation masks maybe the\nlabels are more trustworthy and thus we aren\u2019t as worried about\ninaccuracy. But again, my problem was the lack of justi\ufb01cation.\nThe COCO metric emphasizes better bounding boxes but that\nemphasis must mean it de-emphasizes something else, in this case\nclassi\ufb01cation accuracy. Is there a good reason to think that moreprecise bounding boxes are more important than better classi\ufb01-\ncation? A miss-classi\ufb01ed example is much more obvious than a\nbounding box that is slightly shifted.\nmAP is already screwed up because all that matters is per-class\nrank ordering. For example, if your test set only has these two\nimages then according to mAP two detectors that produce these\nresults are JUST AS GOOD:\nPerson: 99%\nDog: 99%Camel: 99%Bird: 99%\nPerson: 99%\nHorse: 99%Detector #1\nHorse: 52%\nPerson: 42%\nDog: 48%Camel: 10%Bird: 90%\nPerson: 11%\nHorse: 70%Detector #2\nBird: 89%Horse: 60%\nBird: 75%Dog: 45%\nFigure 5. These two hypothetical detectors are perfect according to\nmAP over these two images. They are both perfect. Totally equal.",
        "dafcdc9d-8e92-4147-92bf-ceca55defaf1": "A miss-classi\ufb01ed example is much more obvious than a\nbounding box that is slightly shifted.\nmAP is already screwed up because all that matters is per-class\nrank ordering. For example, if your test set only has these two\nimages then according to mAP two detectors that produce these\nresults are JUST AS GOOD:\nPerson: 99%\nDog: 99%Camel: 99%Bird: 99%\nPerson: 99%\nHorse: 99%Detector #1\nHorse: 52%\nPerson: 42%\nDog: 48%Camel: 10%Bird: 90%\nPerson: 11%\nHorse: 70%Detector #2\nBird: 89%Horse: 60%\nBird: 75%Dog: 45%\nFigure 5. These two hypothetical detectors are perfect according to\nmAP over these two images. They are both perfect. Totally equal.\nNow this is OBVIOUSLY an over-exaggeration of the prob-\nlems with mAP but I guess my newly retconned point is that there\nare such obvious discrepancies between what people in the \u201creal\nworld\u201d would care about and our current metrics that I think if\nwe\u2019re going to come up with new metrics we should focus on\nthese discrepancies. Also, like, it\u2019s already mean average preci-\nsion, what do we even call the COCO metric, average mean aver-\nage precision?\nHere\u2019s a proposal, what people actually care about is given an\nimage and a detector, how well will the detector \ufb01nd and classify\nobjects in the image. What about getting rid of the per-class AP\nand just doing a global average precision? Or doing an AP calcu-\nlation per-image and averaging over that?\nBoxes are stupid anyway though, I\u2019m probably a true believer\nin masks except I can\u2019t get YOLO to learn them.",
        "5a60e651-325d-494f-8687-8f875871653c": "Practice of Epidemiology\nComparison of Random Forest and Parametric Imputation Models for Imputing\nMissing Data Using MICE: A CALIBER Study\nAnoop D. Shah *, Jonathan W. Bartlett, James Carpenter, Owen Nicholas, and Harry Hemingway\n*Correspondence to Dr. Anoop D. Shah, Clinical Epidemiology Group, Department of Epidemiology and Public Health, School of Life and Medical\nSciences, University College London, Wolfson House, 2-10 Stephenson Way, London NW1 2HE, United Kingdom (e-mail: anoop@doctors.org.uk).\nInitially submitted April 5, 2013; accepted for publication November 20, 2013.\nMultivariate imputation by chained equations (MICE) is commonly used for imputing missing data in epidemio-\nlogic research. The \u201ctrue \u201dimputation model may contain nonlinearities which are not included in default imputation\nmodels. Random forest imputation is a machine learning technique which can accommodate nonlinearities and in-teractions and does not require a particular regression model to be specified. We compared parametric MICE with arandom forest-based MICE algorithm in 2 simulation studies. The first study used 1,000 random samples of 2,000persons drawn from the 10,128 stable angina p atients in the CALIBER database (Cardiovascular Disease Re-\nsearch using Linked Bespoke Studies and Electronic Records; 2001 \u20132010) with complete data on all covariates.\nVariables were artificially made \u201cmissing at random, \u201dand the bias and efficiency of parameter estimates obtained\nusing different imputation methods were compared. Both MICE methods produced unbiased estimates of (log) haz-ard ratios, but random forest was more efficient and produced narrower confidence intervals. The second studyused simulated data in which the partially observed variable depended on the fully observed variables in a nonlinearway. Parameter estimates were less biased using random forest MICE, and confidence interval coverage was bet-ter. This suggests that random forest imputation may be useful for imputing complex epidemiologic data sets inwhich some patients have missing data.\nangina, stable; imputation; missing data; missingness at random; regression trees; simulation; survival\nAbbreviations: CALIBER, Cardiovascular Disease Research using Linked Bespoke Studies and Electronic Records; MAR, missing\nat random; MICE, multivariate imputation by chained equations.\nMissing data are a pervasive problem in epidemiologic\nstudies, particularly for rese arch using routinely collected\nclinical health records ( 1). Incomplete data sets are frequently\nanalyzed using multiple imputation, which involves creatingmultiple complete versions of the data with missing valuesimputed through random draws from distributions inferredfrom observed data ( 2). Multiple imputation typically as-\nsumes that data are missing at random (MAR) \u2014that is, that\nmissingness is not associated with the missing value, condi-tional on the observed data ( 3). Multivariate imputation by\nchained equations (MICE), also called full conditional spec-i\ufb01cation, is a common method of generating imputed values\nby drawing from estimated conditional distributions of eachvariable given all the others ( 4). Imputation models must be\nappropriately speci \ufb01ed for analyses based on imputed data toyield unbiased parameter estimates and associated standard\nerrors. The default setting in implementations of MICE isfor imputation models to include continuous variables as lin-ear terms only with no interactions, but omission of importantnonlinear terms may lead to biased results ( 5). Other potential\nproblems with parametric regression models are that 1) theycannot include more predictor variables than the number ofobservations without recourse to prior information ( 6)a n d\n2) inclusion of highly correlated variables may cause prob-lems due to collinearity. In this paper, we propose a new im-putation method which aims to overcome these problemsusing random forest.\nRandom forest is an extension of classi \ufb01cation and regres-\nsion trees ( 7), predictive models that recursively subdivide\nthe data based on values of the predictor variables. They do\n764 Am J Epidemiol. 2014;179(6):764 \u2013774\nAmerican Journal of Epidemiology\n\u00a9 The Author 2014. Published by Oxford University Press on behalf of the Johns Hopkins Bloomberg School of Public Health.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License\n(http://creativecommons.org/licenses/by-nc/3.0), which permits unrestricted reuse, distribution, and reproduction in any medium,\nprovided the original work is properly cited.Vol. 179, No.",
        "5204df8d-781f-4147-bc4d-58e825dc00b2": "In this paper, we propose a new im-putation method which aims to overcome these problemsusing random forest.\nRandom forest is an extension of classi \ufb01cation and regres-\nsion trees ( 7), predictive models that recursively subdivide\nthe data based on values of the predictor variables. They do\n764 Am J Epidemiol. 2014;179(6):764 \u2013774\nAmerican Journal of Epidemiology\n\u00a9 The Author 2014. Published by Oxford University Press on behalf of the Johns Hopkins Bloomberg School of Public Health.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License\n(http://creativecommons.org/licenses/by-nc/3.0), which permits unrestricted reuse, distribution, and reproduction in any medium,\nprovided the original work is properly cited.Vol. 179, No. 6\nDOI: 10.1093/aje/kwt312\nAdvance Access publication:\nJanuary 12, 2014Downloaded from https://academic.oup.com/aje/article/179/6/764/107562 by guest on 27 April 2024",
        "0dca5567-2953-4d3a-a880-e202c4178f49": "not rely on distributional assumptions and can accommodate\nnonlinear relations and interactions. On simulated data setswith interactions between variables, imputation of missingdata using MICE with regression trees resulted in less biasedparameter estimates than MICE with linear regression ( 7).\nHowever, regression trees may \u201cover\ufb01t,\u201dfollowing the pat-\ntern of noise too closely and producing a complex modelwith poor predictive power in new data sets.\nRandom forest uses bootstrap aggregation of multiple re-\ngression trees to reduce the risk of over \ufb01tting, and it com-\nbines the predictions from many trees to produce moreaccurate predictions ( 8,9). Random forest is widely used in\ngenetic epidemiology ( 10) and has also been used for mod-\neling survival ( 11,12) and predicting response to cancer che-\nmotherapy ( 13). We propose that random forest may be\nuseful in multiple imputation of epidemiologic data sets, par-ticularly if there are large numbers of clinical variables perparticipant, as may increasingly be the case (e.g., genomicor proteomic studies).\nStekhoven et al. ( 14) developed a random forest-based al-\ngorithm for missing data imputation called missForest. This\nalgorithm aims to predict individual missing valuesaccurately rather than take random draws from a distribu-tion, so the imputed values may lead to biased parameter es-timates in statistical models. Apart from a comparisonbetween random forest and polytomous regression for imput-ing tumor stage using MICE ( 15), we are not aware of other\npublished evaluations of multiple imputation using randomforest.\nIn this paper, we compare a standard implementation of\nMICE with imputation using missForest, and we propose anew version of MICE which imputes each variable using ran-dom forest. We compare these methods in a realistically com-plex survival analysis based on patients with stable angina inthe CALIBER (Cardiovascular Disease Research usingLinked Bespoke Studies and E lectronic Records) database\n(16) and in a simulation study with interactions.\nMETHODS\nImputation of missing data using MICE, where each\nvariable is imputed using random forest\nWithin the MICE framework, missing values of continu-\nous variables are conventionally imputed by \ufb01tting a linear\nregression model for the observed values, predicting the con-ditional mean for each missing value, and randomly imputinga value from a normal distrib ution centered on this condi-\ntional mean. Our new method is derived from the \u201cmice.\nimpute.norm.boot \u201dfunction in the \u201cmice\u201dpackage in R ( 4), in\nwhich linear regression is applied to a bootstrap sample of rec-ords with observed values of the variable to be imputed. The\npurpose of the bootstrap is to accommodate sampling varia-\ntion in estimating population regression parameters, which ispart of ensuring that imputations are \u201cproper \u201d(3). The ran-\ndom forest algorithm itself involves another level of bootstrapsampling. Records with missing values in the dependent var-iable are imputed by random draws from independent normaldistributions centered on conditional means predicted usingrandom forest. We used the \u201cout-of-bag \u201dmean square erroras the estimator of residual variance (which we assumed to\nbe normally distributed). Random forest \ufb01ts each tree to a dif-\nferent bootstrap sample of the data and aggregates the results;the out-of-bag error is the mean of squared differences be-tween each observed value and the prediction based ontrees for which that observation is not included in the boot-strap sample.\nFor binary or unordered categorical variables, we used ran-\ndom forest to \ufb01t individual regression trees to a bootstrap\nsample of the data and imputed each missing value as the pre-diction of a randomly chosen tree. This is equivalent tochoosing between 0 and 1 with probability according to themean random forest prediction. Our random forest imputationfunctions are available from the Comprehensive R ArchiveNetwork ( 17).\nSimulation study based on CALIBER data\nCALIBER is a database of linked routinely collected elec-\ntronic health records from England ( 16), comprising data\nfrom primary care (Clinical Practice Research Datalink)(18), hospital admissions ( 19), the national registry of acute\ncoronary syndromes ( 20), and the national death registry.\nThe cohort consisted of patients who received a diagnosis\nof stable angina while registered at a general practice contrib-uting to the Clinical Practice Research Datalink.",
        "c3894a95-e0fa-466a-8ba5-b377a0b7bfea": "For binary or unordered categorical variables, we used ran-\ndom forest to \ufb01t individual regression trees to a bootstrap\nsample of the data and imputed each missing value as the pre-diction of a randomly chosen tree. This is equivalent tochoosing between 0 and 1 with probability according to themean random forest prediction. Our random forest imputationfunctions are available from the Comprehensive R ArchiveNetwork ( 17).\nSimulation study based on CALIBER data\nCALIBER is a database of linked routinely collected elec-\ntronic health records from England ( 16), comprising data\nfrom primary care (Clinical Practice Research Datalink)(18), hospital admissions ( 19), the national registry of acute\ncoronary syndromes ( 20), and the national death registry.\nThe cohort consisted of patients who received a diagnosis\nof stable angina while registered at a general practice contrib-uting to the Clinical Practice Research Datalink. Blood pres-sure, smoking status, and measurements of blood biomarkerswere taken from routine clinical records before the diagnosisof stable angina, and patients were followed up for the com-posite endpoint of death or nonfatal myocardial infarction(see Web Appendix 1, available at http://aje.oxfordjournals.\norg/). The CALIBER record-linkage study has received eth-\nical approval, and this study was approved by the ClinicalPractice Research Datalink Independent Scienti \ufb01c Advisory\nCommittee.\nAnalysis of interest. We investigated missing data in the\ncontext of a hypothetical analysis of associations, suggestedin previous studies ( 21\u201323), between 3 commonly measured\nhematological parameters (hemoglobin concentration, lym-phocyte count, and neutrophil count) and prognosis amongpatients with stable angina in CALIBER. The substantiveanalysis was a multivariable Cox model with the compositeendpoint of death or nonfatal myocardial infarction, andwith predictor variables speci \ufb01ed a priori .The fully observed\npredictor variables were: age, age squared, sex, previousmyocardial infarction, diabetes mellitus, previous stroke,peripheral arterial disease, and heart failure. Smoking statuswas a partially observed 3-category variable (never, former,or current smoker), and we included the following partiallyobserved continuous variables: systolic blood pressure(mm Hg), log neutrophil count (10\n9cells/L), log lymphocyte\ncount (109cells/L), high-density lipoprotein cholesterol level\n(mmol/L), hemoglobin concentration (g/dL), and log serum\ncreatinine concentration (\u00b5mol/L). For each of these vari-ables, we took the mean of any observed values in the 2years prior to the start of follow-up. We used the Efron ap-proximation ( 24) for ties. We did not investigate alternative\nmodels and ignored clustering by general practice.\nGeneration of sample data sets for simulation study. For\nthe simulation study, we created data sets with missing dataMultiple Imputation by Random Forest 765\nAm J Epidemiol. 2014;179(6):764 \u2013774Downloaded from https://academic.oup.com/aje/article/179/6/764/107562 by guest on 27 April 2024",
        "2c6ac779-e678-4c8b-b4c6-5d0707b65f21": "for which we knew the \u201ctrue\u201dvalues, with a missingness pat-\ntern similar to that observed in the actual data set but whichwas missing at random, such that the MAR assumption un-derlying most multiple imputation approaches was satis \ufb01ed\n(Figure 1). We denoted the entire cohort of 52,576 stable an-\ngina patients data set \u201cA.\u201dPatients with no missing values for\nany of the variables in the survival model were denoted dataset\u201cB\u201d(13,308 patients).\nWe used logistic regression based on completely observed\nvariables to investigate factors associated with a patient \u2019shaving a complete record (i.e., whether a patient was in\ndata set B). We included the \ufb01rst value after cohort entry of\npartially observed continuous variables as auxiliary variablesto help predict missing covariates in imputation models. Thesubset of patients with complete recording of all analysis andauxiliary variables was denoted data set \u201cC\u201d(10,128 pa-\ntients), and they formed the basis for the resampling study.We arti \ufb01cially made some values of predictor variables miss-\ning in one thousand 2,000-patient random samples (with re-placement) from data set C. We carried out simulations with\nFigure 1. Generation of data sets with artificial missingness from a population of patients with stable angina in the CALIBER database, 2001 \u2013\n2010. Data sets D1, D2, . . . , D1,000 are samples of 2,000 patients with replacement from data set C. CALIBER, Cardiovascular Disease Researchusing Linked Bespoke Studies and Electronic Records; MAR, missing at random; MCAR, missing completely at random; MICE, multivariate impu-\ntation by chained equations.766 Shah et al.\nAm J Epidemiol. 2014;179(6):764 \u2013774Downloaded from https://academic.oup.com/aje/article/179/6/764/107562 by guest on 27 April 2024",
        "ffac8e9a-c9c5-419d-b143-0f08ea400706": "one of 2 missingness mechanisms: 1) MAR in a pattern similar\nto that of data set A or 2) an arti \ufb01cial pattern of missingness\ncompletely at random in which only categorical variableswere missing (see Web Appendix 1 for more details).\nMultiple imputation of test data sets. Imputation models\nincluded all of the variables in the substantive Cox model,event status, marginal Nelson-Aalen cumulative hazard(25), and the following auxiliary variables: type of endpoint,\nwhether the practice was receiving electronic laboratoryresults, and the earliest recorded value after the index datefor blood pressure and the 5 blood biomarkers. We imputedcontinuous variables using MICE with normal-based linearregression, predictive mean matching with 3 nearest neigh-bors, and our new random forest method. We imputed categor-ical variables using either MICE with logistic or polytomousregression or MICE with random forest (choice of 10 or 100trees). We also investigated random forest with a single tree todetermine whether bootstrap aggregation had an advantageover a single regression tree.\nWe generated 10 MICE imputations, each drawn from a\nseparate chain with a different random seed, with 10 cycles\nof imputation before drawing the imputed data set. We as-sessed chain mixing by reviewing plots of chain mean valuesand standard deviations. For two of the methods (random for-est with 10 trees and parametric MICE), we also calculatedresults using 100 imputations.\nIn addition to MICE, we also evaluated missForest ( 14),\nwhich uses random forest in an iterative way to complete adata set with missing values, where imputed values areequal to the random forest predictions rather than being ran-domly sampled from a conditional distribution. We generatedmultiple imputed data sets by running missForest using dif-ferent random seeds, which leads to different random forestmodels being generated.\nRegardless of the imputation method, all data sets were an-\nalyzed using the same multivariable semiparametric Coxmodel as described above. For each set of imputed datasets, the log hazard ratios from the Cox model were combinedusing Rubin \u2019s rules ( 26), which assumes that imputed values\nwere drawn from the appropriate Bayesian posterior. We car-ried out analyses using R 2.12.1 ( 27), with the software pack-\nages mice 2.12 ( 4), missForest 1.3 ( 28), survival 2.36-2 ( 29),\nand randomForest 4.6-6 ( 30). Random numbers were gener-\nated using the Mersenne Twister ( 31).\nComparison of results obtained by different methods. We\nconsidered the Cox proportional hazards model \ufb01tted to the\nentire data set (data set C) the \u201ctrue\u201dresult for the assess-\nment of bias and con \ufb01dence interval coverage of hazard ra-\ntios. We compared the widths of 95% con \ufb01dence intervals\nbetween 2 methods using paired-sample ttests. We com-\npared coverage of 95% con \ufb01dence intervals for each coef \ufb01-\ncient separately using McNemar \u2019s test, de \ufb01ning discordant\npairs as data sets in which the 95% con \ufb01dence interval in-\ncluded the \u201ctrue\u201dvalue for one method but not the other.\nWe compared the ef \ufb01ciency of the estimators by calculat-\ning their empirical standard deviations. We calculated thebetween-imputation variance of the estimated log hazard ra-tios, de \ufb01ned as the mean (across simulations) of the variance\nof the log hazard ratio estimates from the 10 imputations perdata set.Simulation study with interactions\nWe also created simulated data sets to compare the per-\nformance of methods when there were nonlinearities in theassociation between predictor variables. We generated 2independent random normal variables with mean 0 andvariance 1, x\n1andx2, and a third variable x3equal to 0.5( x1+\nx2\u2212x1x2)+e,w h e r e ewas distributed normally with mean\n0 and variance 1. Survival times were generated accordingto an exponential distribution with log hazard 0.5( x\n1+x2+\nx3). This meant that there were no interactions in the substan-\ntive model, but the default parametric imputation model forx\n3(which would not include any interactions) would be in-\ncorrect.",
        "9c84a534-9351-46a4-b869-34d44ef18de7": "We generated 2independent random normal variables with mean 0 andvariance 1, x\n1andx2, and a third variable x3equal to 0.5( x1+\nx2\u2212x1x2)+e,w h e r e ewas distributed normally with mean\n0 and variance 1. Survival times were generated accordingto an exponential distribution with log hazard 0.5( x\n1+x2+\nx3). This meant that there were no interactions in the substan-\ntive model, but the default parametric imputation model forx\n3(which would not include any interactions) would be in-\ncorrect. Observation times were generated according to a uni-form distribution in the range from 0 to the 50th percentileof survival times. If the observation time was less than thesurvival time, the patient was considered censored (eventindicator 0, and the patient \u2019s follow-up ended on his or her\ncensoring date); otherwise the event indicator was 1, withfollow-up ending on the date of the event.\nVariable x\n3was made 20% MAR according to a logistic\nmodel based on x1andx2, the marginal Nelson-Aalen cumu-\nlative hazard and the event indicator.\nWe analyzed 1,000 simulated data sets with 2,000 patients\neach, imputing missing data using random forest and para-metric MICE (without interactions), comparing the resultsas above (Web Appendices 2 and 3).\nRESULTS\nSimulation study based on CALIBER data\nThe prevalence of missingness among partially observed\nvariables ranged from 1.5% for smoking to 56.7% for lympho-cyte counts and 56.8% for neutrophil counts (Web Table 1).Patients with missing data were more likely to experiencethe primary endpoint of death or nonfatal myocardial infarc-tion (age- and sex-adjusted hazard ratio = 1.19, 95% con \ufb01-\ndence interval: 1.13, 1.25) (Figure 2). Patients with missing\ndata also tended to have longer follow-up because they enteredthe cohort earlier (a median index date of March 1, 2002, vs.December 6, 2005; P< 0.0001 by Wilcoxon rank-sum test).\nThe logistic regression model showed that patients with diabe-tes, peripheral arterial disease, and previous stroke were morelikely to have complete records (Table 1) .W e bT a b l e2s h o w s\nthat coef \ufb01cients from Cox models for patients with complete\ndata (data set C) were similar to the average results from fulldata analysis of the subsamples (as we would expect in the ab-sence of small-sample bias), so we compared imputation esti-mates with those from data set C.\nWe found very little difference between results obtained\nusing 10 MICE imputations and those obtained using 100\nMICE imputations (Web Tables 3 and 4), so all results are\nbased on 10 imputations unless stated otherwise.\nBias. Estimates from complete-record analysis were\nbiased for some parameters under MAR (missingnessmechanism 1); this may be expected because we had intro-duced missingness dependent on the outcome (Table 2,\nWeb Table 3, Web Figure 1). For example, the geometricmean hazard ratio per doubling of lymphocyte count wasMultiple Imputation by Random Forest 767\nAm J Epidemiol. 2014;179(6):764 \u2013774Downloaded from https://academic.oup.com/aje/article/179/6/764/107562 by guest on 27 April 2024",
        "7a3c54f0-e4e7-45e8-9e62-ba132e22cf07": "0.738 from complete-record analysis but 0.799 from full-data\nanalysis. There was no material bias with parametric MICE(mean hazard ratio = 0.806) or our random forest MICEmethod with 10 trees ( \u201cMICE RF 10 \u201d; mean hazard ratio =\n0.807). The random forest MICE estimate for smoking (cat-egorical) was biased towards the null (Table 3, Web Figure 2),\nbut there was no material bias in other parameters estimatedby random forest or parametric MICE (Table 2, Web Tables 3\nand 4, Web Figure 3). However, imputation using single-treerandom forest MICE ( \u201cMICE Tree \u201d) or missForest produced\nmaterially biased estimates for all continuous variables miss-ing at random (Figure 3, Table 2, Web Figure 1, Web Table 3).\nEfficiency. All of the imputation methods tested produced\nmore ef \ufb01cient parameter estimates than complete-record anal-\nysis. MICE with random forest produced slightly more ef \ufb01cient\nestimates than parametric MICE, and the average between-imputation variance was also lower (Tables 2and3,W e b\nTables 3 \u20136).\nConfidence intervals. Parametric MICE yielded con \ufb01-\ndence intervals with approximately 93% \u201395% coverage. The\nmean widths of con \ufb01dence intervals were lower using random\nforest MICE than using parametric MICE ( P< 0.001 for each\ncomparison), but coverage was either equal or greater using\nrandom forest MICE (Tables 2and3, Web Tables 3 \u20136).\nFor categorical variables, missForest produced imputed\nvalues which were more likely to be equal to the \u201ctrue\u201d(ob-\nserved) value than the MICE methods, but con \ufb01dence inter-\nvals were too small with below nominal coverage, andbetween-imputation variance was very small. There was nodifference in bias, precision, or coverage between normal-based MICE and predictive mean matching (Web Table 5).Random forest MICE with 100 trees for continuous variablesproduced estimates with slightly narrower con \ufb01dence inter-\nvals than random forest MICE with 10 trees (Web Table 5),but with greater bias, worse coverage of 95% con \ufb01dence in-\ntervals, and 10 times the computational cost. For categoricalvariables, random forest MICE with 10 trees and random for-est MICE with 100 trees produced almost identical results.\nSimulation study with interactions\nThe coef \ufb01cient estimate for the partially observed variable\n(x3) was 10% biased using parametric MICE, 2.6% biased\nusing random forest with 100 trees, and only 1.0% biased\nusing random forest with 10 trees ( P< 0.001 for 2-way com-\nparisons). The bias in the x3coef\ufb01cient varied with the num-\nber of trees, with 10 or 20 trees giving minimal bias (Table 4).\nRandom forest MICE produced narrower 95% con \ufb01dence in-\ntervals for the x3coef\ufb01cient than parametric MICE ( P<\n0.001), and coverage was only 80% using parametric MICEas compared with 95% using random forest MICE with 5 \u2013\n100 trees. Further details are given in Web Appendix 2.\nDISCUSSION\nSummary of main findings\nIn this resampling study of methods for handling missing\ndata, parametric and random forest MICE produced estimateswith no material bias for a Cox model on data with arti \ufb01cially\nintroduced MAR missingness. Random forest-based MICEproduced more ef \ufb01cient estimates and narrower con \ufb01dence\nintervals than parametric MICE, yet in some cases coverageprobability was greater than 95%, suggesting that someTime, years012345 60.10.20.30.4\n0Not data set B\nData set B\nFigure 2. Cumulative incidence of myocardial infarction or death\n(Kaplan-Meier failure curve) for patients with stable angina in the\nCALIBER database, by complete record status, 2001 \u20132010.",
        "5de200b0-8ce9-47b1-8e9b-9467bf6453a7": "Further details are given in Web Appendix 2.\nDISCUSSION\nSummary of main findings\nIn this resampling study of methods for handling missing\ndata, parametric and random forest MICE produced estimateswith no material bias for a Cox model on data with arti \ufb01cially\nintroduced MAR missingness. Random forest-based MICEproduced more ef \ufb01cient estimates and narrower con \ufb01dence\nintervals than parametric MICE, yet in some cases coverageprobability was greater than 95%, suggesting that someTime, years012345 60.10.20.30.4\n0Not data set B\nData set B\nFigure 2. Cumulative incidence of myocardial infarction or death\n(Kaplan-Meier failure curve) for patients with stable angina in the\nCALIBER database, by complete record status, 2001 \u20132010. The\nsolid line represents patients in data set A but not data set B (thosewith missing data; n= 39,268 at the start, dropping to 17,588 in year\n6), and the dashed line represents patients in data set B (those with\ncomplete records; n= 13,308 at the start, dropping to 2,594 in year\n6). CALIBER, Cardiovascular Disease Research using LinkedBespoke Studies and Electronic Records.\nTable 1. Factors Associated With Having a Complete Record in a\nStudy of Patients Diagnosed With Stable Angina (Logistic RegressionModel), CALIBER Database, 2001 \u20132010\nVariableOdds\nRatio95% Confidence\nIntervalPValue\nAge, per 10 years 4.81 4.02, 5.75 <0.001\nAge squared, per\n10 years squared0.89 0.87, 0.90 <0.001\nFemale sex 1.08 1.03, 1.12 0.002\nDiabetes mellitus 1.74 1.64, 1.84 <0.001\nPeripheral arterial\ndisease1.24 1.15, 1.35 <0.001\nPrevious stroke 1.26 1.17, 1.36 <0.001\nHeart failure 0.96 0.89, 1.04 0.333\nPrevious myocardial\ninfarction1.03 0.98, 1.09 0.228\nElectronic laboratory\nresultsa4.74 4.48, 5.01 <0.001\nEndpoint of fatal coronary\nheart disease0.40 0.36, 0.45 <0.001\nEndpoint of nonfatal\nmyocardial infarction0.29 0.26, 0.34 <0.001\nEndpoint of noncoronary\ndeath0.42 0.39, 0.45 <0.001\nCumulative hazard 0.03 0.02, 0.03 <0.001\nAbbreviation: CALIBER, Cardiovascular Disease Research using\nLinked Bespoke Studies and Electronic Records.\naWhether a medical practice was receiving electronic laboratory\nresults.768 Shah et al.\nAm J Epidemiol. 2014;179(6):764 \u2013774Downloaded from https://academic.oup.com/aje/article/179/6/764/107562 by guest on 27 April 2024",
        "5cc55ac9-91aa-464e-8fb9-482189330381": "con\ufb01dence intervals may be conservative. A possible expla-\nnation for the ef \ufb01ciency gain with random forest MICE is that\nit was able to make better use of the available information byaccommodating nonlinearities among the predictors. In sim-ulations with an interaction among the predictor variables butnot in the substantive model, random forest MICE was lessbiased than parametric MICE, which omitted the interaction.Using missForest for multiple imputation resulted in very bi-ased estimates and poor coverage of con \ufb01dence intervals.\nOverall, our results suggest that random forest imputationmay be useful for imputing complex epidemiologic datasets in which some patients have missing data.\nImputation methods for MICE\nIt is important that imputation models be correctly speci-\n\ufb01ed for analyses to yield unbiased estimates, and randomforest may help avoid the bias that can occur with parametric\nMICE if the latter \u2019s imputation models are misspeci \ufb01ed. In\nour main study, standard parametric MICE performed well,suggesting that the true imputation models did not containsigni\ufb01cant nonlinearities or interactions, and hence random\nforest did not confer an advantage from the perspective ofbias. However, the simulated data sets had interactionswhich were not included in the parametric MICE imputationmodels, and in this setting random forest MICE outper-formed parametric MICE (Web Appendix 2). The default set-tings for MICE do not include interactions between thevariables, and it is routine practice to include only those in-teractions that are in the substantive model, rather than ac-tively search for all possible interactions and nonlinearities.This shows the importance of checking to be sure that theimputation models are reasonably well speci \ufb01ed. Random\nforest reduces the need to investigate associations betweenTable 2.",
        "9bc1f2b1-4183-48db-bbbc-b21a1000753e": "In\nour main study, standard parametric MICE performed well,suggesting that the true imputation models did not containsigni\ufb01cant nonlinearities or interactions, and hence random\nforest did not confer an advantage from the perspective ofbias. However, the simulated data sets had interactionswhich were not included in the parametric MICE imputationmodels, and in this setting random forest MICE outper-formed parametric MICE (Web Appendix 2). The default set-tings for MICE do not include interactions between thevariables, and it is routine practice to include only those in-teractions that are in the substantive model, rather than ac-tively search for all possible interactions and nonlinearities.This shows the importance of checking to be sure that theimputation models are reasonably well speci \ufb01ed. Random\nforest reduces the need to investigate associations betweenTable 2. Comparisons Between Methods of Handling Missing Data in 1,000 Samples With Continuous Variables\nMissing at Random in a Pattern Similar to That of the Original Data Set (Missingness Mechanism 1), CALIBER\nDatabase, 2001 \u20132010\nVariable and MethodBiasa\nof Log\nHRzScore\nfor\nBiasbSD of\nEstimated\nLog HRMean\nLength of\n95% CICoverage\nof 95% CI,\n%Between-\nImputation\nVariance\nNeutrophils (109cells/L),\nper doubling\nFull data 0.002 0.43 0.158 0.564 92.2\nComplete recordc\u22120.045 \u22122.67 0.533 1.677 90.1\nMICE normal \u22120.038 \u22125.15 0.232 0.883 93.4 0.0243\nMICE PMM \u22120.042 \u22125.68 0.230 0.889 93.4 0.0245\nmissForest \u22120.266 27.72 0.303 0.781 63.2 0.0014\nMICE RF 10 trees \u22120.024 \u22124.55 0.165 0.798 97.9 0.0143\nLymphocytes (109cells/L),\nper doubling\nFull data \u22120.007 \u22121.23 0.155 0.526 91.6\nComplete recordc\u22120.087 \u22125.87 0.464 1.544 89.8\nMICE normal 0.001 0.13 0.202 0.759 93.2 0.0157\nMICE PMM 0.006 0.99 0.205 0.768 92.4 0.0162\nmissForest \u22120.190 \u221222.21 0.270 0.724 72.5 0.0011\nMICE RF 10 trees 0.003 0.56 0.156 0.727 97.8 0.0109\nHemoglobin, per g/dL\nFull data \u22120.004 \u22121.99 0.057 0.202 91.6\nComplete recordc\u22120.022 \u22123.91 0.180 0.593 90.8\nMICE normal \u22120.007 \u22122.73 0.076 0.279 92.6 0.0019\nMICE PMM \u22120.004 \u22121.47 0.077 0.279 92.7 0.0019\nmissForest \u22120.056 \u221219.96 0.089 0.255 77.3 0.0001\nMICE RF 10 trees \u22120.010 \u22125.61 0.059 0.261 97.2 0.0012\nAbbreviations: CALIBER, Cardiovascular Disease Research using Linked Bespoke Studies and Electronic\nRecords; CI, confidence interval; HR, hazard ratio; MICE, multivariate imputation by chained equations; PMM,\npredictive mean matching; RF 10 trees, random forest with 10 trees; SD, standard deviation.\naBias was measured relative to estimates from analysis of the full data set (data set C) (Web Table 2).\nbThe zscore is defined as the mean bias of the estimate divided by the empirical standard error from simulations,\nand it should lie approximately within the interval ( \u22122, +2).\ncResults for complete records were based on the 986 samples for which it was possible to estimate hazard ratios\nfor all parameters.Multiple Imputation by Random Forest 769\nAm J Epidemiol.",
        "11f36c35-c274-4133-991b-07b67b55356f": "aBias was measured relative to estimates from analysis of the full data set (data set C) (Web Table 2).\nbThe zscore is defined as the mean bias of the estimate divided by the empirical standard error from simulations,\nand it should lie approximately within the interval ( \u22122, +2).\ncResults for complete records were based on the 986 samples for which it was possible to estimate hazard ratios\nfor all parameters.Multiple Imputation by Random Forest 769\nAm J Epidemiol. 2014;179(6):764 \u2013774Downloaded from https://academic.oup.com/aje/article/179/6/764/107562 by guest on 27 April 2024",
        "9c53e5a7-21a5-42f7-8cc4-3e836a7afb32": "predictor variables, because it should automatically accom-\nmodate nonlinearities and interactions. Imputation modelsshould also be compatible with the substantive model ( 32),\nand random forest obviates the need to specify how theoutcome should be conditioned on in the imputation models\nfor covariates.\nWhen using random forest for prediction, a larger number\nof trees is preferred in order to obtain precise predictions ( 30).Table 3. Comparisons Between Methods of Handling Missing Data in 1,000 Samples With Categorical Variables\nMissing Completely at Random (Missingness Mechanism 2), CALIBER Database, 2001 \u20132010\nVariable and MethodBiasa\nof Log\nHRzScore\nfor\nBiasbSD of\nEstimated\nLog HRMean\nLength of\n95% CICoverage\nof 95% CI,\n%% Falsely\nClassifiedc\nPrevious myocardial infarction\nFull data 0.006 1.22 0.154 0.587 94.2 0MICE logistic \u22120.013 \u22122.46 0.168 0.682 95.5 29.6\nmissForest 0.002 0.27 0.179 0.625 91.8 17.3\nMICE RF 10 trees \u22120.020 \u22124.21 0.149 0.662 97.3 28.5\nDiabetes mellitus\nFull data 0.010 2.30 0.156 0.592 93.7 0\nMICE logistic 0.016 3.21 0.171 0.685 95.7 32.0missForest 0.014 2.73 0.182 0.627 90.8 19.7MICE RF 10 trees \u22120.021 \u22124.25 0.149 0.668 97.5 30.7\nPrevious stroke\nFull data 0.005 0.86 0.198 0.707 94.0 0MICE logistic \u22120.005 \u22120.58 0.207 0.828 95.5 17.9\nmissForest 0.004 0.65 0.211 0.763 92.9 8.4\nMICE RF 10 trees \u22120.011 \u22121.79 0.183 0.808 97.9 16.7\nPeripheral arterial disease\nFull data 0.016 2.59 0.199 0.730 93.6 0\nMICE logistic \u22120.002 \u22120.21 0.218 0.858 94.8 15.5\nmissForest 0.028 4.18 0.223 0.788 91.9 7.0MICE RF 10 trees 0.005 0.94 0.192 0.834 97.1 14.5\nHeart failure\nFull data 0.015 2.47 0.191 0.653 91.7 0MICE logistic 0.015 2.22 0.207 0.759 93.8 14.6\nmissForest 0.001 0.08 0.216 0.696 89.4 7.2\nMICE RF 10 trees \u22120.034 \u22125.78 0.190 0.746 95.5 13.7\nSmoking status: current vs. never\nFull data 0.019 2.62 0.264 0.969 93.9 0\nMICE logistic 0.023 2.65 0.292 1.092 94.0 52.4missForest \u22120.036 \u22123.56 0.308 1.062 91.5 35.0\nMICE RF 10 trees \u22120.098 \u221212.92 0.237 1.072 95.5 50.0\nSmoking status: former vs.",
        "cd3eecba-8979-4ee5-9a1c-28c0ca8b154c": "never\nFull data 0.019 2.62 0.264 0.969 93.9 0\nMICE logistic 0.023 2.65 0.292 1.092 94.0 52.4missForest \u22120.036 \u22123.56 0.308 1.062 91.5 35.0\nMICE RF 10 trees \u22120.098 \u221212.92 0.237 1.072 95.5 50.0\nSmoking status: former vs. never\nFull data 0.011 1.66 0.247 0.908 93.6 0MICE logistic \u22120.008 \u22120.82 0.266 1.022 94.1 52.4\nmissForest 0.045 5.34 0.270 0.980 93.2 35.0\nMICE RF 10 trees \u22120.060 \u22128.81 0.212 1.000 97.1 50.0\nAbbreviations: CALIBER, Cardiovascular Disease Research using Linked Bespoke Studies and Electronic\nRecords; CI, confidence interval; HR, hazard ratio; MIC E, multivariate imputation by chained equations; RF 10\ntrees, random forest with 10 trees; SD, standard deviation.\naBias was measured relative to estimates from analysis of the full data set (data set C) (Web Table 2).\nbThe zscore is defined as the mean bias of the estimate divided by the empirical standard error from simulations,\nand it should lie approximately within the interval ( \u22122, +2).\ncPercentage of imputed values that were different from the \u201ctrue \u201d(observed) missing value.770 Shah et al.\nAm J Epidemiol. 2014;179(6):764 \u2013774Downloaded from https://academic.oup.com/aje/article/179/6/764/107562 by guest on 27 April 2024",
        "a7bb6a81-ad47-417c-921e-4421bc2a30ee": "However, when imputing continuous variables using random\nforest MICE, bias seemed to tend towards a nonzero limit asthe number of trees increased, with 10 or 20 trees giving min-imal bias (Table 4). It is possible that the relationship between\nthe number of trees and bias may have the same functionalform but with a different direction of bias, asymptotic limit,and optimal number of trees, depending on the data. This\nphenomenon warrants further investigation.\nA disadvantage of random forest is that the \u201cmodels \u201dare\ncomplex and not easily interpretable, although arguablythis is not a shortcoming for the purpose of imputation. An-other disadvantage is that random forest can be biased inA)\nB)\nC)\nFigure 3. Bias in estimates of log hazard ratios for partially observed variables with data missing at random (missingness mechanism 1) in 1,000\nsamples of patients with stable angina in the CALIBER database, 2001 \u20132010. A) log neutrophil count (109cells/L); B) log lymphocyte count (109\ncells/L); C) hemoglobin concentration (g/dL). The solid horizontal line is the \u201ctrue \u201dlog hazard ratio from the full data set (data set C); the dashed lines\nshow \u00b11 empirical standard error. The boxes span the interquartile range (25th \u201375th percentiles), and the whiskers extend to the most extreme data\npoint, which is no more than 1.5 times the interquartile range from the box. Circles represent outliers. The light gray boxes show results from sim-ulations with 50% complete records, and the dark gray boxes show results from simulations with 25% complete records. CALIBER, Cardiovascular\nDisease Research using Linked Bespoke Studies and Electronic Records; MICE, multivariate imputation by chained equations; PMM, predictive\nmean matching; RF, random forest.Multiple Imputation by Random Forest 771\nAm J Epidemiol. 2014;179(6):764 \u2013774Downloaded from https://academic.oup.com/aje/article/179/6/764/107562 by guest on 27 April 2024",
        "ed7a9951-cc93-4dbd-b728-4bc43837ead2": "some situations, due to random forest predictions of continu-\nous variables at the extremes of their range being biased to-wards less extreme values ( 33). This is because a random\nforest prediction effectively consists of a weighted averageof observed values of the variable being predicted; unlikemodel-based prediction, it is unable to extrapolate beyondobserved values. In a simulation study, we found that randomforest imputation led to bias when the distribution of missingvalues was very different from that for observed values ( 17),\nalthough in such situations any kind of imputation may pro-duce poor results. However, we did not \ufb01nd such a bias in our\nCALIBER study because missing and observed values hadsimilar distributions. Another limitation of our random forest\nMICE method is the assumption that the residuals from the\nrandom forest regression are normally distributed with con-stant variance.\nOn these 2,000-patient data sets, computation time was 3\ntimes as long for random forest MICE with 10 trees as forparametric MICE (137 seconds per data set vs. 48 secondsper data set on a computer with an Intel Xeon 3.47-GHz pro-cessor (Intel Corporation, Santa Clara, California)), but on a10,000-patient data set, random forest took 6.5 times as long.However, random forest may yield a saving in analyst timebecause there is theoretically less need for transformationof fully observed variables or investigation of nonlinearitiesand interactions. It is also possible to include a large numberof related predictor variables in random forest models withoutencountering problems due to collinearity.\nWe included missForest and rfImpute in our study as ex-\namples of algorithms for completing single data sets ( 14).\nThey replace missing values with predicted values ratherthan draw from a distribution, such that the imputed valuesdo not have the correct joint distribution, leading to biasedparameter estimates. Better predictions do not mean bettercoverage of con \ufb01dence intervals; it is important that imputa-\ntion methods incorporate the correct amount of variation inorder to produce unbiased estimates with correct coverageof con \ufb01dence intervals ( 34).There was no difference in the results between linear re-\ngression and predictive mean matching. This was probablybecause the partially observed continuous variables in ourdata were approximately norma lly distributed; predictive\nmean matching may be preferred for variables that are not(conditionally) normally distributed ( 35).\nLimitations\nAlthough this study had strengths (it was based on real\ndata, and the analysis was realistically complex), it also hadimportant limitations. The most important limitation in pro-ducing general recommendations is that it was based on a sin-gle analysis of a single study, so results should be generalizedto other data sets with caution.\nA limitation of our resampling met hodology was that in\norder to avoid excessive computing time we used only 10 im-putations for most of the comparisons, leading to noisy esti-mates of between-imputation variability. To save time, wealso restricted the number of cycles of MICE to 10, and al-though we evaluated plots of chain means and standard devi-\nations between cycles for a few runs, this is a crude way of\nassessing chain convergence; it is possible that the chainsmay not have converged by the end of every run.\nWe ignored practice-level clustering at the imputation and\nanalysis stages, for simplicity. If patients from the same prac-tice are more similar than patients from different practices,the variance of parameter estimates might be underestimated,and parameter estimates may also be biased. This could beproperly accounted for by using hierarchical models for anal-ysis and imputation ( 36).\nRecommendations for further development\nWe consider random forest multiple imputation to be\npromising, but it should be tested on a larger range of datasets and in simulations to explore whether it gives unbiasedestimates where there are nontrivial nonlinearities orTable 4.",
        "7263006d-e2fb-47a3-b8a9-4521c3783b99": "To save time, wealso restricted the number of cycles of MICE to 10, and al-though we evaluated plots of chain means and standard devi-\nations between cycles for a few runs, this is a crude way of\nassessing chain convergence; it is possible that the chainsmay not have converged by the end of every run.\nWe ignored practice-level clustering at the imputation and\nanalysis stages, for simplicity. If patients from the same prac-tice are more similar than patients from different practices,the variance of parameter estimates might be underestimated,and parameter estimates may also be biased. This could beproperly accounted for by using hierarchical models for anal-ysis and imputation ( 36).\nRecommendations for further development\nWe consider random forest multiple imputation to be\npromising, but it should be tested on a larger range of datasets and in simulations to explore whether it gives unbiasedestimates where there are nontrivial nonlinearities orTable 4. Comparisons Between Methods of Handling Missing Data in a Survival Analysis of 1,000 Simulated Data\nSets With a Predictor Variable Missing at Random That Is Associated With Fully Observed Predictors in a Nonlinear\nWay\nMethodBias of\nLog HRaSE of\nBiaszScore\nfor BiasSD of\nEstimateMean Length of\n95% CICoverage of\n95% CI, %\nFull data \u22120.0002 0.001 \u22120.1 0.037 0.148 96.1\nrfImpute 0.119 0.001 86.5 0.044 0.154 17.4\nmissForest 0.079 0.002 53.7 0.046 0.158 50.9\nMICE RF with 5 trees \u22120.021 0.001 \u221217.1 0.038 0.172 95.3\nMICE RF with 10 trees \u22120.005 0.001 \u22123.9 0.039 0.170 97.3\nMICE RF with 20 trees 0.006 0.001 4.5 0.039 0.168 96.3\nMICE RF with 50 trees 0.011 0.001 9.0 0.040 0.167 95.8MICE RF with 100 trees 0.013 0.001 10.5 0.040 0.167 94.7Parametric MICE \u22120.055 0.001 \u221244.8 0.039 0.178 79.8\nAbbreviations: CI, confidence interval; HR, hazard ratio; MICE, multivariate imputation by chained equations; RF,\nrandom forest; SD, standard deviation; SE, standard error.\naThe true log hazard ratio was set at 0.5.772 Shah et al.\nAm J Epidemiol. 2014;179(6):764 \u2013774Downloaded from https://academic.oup.com/aje/article/179/6/764/107562 by guest on 27 April 2024",
        "b3235467-9611-4557-92a0-fae372e56093": "interactions in imputation models, such that a standard para-\nmetric MICE imputation which ignores them gives biased re-sults. Random forest tuning parameters (such as the numberof trees and number of nodes) should be further investigated.\nConclusions\nMICE is one of the recommended met hods for multiple\nimputation in electronic health-record data, and we haveshown that standard parametric MICE and our new randomforest MICE method work reasonably well under arti \ufb01cially\nintroduced missingness at random in a realistically complexdata set. Random forest imputation should be further investi-gated in situations where MICE with default parametric im-putation models produces biased results.\nACKNOWLEDGMENTS\nAuthor af \ufb01liations: Clinical Epidemiology Group, Depart-\nment of Epidemiology and Public Health, School of Life andMedical Sciences, University College London, London,United Kingdom (Anoop D. Shah, Harry Hemingway); De-partment of Medical Statistics, London School of Hygieneand Tropical Medicine, London, United Kingdom (JonathanW. Bartlett, James Carpenter); and National Institute for Car-diovascular Outcomes Research, School of Life and MedicalSciences, University College London, London, United King-dom (Owen Nicholas).\nThis work was supported by grants from the United Kingdom\nNational Institute for Health Research (grant RP-PG-0407-10314); the Wellcome Trust (grants 086091/Z/08/Z and 0938/\n30/Z/10/Z to A.D.S.); the Medical Research Council (grants\nMR/K006584/1, G0902393, and G0900724 to J.W.B.); theUnited Kingdom Biobank; and the Farr Institute of Health Infor-matics Research (Health eResearch Centre Network), fundedby the Medical Research Council in partnership with Arthri-tis Research UK, the British Heart Foundation, Cancer Re-search UK, the Economic and Social Research Council, theEngineering and Physical Sciences Research Council, theNational Institute of Health Research, the National Institutefor Social Care and Health Research (Welsh Assembly Gov-ernment), the Chief Scientist Of \ufb01ce (Scottish Government\nHealth Directorates), and the Wellcome Trust.\nThe views and opinions expressed herein are those of the\nauthors and do not necessarily re \ufb02ect those of the National\nInstitute for Health Research or the United Kingdom Depart-ment of Health. All authors reviewed and approved the \ufb01nal\nmanuscript.\nCon\ufb02ict of interest: none declared.\nREFERENCES\n1. Marston L, Carpenter JR, Walters KR, et al. Issues in multiple\nimputation of missing data for large general practice clinical\ndatabases. Pharmacoepidemiol Drug Saf . 2010;19(6):618 \u2013626.\n2. Schafer JL. Analysis of Incomplete Multivariate Data . London,\nUnited Kingdom: Chapman & Hall Ltd; 1997.3. Little RJA, Rubin DB. Statistical Analysis With Missing Data .\n2nd ed. Hoboken, NJ: John Wiley & Sons, Inc; 2002.\n4. van Buuren S, Groothuis-Oudshoorn K. mice: Multivariate\nImputation by Chained Equations in R. J Stat Softw . 2011;\n45(3):1 \u201367.\n5. Seaman SR, Bartlett JW, White IR. Multiple imputation of\nmissing covariates with non-linear effects and interactions: anevaluation of statistical methods. BMC Med Res Methodol .\n2012;12(1):46.\n6. Hardt J, Herke M, Leonhart R. Auxiliary variables in multiple\nimputation in regression with missing X: a warning against\nincluding too many in small sample research. BMC Med Res\nMethodol . 2012;12(1):184.\n7. Burgette LF, Reiter JP. Multiple imputation for missing data via\nsequential regression trees. Am J Epidemiol . 2010;172(9):\n1070\u20131076.\n8. Breiman L. Random forests. Mach Learn . 2001;45(1):5 \u201332.\n9. Breiman L, Cutler A. Manual on Setting Up, Using, and\nUnderstanding Random Forests V3.1 . Berkeley, CA:\nUniversity of California, Berkeley; 2002. ( http://oz.berkeley.\nedu/users/breiman/Using_random_forests_V3.1.pdf ).\n(Accessed November 11, 2013).\n10.",
        "3825ed20-17e8-459f-8673-a0cf518a51fd": "BMC Med Res\nMethodol . 2012;12(1):184.\n7. Burgette LF, Reiter JP. Multiple imputation for missing data via\nsequential regression trees. Am J Epidemiol . 2010;172(9):\n1070\u20131076.\n8. Breiman L. Random forests. Mach Learn . 2001;45(1):5 \u201332.\n9. Breiman L, Cutler A. Manual on Setting Up, Using, and\nUnderstanding Random Forests V3.1 . Berkeley, CA:\nUniversity of California, Berkeley; 2002. ( http://oz.berkeley.\nedu/users/breiman/Using_random_forests_V3.1.pdf ).\n(Accessed November 11, 2013).\n10. Dasgupta A, Sun YV, K\u00f6nig IR, et al. Brief review of\nregression-based and machine learning methods in genetic\nepidemiology: the Genetic Analysis Workshop 17 experience.Genet Epidemiol . 2011;35(S1):S5 \u2013S11.\n11. Ishwaran H, Blackstone EH, Pothier CE, et al. Relative risk\nforests for exercise heart rate recovery as a predictor ofmortality. J Am Stat Assoc . 2004;99(467):591 \u2013600.\n12. Ishwaran H, Kogalur UB, Blackstone EH, et al. Random\nsurvival forests. Ann Appl Stat . 2008;2(3):841 \u2013860.\n13. Tsuji S, Midorikawa Y, Takahashi T, et al. Potential responders\nto FOLFOX therapy for colorectal cancer by random forestsanalysis. Br J Cancer . 2012;106(1):126 \u2013132.\n14. Stekhoven DJ, B\u00fchlmann P. MissForest \u2014non-parametric\nmissing value imputation for mixed-type data. Bioinformatics .\n2012;28(1):112 \u2013118.\n15. Eisemann N, Waldmann A, Katalinic A. Imputation of missing\nvalues of tumour stage in population-based cancer registration.\nBMC Med Res Methodol . 2011;11(1):129.\n16. Denaxas S, George J, Herrett E, et al. Data resource pro \ufb01le:\nCArdiovascular disease research using LInked BEspoke studies\nand electronic Records (CALIBER). Int J Epidemiol . 2012;\n41(6):1625 \u2013\n1638.\n17. Shah AD. CALIBERr \ufb01mpute: Imputation in MICE using\nRandom Forest . (R package, version 0.1-2). Vienna, Austria:\nComprehensive R Archive Network; 2013. ( http://cran.r-project.\norg/web/packages/CALIBERr \ufb01mpute/index.html ). (Accessed\nNovember 12, 2013).\n18. Herrett E, Thomas SL, Schoonen WM, et al. Validation and\nvalidity of diagnoses in the General Practice ResearchDatabase: a systematic review. Br J Clin Pharmacol . 2010;\n69(1):4 \u201314.\n19. Health and Social Care Information Centre. Hospital Episode\nStatistics . Leeds, United Kingdom: Health and Social Care\nInformation Centre; 2013. ( http://www.hscic.gov.uk/hes ).\n(Accessed November 11, 2013).\n20. Herrett E, Smeeth L, Walker L, et al. The Myocardial Ischaemia\nNational Audit Project (MINAP). Heart . 2010;96(16):1264 \u20131267.\n21. Shah AD, Nicholas O, Timmis AD, et al. Threshold\nhaemoglobin levels and the prognosis of stable coronarydisease: two new cohorts and a systematic review and\nmeta-analysis. PLoS Med . 2011;8(5):e1000439.\n22. Guasti L, Dentali F, Castiglioni L, et al. Neutrophils and clinical\noutcomes in patients with acute coronary syndromes and/orMultiple Imputation by Random Forest 773\nAm J Epidemiol. 2014;179(6):764 \u2013774Downloaded from https://academic.oup.com/aje/article/179/6/764/107562 by guest on 27 April 2024",
        "1d6adf53-9ec3-47cc-9643-72611e61b00c": "cardiac revascularization: a systematic review on more\nthan 34,000 subjects. Thromb Haemost . 2011;106(4):\n591\u2013599.\n23. N\u00fa\u00f1ez J, Mi\u00f1ana G, Bod\u00ed V, et al. Low lymphocyte count and\ncardiovascular diseases. Curr Med Chem . 2011;18(21):\n3226\u20133233.\n24. Hertz-Picciotto I, Rockhill B. Validity and ef \ufb01ciency of\napproximation methods for tied survival times in Cox\nregression. Biometrics . 1997;53(3):1151 \u20131156.\n25. White IR, Royston P. Imputing missing covariate values for the\nCox model. Stat Med . 2009;28(15):1982 \u20131998.\n26. Barnard J, Rubin D. Small-sample degrees of freedom with\nmultiple imputation. Biometrika . 1999;86(4):948 \u2013955.\n27. R Development Core Team. R: A Language and Environment\nfor Statistical Computing . Vienna, Austria: R Foundation for\nStatistical Computing; 2010. ( http://www.R-project.org/ ).\n(Accessed November 11, 2013).\n28. Stekhoven DJ. missForest: Nonparametric Missing Value\nImputation using Random Forest . (R package, version 1.3).\nVienna, Austria: Comprehensive R Archive Network; 2012.(http://cran.r-project.org/web/packages/missForest/index.\nhtml). (Accessed November 11, 2013).\n29. Therneau T, Lumley T. survival: Survival Analysis, Including\nPenalised Likelihood . (R package, version 2.36-2). Vienna,\nAustria: Comprehensive R Archive Network; 2010. ( http://cran.r-project.org/web/packages/survival/index.html ). (Accessed\nNovember 23, 2010).\n30. Liaw A, Wiener M. Classi \ufb01cation and regression by\nrandomForest. RN e w s . 2002;2(3):18 \u201322.\n31. Matsumoto M, Nishimura T. Mersenne twister: a\n623-dimensionally equidistributed uniform pseudo-random\nnumber generator. ACM Trans Model Comput Simul . 1998;\n8(1):3 \u201330.\n32. Bartlett JW, Seaman SR, White IR, et al. Multiple Imputation of\nCovariates by Fully Conditional Speci \ufb01cation: Accommodating\nthe Substantive Model . Ithaca, NY: Cornell University Library;\n2012. ( http://arxiv.org/pdf/1210.6799v3.pdf ). (Accessed\nNovember 11, 2013).\n33. Mendez G, Lohr S. Estimating residual variance in random\nforest regression. Comput Stat Data Anal . 2011;55(11):\n2937\u20132950.\n34. Rubin DB. Multiple imputation after 18+ years. J Am Stat\nAssoc . 1996;91(434):473 \u2013489.\n35. Marshall A, Altman DG, Holder RL. Comparison of imputation\nmethods for handling missing covariate data when \ufb01tting a Cox\nproportional hazards model: a resampling study. BMC Med Res\nMethodol . 2010;10:112.\n36. Carpenter JR, Goldstein H, Kenward MG. REALCOM-\nIMPUTE software for multilevel multiple imputation with\nmixed response types. J Stat Softw . 2011;45(5):1 \u201314.774 Shah et al.\nAm J Epidemiol. 2014;179(6):764 \u2013774Downloaded from https://academic.oup.com/aje/article/179/6/764/107562 by guest on 27 April 2024",
        "fac66c59-efce-47b3-82ce-bf22cd306770": "A pancreatic cancer risk prediction model (Prism) developed\nand validated on large-scale US clinical data\nKai Jia,aSteven Kundrot,bMatvey B. Palchuk,bJeff Warnick,bKathryn Haapala,bIrving D. Kaplan,cMartin Rinard,a,dand Limor Appelbaumc,\u2217,d\naDepartment of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, 02139, USA\nbTriNetX, LLC, Cambridge, MA, 02140, USA\ncBeth Israel Deaconess Medical Center, Boston, MA, 02215, USA\nSummary\nBackground Pancreatic Duct Adenocarcinoma (PDAC) screening can enable early-stage disease detection and long-\nterm survival. Current guidelines use inherited predisposition, with about 10% of PDAC cases eligible for\nscreening. Using Electronic Health Record (EHR) data from a multi-institutional federated network, we developedand validated a PDAC RISk Model (Prism) for the general US population to extend early PDAC detection.\nMethods Neural Network (PrismNN) and Logistic Regression (PrismLR) were developed using EHR data from 55 US\nHealth Care Organisations (HCOs) to predict PDAC risk 6 \u201318 months before diagnosis for patients 40 years or older.\nModel performance was assessed using Area Under the Curve (AUC) and calibration plots. Models were internal-externally validated by geographic location, race, and time. Simulated model deployment evaluated Standardised\nIncidence Ratio (SIR) and other metrics.\nFindings With 35,387 PDAC cases, 1,500,081 controls, and 87 features per patient, PrismNN obtained a test AUC of\n0.826 (95% CI: 0.824 \u20130.828) (PrismLR: 0.800 (95% CI: 0.798 \u20130.802)). PrismNN \u2019s average internal-external validation\nAUCs were 0.740 for locations, 0.828 for races, and 0.789 (95% CI: 0.762 \u20130.816) for time. At SIR = 5.10 (exceeding the\ncurrent screening inclusion threshold) in simulated model deployment, PrismNN sensitivity was 35.9% (speci \ufb01city\n95.3%).\nInterpretation Prism models demonstrated good accuracy and generalizability across diverse populations. PrismNN\ncould \ufb01nd 3.5 times more cases at comparable risk than current screening guidelines. The small number of features\nprovided a basis for model interpretation. Integration with the federated network provided data from a large, het-erogeneous patient population and a pathway to future clinical deployment.\nFunding Prevent Cancer Foundation, TriNetX, Boeing, DARPA, NSF, and Aarno Labs.\nCopyright \u00a9 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND\nlicense ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).\nKeywords: Pancreatic cancer; Risk prediction; Machine learning; Electronic health records; Federated data\nIntroduction\nMost cases of Pancreatic Duct Adenocarcinoma (PDAC)\nare diagnosed as advanced-stage disease, leading to a\ufb01ve-year relative survival rate of only 11%.\n1Expanding\nthe population currently being screened is crucial for\nincreasing early detection and improving survival. Cur-\nrent screening guidelines2\u20134targeting stage I cancers\nand high-grade PDAC precursors have signi \ufb01cantly\nimproved long-term survival.5,6Current guidelines\ntarget patients with a family history or genetic predis-position to PDAC,\n7,8with screening eligibility based onestimated absolute (5%) and relative ( \ufb01ve times) risk\ncompared to the general population.6These patients\ncomprise only about 10% of all PDAC cases. No\nconsensus or guidelines exist for PDAC screening in thegeneral population ,\n9where the majority of PDAC cases\nare found.\nSeveral groups have developed PDAC risk models for\nthe general population using various data sources.10\u201312\nMost such models aim for integration with Electronic\nHealth Record (EHR) systems for clinical implementa-tion. One effort used EHR data from an aggregated\n*Corresponding author.",
        "5bf88380-d9ee-4ea7-916e-7f7e3ae85d42": "Cur-\nrent screening guidelines2\u20134targeting stage I cancers\nand high-grade PDAC precursors have signi \ufb01cantly\nimproved long-term survival.5,6Current guidelines\ntarget patients with a family history or genetic predis-position to PDAC,\n7,8with screening eligibility based onestimated absolute (5%) and relative ( \ufb01ve times) risk\ncompared to the general population.6These patients\ncomprise only about 10% of all PDAC cases. No\nconsensus or guidelines exist for PDAC screening in thegeneral population ,\n9where the majority of PDAC cases\nare found.\nSeveral groups have developed PDAC risk models for\nthe general population using various data sources.10\u201312\nMost such models aim for integration with Electronic\nHealth Record (EHR) systems for clinical implementa-tion. One effort used EHR data from an aggregated\n*Corresponding author.\nE-mail addresses: lappelb1@bidmc.harvard.edu (L. Appelbaum), jiakai@mit.edu (K. Jia), steve.kundrot@trinetx.com (S. Kundrot), matvey.pal-\nchuk@trinetx.com (M.B. Palchuk), jeff.warnick@trinetx.com (J. Warnick), kathryn.haapala@trinetx.com (K. Haapala), ikaplan@bidmc.harvard.edu\n(I.D. Kaplan), rinard@csail.mit.edu (M. Rinard).\ndCo-senior authors.eBioMedicine\n2023;98: 104888\nPublished Online 25\nNovember 2023\nhttps://doi.org/10.\n1016/j.ebiom.2023.104888\nwww.thelancet.com Vol 98 December, 2023 1Articles",
        "5b0a97fe-d70e-42b0-bece-47a3a3026560": "multi-institutional database.13Their evaluation focused\non risk prediction up to one month before diagnosis\nwithout evaluating generalizability across locations or\nraces. Several other efforts using EHR data had limitedvalidation across locations and races.\n14\u201316Other efforts\nworked with small sample sizes10,17and internal valida-\ntion only.12,17\nWe used EHR data from 55 US Health Care Orga-\nnisations (HCOs) from a federated data network to\ndevelop and validate PDAC risk prediction models forthe general population. Our models enable identifying\nindividuals at high risk for PDAC from the general\npopulation, so they can be offered early screening orreferred for lower overhead testing such as biomarkertesting.\nThe data network provides access to harmonised, de-\nidenti \ufb01ed EHR data of over 89 million patients for\nmodel development and testing. Because the network is\nconnected to the EHR systems of the participating\nHCOs, it provides a pathway to model deployment in aclinical setting, a critical step in the progression toward\nsuccessful clinical adoption.\n18\nWe developed a methodology to train PDAC RISk\nprediction Model (Prism) on federated network EHR\ndata. We worked with two classes of models: neural\nnetworks (PrismNN) and logistic regression(PrismLR). Prism models identify high-risk patients6\u201318 months before an initial PDAC diagnosis. Our\nevaluation reports Area Under the Curve (AUC) and\nrisk calibration. We also conducted three types ofinternal-external validation: location-based, race-based,\nand temporal. Furthermore, we simulated the deploy-\nment of Prism models with temporally separatetraining/test data to evaluate their performance in a\nmore realistic setting. Evaluation metrics include\nsensitivity and Standardised Risk Ratio (SIR). SIR was\nbased on demographically matched PDAC incidencerates of the general US population from the SEER\ndatabase.\n19Fig. 1 summarises the process of our model\ndevelopment.\nMethods\nData source and setting\nThis is an observational retrospective study with both acase \u2013control and cohort design. Our goal is to develop\nmachine learning models to predict risk of PDACdiagnosis 6 \u201318 months in the future based on existing\nEHRs of a patient. We used data from the federated\nEHR database platform of TriNetX.\n20TriNetX is global\nresearch network that specialises in EHR data collection\nand distribution. The network consists of mostly large,\nacademic medical centres, community hospitals, andoutpatient clinics. TriNetX supports pulling data from\nany EHR systems used by HCOs.\nWe used retrospective de-identi \ufb01ed EHR data from\n55 HCOs across the United States. On average, eachHCO provides approximately 13 years of historical data.\nData include values from structured EHR \ufb01elds (e.g.,\ndemographics, date-indexed encounters, diagnoses,procedures, labs, and medications) as well as facts and\nnarratives from free text (e.g., medications identi \ufb01ed\nthrough Natural Language Processing (NLP)). TriNetXharmonises all data from each HCO \u2019s EHR to the Tri-\nNetX standard data model and a common set of\ncontrolled terminologies.Research in context\nEvidence before this study\nWe searched PubMed for publications on pancreatic cancer\nrisk prediction models for the general population. We focusedon articles published between 2013 and 2023, using the\nsearch terms \u201cpancreatic cancer \u201d,\u201crisk prediction models \u201d,\nand \u201cgeneral population \u201d. Previous studies have developed\nand validated Pancreatic Duct Adenocarcinoma (PDAC) riskmodels on large populations. However, they are limited by\ntheir lack of racial and geographic diversity, external\nvalidation, and a clear pathway to clinical implementation.Moreover, while other models use standard classi \ufb01cation\nmetrics such as AUC for performance evaluation, they provide\nlittle insight into the comparison with currently utilised PDACscreening inclusion criteria for high-risk individuals with an\ninherited predisposition.\nAdded value of this study\nWe used Electronic Health Record (EHR) data from 55 Health\nCare Organisations (HCOs) across the US within a federatednetwork platform including over 1.5 million PDAC cases andcontrols. We developed, internally and internal-externally\nvalidated, and simulated the deployment of PDAC risk models\nfor early prediction of 6 \u201318 months before diagnosis.",
        "70e3f3d2-5f32-4edf-b9eb-cf2a87505a26": "Previous studies have developed\nand validated Pancreatic Duct Adenocarcinoma (PDAC) riskmodels on large populations. However, they are limited by\ntheir lack of racial and geographic diversity, external\nvalidation, and a clear pathway to clinical implementation.Moreover, while other models use standard classi \ufb01cation\nmetrics such as AUC for performance evaluation, they provide\nlittle insight into the comparison with currently utilised PDACscreening inclusion criteria for high-risk individuals with an\ninherited predisposition.\nAdded value of this study\nWe used Electronic Health Record (EHR) data from 55 Health\nCare Organisations (HCOs) across the US within a federatednetwork platform including over 1.5 million PDAC cases andcontrols. We developed, internally and internal-externally\nvalidated, and simulated the deployment of PDAC risk models\nfor early prediction of 6 \u201318 months before diagnosis. Our\nPDAC RISk Model (Prism) uses 87 features derived from EHR\ndiagnosis, medication, lab, and demographic data from a\nracially and geographically diverse population. Prismmaintained its accuracy across internal-external validation andsimulated deployment within the network platform and can\nnow be tested prospectively on multiple institutional data\nwithin the network. The model captured 3.5 times morepatients than the current inclusion criteria used to identify\npatients for PDAC screening programs at similar risk levels.\nImplications of all the available evidence\nPrism can potentially help primary care providers nationwide\nnoninvasively identify high-risk individuals for PDACscreening or serve as a \ufb01rst\ufb01lter before subsequent biomarker\ntesting. Prism sets the stage for model deployment within a\nfederated network to identify high-risk patients at multipleinstitutions participating in the network.Articles\n2 www.thelancet.com Vol 98 December, 2023",
        "1bd0cfd1-0a94-499b-b2be-2e2d2d2fc780": "Study population\nWe worked with a PDAC case group and a control\ngroup. We obtained all data during Nov and Dec, 2022.\nWe obtained the PDAC group by querying TriNetX toobtain EHR data for patients currently 40 years old or\nolder with one of the following ICD-10/ICD-9 codes:\nC25.0, C25.1, C25.2, C25.3, C25.7, C25.8, C25.9, and157. We obtained n = 132,789 PDAC cases. We excluded\npatients who were diagnosed before 40 years of age\n(n = 1924), patients with entries two months after theirdeath record (n = 484, likely due to mixed entries of\ndifferent patients), and patients with insuf \ufb01cient medi-\ncal history (n = 94,994, de \ufb01ned in Section 2.3), to obtain\na PDAC group with n = 35,387 cases.\nFor the control group, we queried TriNetX for pa-\ntients at least 40 years old without any of the above ICD-\n10 or ICD-9 codes. The query matched n = 51,139,587patients. From them, we uniformly sampled\nn = 6,499,996 patients. We excluded patients with a\nPDAC tumour registry entry but no PDAC diagnosisentries (n = 304), patients with records two months after\ntheir death record (n = 26,783), and patients with\ninsuf \ufb01cient medical history (n = 4,972,828, de \ufb01ned in\nSection 2.3) to obtain a control group with n = 1,500,081\ncases.\nModel development\nWe trained and evaluated two model classes, neuralnetworks (PrismNN) and logistic regression (PrismLR).\nData were randomly partitioned into training (75%),validation (10%), and test (15%) sets. Note that the\nvalidation set refers to the dataset for modelhyperparameter selection and should not be confused\nwith our internal-external validation that accesses model\nperformance.\nOur training and testing procedures work with a\ncutoff date Cfor every patient. We derived features from\nentries available before Cto predict PDAC risk between\nC+6 months and C+18 months. We sampled Cuni-\nformly between 6 and 18 months before diagnosis for\neach PDAC case and matched the cutoff dates for con-\ntrol cases. Patient age was assessed at the cutoff date.Given a cutoff date C, we empirically de \ufb01ned a patient to\nhave suf \ufb01cient medical history if their total number of\ndiagnosis, medication, or lab entries within 2 yearsbefore Cis at least 16, their \ufb01rst entry is at least 3\nmonths earlier than their last entry before C, and their\nage at Cis at least 38.5 years (40 years minus 18\nmonths). We removed patients without suf \ufb01cient med-\nical history as described in Section 2.2.\nWe derived four classes of features from EHR: basic\nfeatures, diagnosis features, medication features, andlab features. Basic features have six values, four for de-\nmographic information (age and sex, and their existence\nbits) and two for frequencies of clinical encounters. Wecounted the number of diagnosis, medication, or lab\nentries greater than 18 months before the cutoff date as\nthe number of early records, and entries within 18months as the number of recent records. The numbersof records serve as a proxy of clinical encounter fre-\nquencies, which are correlated with cancer diagnosis.\n21\nOther features encode information about the existence,\nfrequency, time span, and lab results (if applicable) of\nthe corresponding type of EHR entries. We ignoredFig. 1: Flowchart of our study, including data collection and training/evaluation of simulated deployment.Articles\nwww.thelancet.com Vol 98 December, 2023 3",
        "9765b803-ba3b-4c24-b26d-96e475cbc431": "EHR entry types appearing in less than 1% of the PDAC\ncases in the training set.\nFor each EHR entry type, we included an existence\nbit feature (0 or 1) indicating if the patient \u2019s EHR con-\ntains such entries. This encoding accounts for the\nadditional effect of the healthcare process on EHRdata.\n22The encoding also enables model predictions\nbased on whether a feature is present or missing.\nBecause PrismNN can use sophisticated nonlinearreasoning, data imputation provides little to no usefuladditional information for these models. Therefore, we\ndid not use any imputation.\nThe above process generates over \ufb01ve thousand fea-\ntures. To improve interpretability, we automatically\nselected fewer features by L0 regularisation on a binary\ninput mask\n23and iterative feature removal. We will\npresent all the selected features ranked by their uni-\nvariate predictive power with the PrismNN model,\ncalculated as the AUC achieved by using each type offeature while zeroing all other features.\nModel was calibrated with a variant of the Platt cali-\nbration.\n24Calibration was evaluated with calibration\nplots on the test set. We also calculated the GeometricMean of Over Estimation (GMOE), the geometric mean\nof the ratios of predicted risks to the actual risks, for\nquantitative calibration evaluation.\nWe evaluated the mean AUC and GMOE on test sets\nin nine independent runs with different random seeds\nfor dataset split and model initialisation. We also eval-uated model AUC and GMOE for a few age subgroups:\n40\u201349, 50 \u201369, 70+, and 50+ years old. Since boot-\nstrapping has shown more accurate performance esti-mations in certain settings,\n25we further evaluated the\nmodel AUC using optimism-corrected bootstrapping.\nThe Supplemental Material (Sections A.2 and A.3)\nprovides more details on model development, includingdetails on feature selection, model training/calibration,\nand model evaluation.\nInternal-external validation\nOur use of a large federated network enabled internal-\nexternal validation by splitting data within thenetwork.\n26,27Our internal-external validation considered\nthree attributes: geographic location of the HCO (or its\nheadquarter if an HCO covers multiple locations), pa-tient race, and training data time. For each attribute, wesplit the dataset accordingly, trained models on one\nsplit, and tested on the other. We repeated all model\ndevelopment steps, including the automatic feature se-lection, on the training set for each internal-external\nvalidation.\nFor location and race-based validations, we assessed\nmodel generalizability by comparing the AUCs of\ninternal-external-validation models with corresponding\ncontrol models. Control models used the same sizes oftraining and test sets as the attribute-based split but\nused random splitting that ignores attribute values. Wealso calculated the gap between AUCs on the validation\nand test sets as an extra generalizability assessment. We\nfurther calculated the I\n2index of the AUCs of\ngeographic or racial subgroups to assess the heteroge-\nneity of model performance. Table 1 provides the loca-\ntion and race distributions. We excluded patients withunknown HCO locations or races in the internal-\nexternal validation.\nFor temporal validation, we selected the 50%,\n60%,...,90% percentile of diagnosis dates as split dates.We trained models on data available prior to those split\ndates. We used the data available after Oct 10, 2021 (the\n90% percentile) as the test set for all models. Since thesplit date also impacts the size of the training set, we\ntrained control models with the same number of\nrandomly sampled cases (i.e., the control models useddata available before each split dates but sampled only N\nPDAC cases from them, where Nis half the total\nnumber of PDAC cases, corresponding to the numberof cases with 50% percentile).\nSimulated deployment\nWe estimated the model performance in a clinicalsetting by simulating model deployment using a pro-\nspective cohort study design. We trained the model on\ndata available before Apr 11, 2020 (70% percentile ofdiagnosis dates) as the above temporal validation. We\nthen simulated a clinical study. We periodically enrolled\nnew patients into the study when they \ufb01rst satis \ufb01ed the\nage and suf \ufb01cient medical history requirements (see\nSection 2.3) after Apr 11, 2020.",
        "95313052-639f-4d04-b29f-9fe768de9eee": "Since thesplit date also impacts the size of the training set, we\ntrained control models with the same number of\nrandomly sampled cases (i.e., the control models useddata available before each split dates but sampled only N\nPDAC cases from them, where Nis half the total\nnumber of PDAC cases, corresponding to the numberof cases with 50% percentile).\nSimulated deployment\nWe estimated the model performance in a clinicalsetting by simulating model deployment using a pro-\nspective cohort study design. We trained the model on\ndata available before Apr 11, 2020 (70% percentile ofdiagnosis dates) as the above temporal validation. We\nthen simulated a clinical study. We periodically enrolled\nnew patients into the study when they \ufb01rst satis \ufb01ed the\nage and suf \ufb01cient medical history requirements (see\nSection 2.3) after Apr 11, 2020. For each enrolled pa-\ntient, we evaluated their PDAC risk using our modelsevery 90 days until there was no more suf \ufb01cient data\n(i.e., 18 months before dataset query date, or no more\nsuf\ufb01cient medical history on future dates) or the patient\ngot a PDAC diagnosis. We followed up each enrolledpatient starting 6 months after their \ufb01rst risk evaluation\nuntil 18 months after their last risk evaluation to see if\nthey were diagnosed with PDAC during the follow upperiod. The selection of enrolment, risk evaluation, and\nfollow up dates was independent of the model. We\ncomputed model performance statistics, includingsensitivity, speci \ufb01city, Positive Predictive Value (PPV),\nand Standardised Incidence Ratio (SIR), based on\nwhether a patient ever received a high-risk predictionbetween 6 and 18 months before their PDAC diagnosis.\nWe chose multiple high-risk thresholds according to\nthe 89.00%, 92.00%, 96.60%, 97.80%, 99.70%, and\n99.95% speci \ufb01city levels on the validation set. Overall,\nour design simulated the anticipated clinical application\nof our models to periodically evaluating every patient \u2019s\nPDAC risk. Supplemental Material (Section A.4) pro-\nvides more details on the study design.\nWe accounted for unbalanced data sampling (we\nused all PDAC cases but a subset of the control cases) toestimate the PPV and SIR that would be obtained if we\nhad evaluated the model on the entire TriNetXArticles\n4 www.thelancet.com Vol 98 December, 2023",
        "fe52d870-f0a6-4b74-96d3-05d039ecc09d": "population. SIR is the ratio of the observed PDAC cases\n(true positives) in the high-risk group to the expected\nnumber of PDAC cases of that group. To calculate the\nexpected number of cases, we used the SEER database,19\nmatched with age, sex, race, and calendar year for each\nindividual in the high-risk group, as done by Porter,\nLaheru, Lau, He, Zheng, Narang et al.28\nEthics\nAll EHR data were obtained through TriNetX and de-identi \ufb01ed by TriNetX. We accessed the data under a\nno-cost collaboration agreement. Some EHR entries\nwere obfuscated by TriNetX to protect patient privacy.\nNo human subjects were dir ectly involved in this\nstudy. No patient could be re-identi \ufb01ed from the data\nused in this study. Because this study used only de-\nidenti \ufb01ed patient records and did not involve the\ncollection, use, or transmittal of individually identi \ufb01-\nable data, this study was exempted from Institutional\nReview Board approval, as determined by the WesternIRB.Statistics\nThe PDAC group used all available cases on TriNetX.\nThe sample size of control group (6,499,996) was\ndetermined by the size that can be effectively handled byour storage capacity and computational resources.\nOur dataset took up 734 GiB of storage. We aimed to\ninclude everyone satisfying our age and medical historysuf\ufb01ciency requirements, with minimum exclusion to\nimprove data quality.\nBootstrapping was performed with 16 repetitions;\neach repetition included all model development stepsincluding automatic feature selection and took about an\nhour. We followed the algorithm for optimism-corrected\nbootstrapping described by Steyerberg, Section 5.3.4.\n29\nCon\ufb01dence intervals of AUCs were calculated with\nan optimised version of the DeLong \u2019s algorithm.30,31\nCon\ufb01dence intervals of binomial proportions (e.g.,\nsensitivity, speci \ufb01city) were calculated with the exact\nClopper-Pearson method.32Con\ufb01dence intervals of the\nAUC mean in internal-external validation were calcu-lated assuming a Gaussian mixture model to avoid theAttribute Cancer group (n = 35,387) Control group (n = 1,500,081)\nN (%) N (%)\nSex\nFemale 18,341 (51.83) 841,042 (56.07)\nMale 17,045 (48.17) 637,674 (42.51)\nUnknown 1 (0.00) 21,365 (1.42)\nAge at cutoff\nMean (SD) 67.55 (10.61) 59.50 (12.87)\n<40 135 (0.38) 103,566 (6.90)\n40\u201350 2052 (5.80) 285,492 (19.03)\n50\u201360 5762 (16.28) 342,037 (22.80)\n60\u201370 10,727 (30.31) 354,907 (23.66)\n70\u201380 10,531 (29.76) 251,333 (16.75)\n>80 4175 (11.80) 86,841 (5.79)\nUnknown 2005 (5.67) 75,905 (5.06)\nRace\nAIAN 93 (0.26) 5527 (0.37)Asian 504 (1.42) 32,998 (2.20)\nBlack 5315 (15.02) 228,256 (15.22)\nNHPI 21 (0.06) 1883 (0.13)White 25,634 (72.44) 1,046,240 (69.75)Unknown 3820 (10.79) 185,177 (12.34)\nHCO location\nMidwest 8371 (23.66) 230,088 (15.34)Northeast 11,831 (33.43) 426,469 (28.43)South 12,246 (34.61) 682,417 (45.49)West 2595 (7.33) 120,961 (8.06)Unknown 344 (0.97) 40,146 (2.68)\nNo.",
        "aaa351dd-6c11-41d5-ad19-e2953bdd405d": "medical records\nMean (SD) 854.06 (1501.89) 440.23 (939.50)\nRace abbreviations: AIAN: American Indian or Alaska Native; Black: Black or African American; NHPI: Native Hawaiian or Other Paci \ufb01c Islander. The numbers in brackets\nindicate percentages of the corresponding category, except for the two rows with Mean (SD) where the bracketed numbers are standard deviations.\nTable 1: Demographics of our dataset.Articles\nwww.thelancet.com Vol 98 December, 2023 5",
        "1c73f977-a021-431b-8843-7e72d0d28066": "assumption of a global mean. The Supplemental\nMaterial (Section A.4) provides more details for calcu-\nlation in simulated deployment.\nCalculation of I2for geographic/racial subgroups\nassumes a random effects model yi=\u03bc+ei+Eiwhere \u03bc\nis assumed as the \u201ctrue \u201dAUC, yiis the measurement\nof\u03bcbased on subgroup i,ei\u223cN(0,\u03c32\ni)models the\nmeasurement uncertainty, and Ei\u223cN(0,\u03c42) models\nthe heterogeneity between subgroups.33We used the\nde\ufb01nition I2=(Q\u2212(k\u22121))/Q.34We calculated the Q\nstatistics using the log-odds of AUCs to better match\nthe normal assumption.35We estimated Q\u2019sc o n \ufb01dence\ninterval using Monte Carlo simulation with 3 \u00d7 106\nsamples.\nRole of funders\nTriNetX provided cloud computing resources and access\nto the TriNetX data platform. A few TriNetX employees,\nlisted as coauthors, made direct contributions to thisresearch (see the Contributors section). Other funders\ndid not have any role in the study design, data collection,\ndata analyses, interpretation, or writing.\nResults\nModel evaluation\nBoth PrismNN and PrismLR used 35,387 patients with\ncancer and 1,500,081 controls up to 98.1 years old.\nTable 1 shows demographics, including sex, age, race,\nand HCO location. Fig. 1 presents a \ufb02owchart of dataset\ncreation. The Supplemental Material (Table A1 ) has\nmore demographic details.\nThe average AUCs of PrismNN and PrismLR on\nnine random runs were 0.826 (95% CI: 0.824 \u20130.828)\nand 0.800 (95% CI: 0.798 \u20130.802), respectively. With\nbootstrapping, the optimism-corrected AUCs of.PrismNN and PrismLR were 0.825 (95% CI:\n0.823 \u20130.827) and 0.801 (95% CI: 0.799 \u20130.804), respec-\ntively. Because our models incorporate the presence orabsence of features, each feature is a predictor and we\nhave no participants with missing predictors.\n22The\naverage GMOE on nine random runs was 1.169 (95%CI: 1.145 \u20131.192) and 0.969 (95% CI: 0.945 \u20130.993) for\nPrismNN and PrismLR, respectively.\nFig. 2 a shows the ROC curve of one of the nine\nrandom runs, with AUCs being 0.825 (95% CI:0.819 \u20130.830) (PrismNN) and 0.798 (95% CI:\n0.793 \u20130.804) (PrismLR). Fig. 2 b shows the correspond-\ning log-scale calibration plots on the test set. GMOE was1.161 (PrismNN) and 0.982 (PrismLR).\nThe PrismNN AUCs for different age groups were\n0.847 (95% CI: 0.826 \u20130.869), 0.796 (95% CI:\n0.787 \u20130.806), 0.775 (95% CI: 0.765 \u20130.785), and 0.797\n(95% CI: 0.790 \u20130.804) for 40 \u201349, 50 \u201369, 70+, and 50+\nyears old, respectively. The corresponding GMOEs were11.277, 1.057, 1.400, and 1.201. The PrismLR AUCs\nwere 0.822 (95% CI: 0.799 \u20130.846), 0.767 (95% CI:0.757 \u20130.777), 0.741 (95% CI: 0.730 \u20130.752), and 0.766\n(95% CI: 0.759 \u20130.773) and GMOEs were 90.107, 0.804,\n1.253, and 1.068. Fig. 3 presents all the selected features\nranked by feature predictive power with PrismNN.",
        "56b1019b-e4b6-4dc9-ab71-b2a31a183b37": "The corresponding GMOEs were11.277, 1.057, 1.400, and 1.201. The PrismLR AUCs\nwere 0.822 (95% CI: 0.799 \u20130.846), 0.767 (95% CI:0.757 \u20130.777), 0.741 (95% CI: 0.730 \u20130.752), and 0.766\n(95% CI: 0.759 \u20130.773) and GMOEs were 90.107, 0.804,\n1.253, and 1.068. Fig. 3 presents all the selected features\nranked by feature predictive power with PrismNN.\nModel features include known PDAC risk factors such\nas age, sex, diabetes mellitus, pancreatitis, pancreaticcysts, and abdominal pain; other features include hy-\npertension, hypercholesterolemia, kidney function, and\nfrequency of clinical visits preceding PDAC diagnosis.Figure A1 in the Supplemental Material presents how\nmodel performance varies with different numbers of\nselected features.\nInternal-external validation results\nFig. 4 shows the results for location-based, race-based,\nand temporal internal-external validation. The numberof patients of each HCO location or racial group can be\nseen in Table 1 . Note that as stated in Section 2.4, the\nresults were obtained by training models on non-random data splits according to race/location/time.\nFig. 4 a presents location-based internal-external valida-\ntion results. PrismNN AUCs on the test sets were 0.735(95% CI: 0.730 \u20130.741), 0.723 (95% CI: 0.719 \u20130.728),\n0.747 (95% CI: 0.743 \u20130.751), and 0.754 (95% CI:\n0.745 \u20130.764) for the Midwest, Northeast, South, and\nWest, respectively. PrismLR AUCs were 0.748 (95% CI:0.743 \u20130.753), 0.748 (95% CI: 0.744 \u20130.753), 0.751 (95%\nCI: 0.746 \u20130.755), and 0.730 (95% CI: 0.720 \u20130.740). AUC\ndrop between test and control models was between0.078 and 0.099 for PrismNN, and between 0.049 and\n0.072 for PrismLR. The average test AUCs on the four\nlocations were 0.740 (95% CI: 0.716 \u20130.764) and 0.744\n(95% CI: 0.727 \u20130.762) for PrismNN and PrismLR,\nrespectively. The I\n2indexes of PrismNN and PrismLR\nwere 99.2% (95% CI: 86.7% \u201399.8%) and 95.9% (95%\nCI: 33.5% \u201398.8%), respectively.\nFig. 4 b presents race-based internal-external valida-\ntion results. PrismNN AUCs on the test sets were 0.822\n(95% CI: 0.782 \u20130.862), 0.835 (95% CI: 0.818 \u20130.851),\n0.821 (95% CI: 0.816 \u20130.827), 0.893 (95% CI:\n0.839 \u20130.947), and 0.768 (95% CI: 0.765 \u20130.771) for\nAIAN, Asian, Black, NHPI, and White, respectively. ThePrismLR AUCs were 0.787 (95% CI: 0.745 \u20130.829), 0.809\n(95% CI: 0.791 \u20130.828), 0.803 (95% CI: 0.798 \u20130.809),\n0.877 (95% CI: 0.809 \u20130.945), and 0.793 (95% CI:\n0.790 \u20130.796). AUC drop between test and control\nmodels was between \u22120.067 and 0.018 for PrismNN,\nand between \u22120.054 and 0.018 for PrismLR.",
        "84098651-f2fd-4807-a2a7-c7326e579d9d": "ThePrismLR AUCs were 0.787 (95% CI: 0.745 \u20130.829), 0.809\n(95% CI: 0.791 \u20130.828), 0.803 (95% CI: 0.798 \u20130.809),\n0.877 (95% CI: 0.809 \u20130.945), and 0.793 (95% CI:\n0.790 \u20130.796). AUC drop between test and control\nmodels was between \u22120.067 and 0.018 for PrismNN,\nand between \u22120.054 and 0.018 for PrismLR. The average\ntest AUCs on the \ufb01ve races were 0.828 (95% CI:\n0.744 \u20130.912) and 0.814 (95% CI: 0.740 \u20130.888) for\nPrismNN and PrismLR, respectively. The I\n2indexes of\nPrismNN and PrismLR were 99.8% (95% CI: 92.9% \u2013\n100.0%) and 96.4% (95% CI: 2.9% \u201399.2%), respectively.\nFig. 4 c present temporal validation results. Models\nachieved average test AUCs 0.789 (95% CI: 0.762 \u20130.816)\n(PrismNN) and 0.780 (95% CI: 0.763 \u20130.798) (PrismLR).\nPerformance tends to become better with more recentArticles\n6 www.thelancet.com Vol 98 December, 2023",
        "a5e82535-32f9-4a7d-8962-2e664f6eb9bb": "training data and larger training sets, but the change is\nnot statistically signi \ufb01cant.\nSimulated deployment results\nWe simulated model deployment on 185,932 patients\n(with 7095 PDAC cases) in the test set, with enrolment\nfrom Apr 11, 2020 to Apr 6, 2021. Mean age at enrol-\nment was 61.62 (SD 11.98). Mean age at PDAC diag-nosis was 69.75 (SD 10.37). Each patient was followedup for 1.82 (SD 0.31) years ( Table 2 ).\nThe estimated model PPV range on the whole Tri-\nNetX population was 0.28% \u20138.62% for PrismNN and\n0.29% \u20132.88% for PrismLR. PrismNN and PrismLR SIR\nranges were 2.38 \u201396.0 and 2.22 \u201324.2, respectively. The\nSIR of all the enrolled patients during the follow-upperiod was 1.00 (95% CI: 0.98 \u20131.01). A SIR close to\none indicates that our TriNetX test population with pa-\ntient exclusion has similar PDAC incidence as thegeneral US population.\nWe determined the high-risk group to be individuals\nwith a SIR of 5.10 or above, based on PrismNN, which iscorrelated with a 35.9% sensitivity and 95.3% speci \ufb01city.\nThis SIR threshold is similar to the current eligibility\ncutoff for inclusion into screening programs.\n2\nTheSupplemental Material has more comprehensive\nsimulated deployment results with more risk levels\n(Tables A2 and A3 ) and breakdowns of \ufb01nal model\nperformance within different race/location/age/sexsubgroups ( Tables A4 \u2013A9).\nDiscussion\nOur study leveraged routinely collected EHR data from\na federated network including 55 US HCOs to developand validate two families of models (PrismNN andPrismLR) for identifying patients in the general pop-\nulation at high PDAC risk, 6 \u201318 months before the \ufb01rst\nPDAC diagnosis. Both models were trained on 35,387PDAC cases and 1,500,081 controls with featuresderived from demographics, diagnosis, medication,\nand lab entries in EHR. Both models used 87 features(Fig. 3 ) automatically selected using the training data.\nPrismNN achieved better AUC than PrismLR on the\ntest set, delivering test AUCs of 0.826 (95% CI:0.824 \u20130.828) and 0.800 (95% CI: 0.798 \u20130.802),\nrespectively. Bootstrapping gave similar AUC estima-\ntions. Both models showed worse performance forpatients over 50 years old compared to over 40 yearsold, while PrismNN maintained better performance\nthan PrismLR for different age groups; the deviation of\nGMOE from 1 indicates recalibration is needed if anage-based subgroup of patients is targeted. PrismNN\nshowed worse average AUC in location-based valida-\ntion than PrismLR, but performed favourablycompared to PrismLR in other validations. The large\ncapacity and \ufb02exibility of neural networks make them a\ngood choice for modelling complex relationships inEHR data, but such capacity may hinder generaliz-\nability compared to simpler models. Although inter-\npretation of neural networks is more challenging, ourautomatic feature selection provides insight into thereasoning process of the models.\nWe anticipate two potential clinical use cases for\nPrism. The \ufb01rst is to expand the eligibility for current\nscreening programs that utilise imaging modalities such\nas Endoscopic UltraSound (EUS) and MRI/MRCP.\n6\nCurrent eligibility criteria are based on familial PDAC\nor a known germline mutation syndrome (e.g., Lynch,\nPeutz-Jeghers).6The identi \ufb01ed population have a mini-\nmum lifetime SIR of 5 and includes only about 10% ofPDAC cases.\n7,8Depending on the chosen high-risk\nthreshold, PrismNN exhibited a two-year SIR of\n2.38 \u201396.0. At a SIR of 5.10, PrismNN identi \ufb01ed 35.9%\nof the PDAC cases as high risk 6 \u201318 months before\ndiagnosis, a signi \ufb01cant improvement over current\neligibility criteria.",
        "1cfd446b-158d-42de-b8a2-a596870ef055": "We anticipate two potential clinical use cases for\nPrism. The \ufb01rst is to expand the eligibility for current\nscreening programs that utilise imaging modalities such\nas Endoscopic UltraSound (EUS) and MRI/MRCP.\n6\nCurrent eligibility criteria are based on familial PDAC\nor a known germline mutation syndrome (e.g., Lynch,\nPeutz-Jeghers).6The identi \ufb01ed population have a mini-\nmum lifetime SIR of 5 and includes only about 10% ofPDAC cases.\n7,8Depending on the chosen high-risk\nthreshold, PrismNN exhibited a two-year SIR of\n2.38 \u201396.0. At a SIR of 5.10, PrismNN identi \ufb01ed 35.9%\nof the PDAC cases as high risk 6 \u201318 months before\ndiagnosis, a signi \ufb01cant improvement over current\neligibility criteria.\nThe second use case is to identify an enriched group\nfor lower overhead testing (such as biomarker testing)ab\nFig. 2: Model evaluation results. (a) ROC (Receiver Operating Characteristic) curves on the test set. (b) Log-scale calibration plots on the test set.Articles\nwww.thelancet.com Vol 98 December, 2023 7",
        "ee48c04d-b1e1-4191-937f-f2a71c4064e0": "Fig. 3: List of selected features ranked by univariate AUC of PrismNN. The label diag refers to diagnosis features, med to medication features,\nand lab to lab features. Letters in the brackets indicate the types of derived features: e for existence, fd for \ufb01rst date, ld for last date, p for time\nspan, f for frequency, v for latest lab value, ve for whether a valid lab value exists, s for lab value slope, and se for whether lab value slope can becomputed.Articles\n8 www.thelancet.com Vol 98 December, 2023",
        "81e6d335-5e82-4c83-a815-0a99119c2618": "followed by screening based on the lower overhead test.\nIn this use case, the model could be deployed at a highersensitivity than in our \ufb01rst use case. For example, at\n85.3% speci \ufb01city, PrismNN exhibited 54.6% sensitivity.\nTo facilitate the integration of this model into clinical\npractice, a prospective study that examines the impact ofimplementing a PDAC risk model in real-time on\nscreening behaviour is needed. This study would involve\ndeploying the Prism models on patient EHR data andscreening the high-risk group. The aim would be to\nevaluate both quantitative clinical outcome measures,\nsuch as the identi \ufb01cation of a higher number of early-\nstage PDAC cases compared to typical hospital \ufb01gures,and qualitative measures of model adoption by practi-\ntioners such as end-user (primary care provider)satisfaction.\nWe considered three types of internal-external vali-\ndation to evaluate model generalizability. We split thedataset according to some attribute, trained models onone part, and tested the models on the other. Results of\nrace-based validation highlight the generalizability\nacross diverse racial populations. When tested on White,there was a small AUC drop, which we attribute to the\nfact that the White group constituted about 70% of the\ndataset while training was performed on the remaining30% of data. Location-based validation showed modestab\nc\nFig. 4: Internal-external validation results. Error bars indicate 95% CI. Dashed horizontal lines and the surrounding regions indicate the original\nNN/LR test AUCs and 95% CI without attribute-based splitting. NN is short for PrismNN, LR short PrismLR, and ctrl- for control models that use\nrandom data split with matched training set size for race/location validation and use matched training size to separate the effect of data size\nincrease for temporal validation. (a) Location-based internal-external validation. (b) Race-based internal-external validation. See notes und er\nTable 1 for race abbreviations. (c) Model performance over time in temporal validation.Articles\nwww.thelancet.com Vol 98 December, 2023 9",
        "d75fe20c-deec-4707-82c6-aa78b8b5caa4": "AUC drops, implying potentially systematic differences\nbetween EHR data from geographically different HCOs.Additional validation is urged for model deployment to\nHCOs outside of the network. The high values of I\n2\nindicate that model performance differences between\nlocations or races were more likely due to intrinsicheterogeneity instead of randomness, while PrismLR\nexhibited slightly less heterogeneity. Temporal valida-\ntion results showed good performance across time andsuggested (with insuf \ufb01cient statistical signi \ufb01cance) that\nmodel performance may improve with more recent\ntraining data and/or larger training data.\nWe evaluated the effectiveness of Prism in clinical\nimplementation by simulated deployment. A key aspect\nis training models on data available before a simulatedenrolment date to identify high-risk individuals afterthat date. We periodically evaluated PDAC risk for each\nindividual and followed the identi \ufb01ed high-risk in-\ndividuals over time to evaluate model performance. Thissimulated deployment methodology contrasts previous\nmethodologies that do not temporally separate the\ntraining and test data or test each individual at multiplecutoff dates.\n10,13By tracking the envisioned deployment\nscenario more closely, we eliminated a potential source\nof inaccuracy and obtained a potentially more accurateprediction of model performance in clinical use. Simu-\nlated deployment also reveals model performance dif-\nferences overlooked by traditional evaluation; althoughPrismNN and PrismLR have numerically close AUCs,the gap between their sensitivities in simulated deploy-\nment with SIR \u22655 is large.\nA signi \ufb01cant strength of our work is using a feder-\nated EHR network that ingests EHR data from multiple\nHCOs and presents data as a single format. This\nnetwork enabled our three types of external validationand simulated deployment.Integration with EHR systems is crucial in clinical\ndeployment of risk prediction models. Without properintegration, clinicians must manually enter information\ninto a program, which forms a signi \ufb01cant barrier to\nmodel adoption.\n18By contrast, federated networks allow\nseamless model integration due to their close interac-tion with existing HCO EHR systems. Federated net-\nworks provide a clear pathway for integrated model\ndevelopment, validation, and clinical deployment, allwithin a single platform.\n36\nOther researchers have used EHR data to develop\nPDAC risk prediction models for the generalpopulation.\n10,13\u201316Data set sizes ranged from 1792 PDAC\ncases/1.8 M controls15to 24,000 PDAC cases/6.2 M\ncontrols.14Some studies lacked an external validation,13\ncompleted the external validation/evaluated modelgeneralizability only with data from a single geographic\narea,\n14,16or validated only on one sex (male)15or race.11\nWhile some studies worked with data obtained from\nmultiple organisations,13\u201315none worked with a feder-\nated network that harmonises and standardises the data,\nnone provided a clear path to clinical deployment, andnone supported seamlessly deploying the model to new\nHCOs joining the federated network. Some previous\nstudies evaluated the ability of their models to identifyhigh-risk individuals either until or shortly before PDAC\ndiagnosis,\n13\u201315when the clinical bene \ufb01t is improbable.\nBy contrast, our evaluation focused on risk identi \ufb01cation\nat least six months before diagnosis, when early-stagedisease detection and potential cure are more likely.\nOur study has a few limitations: (i) Model develop-\nment and validation were retrospective. Prospectivestudies are needed to evaluate the ef \ufb01cacy of clinical\ndetection of early-stage disease; (ii) Despite the favour-\nable generalizability across racial groups demonstratedin our internal-external validation, certain racial groupsModel Risk level Sensitivity Speci \ufb01city PPV (TrxPop. Est.) SIR (TrxPop. Est.)",
        "aa7b0851-9e5c-433e-85bb-7f780ddf92b5": "Some previous\nstudies evaluated the ability of their models to identifyhigh-risk individuals either until or shortly before PDAC\ndiagnosis,\n13\u201315when the clinical bene \ufb01t is improbable.\nBy contrast, our evaluation focused on risk identi \ufb01cation\nat least six months before diagnosis, when early-stagedisease detection and potential cure are more likely.\nOur study has a few limitations: (i) Model develop-\nment and validation were retrospective. Prospectivestudies are needed to evaluate the ef \ufb01cacy of clinical\ndetection of early-stage disease; (ii) Despite the favour-\nable generalizability across racial groups demonstratedin our internal-external validation, certain racial groupsModel Risk level Sensitivity Speci \ufb01city PPV (TrxPop. Est.) SIR (TrxPop. Est.)\nPRISMNN 1 54.6% (53.4 \u201355.8) 85.3% (85.1 \u201385.5) 0.28% (0.27 \u20130.29) 2.38 (2.34 \u20132.41)\n2 48.7% (47.5 \u201349.9) 89.1% (89.0 \u201389.3) 0.34% (0.33 \u20130.35) 2.87 (2.82 \u20132.91)\n3 35.9% (34.8 \u201337.1) 95.3% (95.2 \u201395.4) 0.58% (0.56 \u20130.60) 5.10 (5.02 \u20135.18)\n4 31.4% (30.4 \u201332.5) 96.9% (96.9 \u201397.0) 0.77% (0.74 \u20130.80) 7.07 (6.96 \u20137.17)\n5 17.3% (16.4 \u201318.2) 99.5% (99.5 \u201399.6) 2.81% (2.59 \u20133.06) 29.0 (28.6 \u201329.5)\n6 11.9% (11.2 \u201312.7) 99.9% (99.9 \u201399.9) 8.62% (7.42 \u201310.0) 96.0 (94.3 \u201397.6)\nPRISMLR 1 52.3% (51.1 \u201353.4) 86.2% (86.1 \u201386.4) 0.29% (0.28 \u20130.29) 2.22 (2.19 \u20132.25)\n2 46.4% (45.2 \u201347.6) 89.9% (89.7 \u201390.0) 0.35% (0.34 \u20130.36) 2.66 (2.61 \u20132.70)\n3 31.8% (30.7 \u201332.9) 95.4% (95.3 \u201395.5) 0.53% (0.50 \u20130.55) 4.06 (3.99 \u20134.12)\n4 26.4% (25.3 \u201327.4) 97.0% (96.9 \u201397.1) 0.66% (0.63 \u20130.69) 5.17 (5.09 \u20135.25)\n5 8.95% (8.30 \u20139.64) 99.6% (99.5 \u201399.6) 1.51% (1.36 \u20131.67) 13.1 (12.9 \u201313.3)\n6 2.93% (2.55 \u20133.35) 99.9% (99.9 \u201399.9) 2.88% (2.32 \u20133.56) 24.2 (23.7 \u201324.7)\nNumbers in brackets are 95% CI. PPV: Positive Predictive Value. SIR: Standardised Incidence Ratio. TrxPop. Est.: Estimation on the whole TriNetX pop ulation that accounts\nfor unbalanced sampling. All calculations were based on the outcome during the followup period of individual patients. Followup period was determin ed by the age and\nEHR data availability of each patient. More details can be found in Section 2.5 and in the Supplemental Material .",
        "f8118ae6-c772-42f9-a01f-9958c426526c": "PPV: Positive Predictive Value. SIR: Standardised Incidence Ratio. TrxPop. Est.: Estimation on the whole TriNetX pop ulation that accounts\nfor unbalanced sampling. All calculations were based on the outcome during the followup period of individual patients. Followup period was determin ed by the age and\nEHR data availability of each patient. More details can be found in Section 2.5 and in the Supplemental Material .\nTable 2: Simulated deployment results.Articles\n10 www.thelancet.com Vol 98 December, 2023",
        "de8ec535-1502-450e-bce5-097e39d0ef29": "may have biased presentation in our data because their\nsocioeconomic status limits their access to the health-\ncare system. Future research should further evaluate thefairness of Prism, particularly concerning underrepre-\nsented groups; (iii) Although TriNetX incorporates a\ndiverse set of US HCOs, they are still a small portion onthe global scale; future work should evaluate Prism on\nmore geographically diverse data. The lack of stand-\nardisation in data collection and the heterogeneity ofEHR systems may have impacted model generaliz-ability, as hinted by the PDAC prevalence differences\nand the performance drop in location-based validation;\nand (iv) Our study does not try to interpret the modelreasoning process or extract clinical knowledge from\nmodels. Future work should improve model interpret-\nability to make the decision process more reliable andtransparent.\nIn conclusion, we have built, validated, and simu-\nlated the deployment of PDAC risk prediction modelsfor the general population on multi-institutional EHR\ndata from a federated network. Prism models can be\nused to help primary care providers across the countryidentify high-risk individuals for PDAC screening orused as a \ufb01rst\ufb01lter before subsequent biomarker\ntesting. Both PrismNN and PrismLR maintained their\naccuracy across diverse racial groups and geographicregions in the US and over time, outperforming widely-\nused clinical guideline criteria\n2,3for PDAC screening\ninclusion.\nOur approach enables potential expansion of the\npopulation targeted for screening beyond the tradition-\nally screened minority with an inherited predisposition.Prism models set the stage for model deployment\nwithin the network to identify high-risk patients at\nmultiple institutions within the network. The next stepis a prospective study to validate the models before fullclinical deployment.\nContributors\nConceptualisation: LA, MR, KJ, SK. Data acquisition: KH, JW, KJ. Datacuration: KJ, MR, LA. Data veri \ufb01cation: KJ, MR, LA. Data interpretation:\nKJ, MR, LA, MP, IDK. Project administration: LA, MR, KH. Supervision:MR, LA, SK, MP. ALL writing review and editing. ALL approved pub-\nlished version and agreed to be accountable for all aspects of the work.\nData sharing statement\nThe de-identi \ufb01ed data in TriNetX federated network database can only\nbe accessed by researchers that are either part of the network or have a\ncollaboration agreement with TriNetX. We used data from the TriNetX\ndatabase under a no-cost collaboration agreement between BIDMC,MIT, and TriNetX. Under this agreement, we accessed de-identi \ufb01ed data\nunder the agreements and institutional approvals already in place be-tween TriNetX and their partner institutions. The data used in this studyare stored on Amazon S3 storage under a TriNetX account and could be\nshared with future researchers who establish a collaboration with\nTriNetX.\nDeclaration of interests\nKJ and MR are not aware of any payments or services, paid to them-selves or MIT, that could be perceived to in \ufb02uence the submitted work.\nIK and LA are not aware of any payments or services, paid to themselvesor BIDMC, that could be perceived to in \ufb02uence the submitted work.\nDuring the time the research was performed MR received consulting\nfees and payment for expert testimony for Comcast, Google, Motorola,Qualcomm, and IBM, is a member of the scienti \ufb01c advisory board and\nowns stock at Vali Cyber, and acknowledges support from Boeing,DARPA, and the NSF for salary and research support including meetingattendance and travel. MR has the following patents: United States\nPatent 10,539,419. Method and apparatus for reducing sensor power\ndissipation. Phillip Stanley- Marbell, Martin Rinard. United States Pat-ent 10,135,471. System, method, and apparatus for reducing powerdissipation of sensor data on bit-serial communication interfaces. Phil-lip Stanley-Marbell, Martin Rinard. United States Patent 9,189,254.Translating text to, merging, and optimizing graphical user interfacetasks. Nathaniel Kushman, Regina Barzilay, Satchuthananthavale Bra-\nnavan, Dina Katabi, Martin Rinard. United States Patent 8,839,221.\nAutomatic acquisition and installation of software upgrades for collec-tions of virtual machines.",
        "e66a6b8f-4593-49d3-aa0f-934c21232b1a": "MR has the following patents: United States\nPatent 10,539,419. Method and apparatus for reducing sensor power\ndissipation. Phillip Stanley- Marbell, Martin Rinard. United States Pat-ent 10,135,471. System, method, and apparatus for reducing powerdissipation of sensor data on bit-serial communication interfaces. Phil-lip Stanley-Marbell, Martin Rinard. United States Patent 9,189,254.Translating text to, merging, and optimizing graphical user interfacetasks. Nathaniel Kushman, Regina Barzilay, Satchuthananthavale Bra-\nnavan, Dina Katabi, Martin Rinard. United States Patent 8,839,221.\nAutomatic acquisition and installation of software upgrades for collec-tions of virtual machines. Constantine Sapuntzakis, Martin Rinard,Gautam Kachroo. United States Patent 8,788,884. Automatic correctionof program logic. Jeff Perkins, Stylianos Sidiroglou, Martin Rinard, EricLahtinen, Paolo Piselli, Basil Krikeles, Timothy Anderson, Greg Sulli-\nvan. United States Patent 7,260,746. Speci \ufb01cation based detection and\nrepair of errors in data structures. Brian Demsky, Martin Rinard. JW is aTriNetX employee and owns TriNetX stock. The remaining authorsdeclare no competing interests.\nAcknowledgements\nWe are grateful to Gadi Lachman and TriNetX for providing support and\nresources for this work, and to Lydia Gonz\u00e1lez for her help with iden-\ntifying and mitigating data quality issues. MR, LA, KJ acknowledge thecontribution of resources by TriNetX, including secured laptop com-puters, access to the TriNetX EHR database, and clinical, technical,legal, and administrative assistance from the TriNetX team of clinicalinformaticists, engineers, and technical staff. LA acknowledges support\nfrom the Prevent Cancer Foundation for this work. MR and KJ received\nfunding from DARPA and Boeing. MR also received funding from theNSF and Aarno Labs. SK, MP, JW, and KH are employees of TriNetX.\nAppendix A. Supplementary data\nSupplementary data related to this article can be found at https://doi.\norg/10.1016/j.ebiom.2023.104888 .\nReferences\n1 Siegel RL, Miller KD, Fuchs HE, Jemal A. Cancer statistics, 2022.\nCA Cancer J Clin . 2022;72:7 \u201333.\n2 Goggins M, Overbeek KA, Brand R, et al. Management of patients\nwith increased risk for familial pancreatic cancer: updated recom-\nmendations from the International Cancer of the Pancreas\nScreening (CAPS) consortium. Gut. 2020;69:7 \u201317.\n3 Daly MB, Pal T, AlHilli Z, et al. Genetic/familial high-risk assessment:\nbreast, ovarian, and pancreatic ; 2023. https://www.nccn.org/profes\nsionals/physician_gls/pdf/genetics_bop.pdf . Accessed January 21,\n2023.\n4 Aslanian HR, Lee JH, Canto MI. AGA clinical practice update on\npancreas cancer screening in high-risk individuals: expert review.\nGastroenterology . 2020;159:358 \u2013362.\n5 Lu C, Xu CF, Wan XY, Zhu HT, Yu CH, Li YM. Screening for\npancreatic cancer in familial high-risk individuals: a systematic\nreview. World J Gastroenterol . 2015;21:8678 .\n6 Canto MI, Harinck F, Hruban RH, et al. International cancer of the\npancreas screening (CAPS) consortium summit on the manage-ment of patients with increased risk for familial pancreatic cancer.\nGut. 2013;62:339 \u2013347.\n7 Humphris JL, Johns AL, Simpson SH, et al. Clinical and pathologic\nfeatures of familial pancreatic cancer. Cancer . 2014;120:3669 \u20133675 .\n8 Petersen GM. In: Familial pancreatic cancer. In: Semin Oncol 43.\n9 Owens DK, Davidson KW, Krist AH, et al. Screening for pancreatic\ncancer: US preventive services task force reaf \ufb01rmation recom-\nmendation statement.",
        "ba07f41b-0143-40d1-8afe-662d4fabdd4e": "World J Gastroenterol . 2015;21:8678 .\n6 Canto MI, Harinck F, Hruban RH, et al. International cancer of the\npancreas screening (CAPS) consortium summit on the manage-ment of patients with increased risk for familial pancreatic cancer.\nGut. 2013;62:339 \u2013347.\n7 Humphris JL, Johns AL, Simpson SH, et al. Clinical and pathologic\nfeatures of familial pancreatic cancer. Cancer . 2014;120:3669 \u20133675 .\n8 Petersen GM. In: Familial pancreatic cancer. In: Semin Oncol 43.\n9 Owens DK, Davidson KW, Krist AH, et al. Screening for pancreatic\ncancer: US preventive services task force reaf \ufb01rmation recom-\nmendation statement. JAMA . 2019;322:438 \u2013444.\n10 Baecker A, Kim S, Risch HA, et al. Do changes in health reveal the\npossibility of undiagnosed pancreatic cancer? development of a\nrisk-prediction model based on healthcare claims data. PLoS One .\n2019;14:e0218580 .Articles\nwww.thelancet.com Vol 98 December, 2023 11",
        "0d03c145-0507-4e65-83c5-3ca6f2df8e33": "11 Kim J, Yuan C, Babic A, et al. Genetic and circulating biomarker\ndata improve risk prediction for pancreatic cancer in the general\npopulation. Cancer Epidemiol Biomarkers Prev . 2020;29:999 \u20131008 .\n12 Klein AP, Lindstr\u00f6m S, Mendelsohn JB, et al. An absolute risk\nmodel to identify individuals at elevated risk for pancreatic cancerin the general population. PLoS One . 2013;8:e72311 .\n13 Chen Q, Cherry DR, Nalawade V, et al. Clinical data prediction\nmodel to identify patients with early-stage pancreatic cancer. JCO\nClin Cancer Inform . 2021;5:279 \u2013287.\n14 Placido D, Yuan B, Hjaltelin JX, et al. A deep learning algorithm to\npredict risk of pancreatic cancer from disease trajectories. Nat Med .\n2023;29:1 \u201310.\n15 Chen W, Zhou Y, Xie F, et al. Derivation and external validation of\nmachine learning-based model for detection of pancreatic cancer.\nAm J Gastroenterol . 2022;118(1):157 \u2013167.\n16 Appelbaum L, Cambronero JP, Stevens JP, et al. Development and\nvalidation of a pancreatic cancer risk model for the general popu-lation using electronic health records: an observational study. Eur J\nCancer . 2021;143:19 \u201330.\n17 Muhammad W, Hart GR, Nartowt B, et al. Pancreatic cancer prediction\nthrough an arti \ufb01cial neural network. Front Artif Intell . 2019;2:2 .\n18 Videha Sharma IA, van der Veer S, Martin G, Ainsworth J,\nAugustine T. Adoption of clinical risk prediction tools is limited by\na lack of integration with electronic health records. BMJ Health\nCare Inform . 2021;28:e100253 .\n19 Surveillance, epidemiology, and end results (SEER) program SEER*stat\ndatabase: incidence SEER research limited- \ufb01eld data, 22 registries, Nov\n2021 sub (2000-2019) - linked to county attributes time dependent\n(1990-2019) income/rurality, 1969-2020 counties, National Cancer\nInstitute, DCCPS, Surveillance Research Program, Released April\n2022, based on the November 2021 submission . 2022 .\n20 Topaloglu U, Palchuk MB. Using a federated network of real-world\ndata to optimize clinical trials operations. JCO Clin Cancer Inform .\n2018;2:1 \u201310.\n21 Jensen H, Vedsted P, M\u00f8ller H. Consultation frequency in general\npractice before cancer diagnosis in relation to the patient \u2019s usual\nconsultation pattern: a population-based study. Cancer Epidemiol .\n2018;55:142 \u2013148.\n22 Agniel D, Kohane IS, Weber GM. Biases in electronic health record\ndata due to processes within the healthcare system: retrospective\nobservational study. BMJ. 2018;361:k1479 .23 Jia K, Rinard M. Effective neural network L0 regularization with\nbinmask. arXiv . 2023 .\n24 Platt J. Probabilistic outputs for support vector machines and\ncomparisons to regularized likelihood methods. Adv Large Margin\nClassi \ufb01ers. 1999;10:61 \u201374.\n25 Steyerberg EW, Harrell FE. Prediction models need appropriate\ninternal, internal \u2013external, and external validation. J Clin Epidemiol .\n2016;69:245 \u2013247.\n26 Riley RD, Ensor J, Snell KI, et al. External validation of clinical\nprediction models using big datasets from e-health records orIPD meta-analysis: opportunities and challenges. BMJ. 2016;353:\ni3140 .\n27 Sperrin M, Riley RD, Collins GS, Martin GP. Targeted validation:\nvalidating clinical prediction models in their intended population\nand setting. Diagn Progn Res . 2022;6:24 .\n28 Porter N, Laheru D, Lau B, et al. Risk of pancreatic cancer in the\nlong-term prospective follow-up of familial pancreatic cancer kin-dreds. J Natl Cancer Inst . 2022;114:1681 \u20131688 .\n29 Steyerberg E. Clinical prediction models: a practical approach to\ndevelopment, validation, and updating.",
        "e73f60f1-fae4-44a1-8330-edaee28a39e6": "J Clin Epidemiol .\n2016;69:245 \u2013247.\n26 Riley RD, Ensor J, Snell KI, et al. External validation of clinical\nprediction models using big datasets from e-health records orIPD meta-analysis: opportunities and challenges. BMJ. 2016;353:\ni3140 .\n27 Sperrin M, Riley RD, Collins GS, Martin GP. Targeted validation:\nvalidating clinical prediction models in their intended population\nand setting. Diagn Progn Res . 2022;6:24 .\n28 Porter N, Laheru D, Lau B, et al. Risk of pancreatic cancer in the\nlong-term prospective follow-up of familial pancreatic cancer kin-dreds. J Natl Cancer Inst . 2022;114:1681 \u20131688 .\n29 Steyerberg E. Clinical prediction models: a practical approach to\ndevelopment, validation, and updating. In: Statistics for biology and\nHealth . 2nd ed. Springer International Publishing; 2019 .\n30 Sun X, Xu W. Fast implementation of DeLong \u2019s algorithm for\ncomparing the areas under correlated receiver operating charac-\nteristic curves. IEEE Signal Process Lett . 2014;21:1389 \u20131393 .\n31 DeLong ER, DeLong DM, Clarke-Pearson DL. Comparing the\nareas under two or more correlated receiver operating character-\nistic curves: a nonparametric approach. Biometrics . 1988;44:837 \u2013\n845.\n32 Clopper CJ, Pearson ES. The use of con \ufb01dence or \ufb01ducial limits\nillustrated in the case of the binomial. Biometrika . 1934;26:404 \u2013\n413.\n33 Biggerstaff B, Tweedie R. Incorporating variability in estimates of\nheterogeneity in the random effects model in meta-analysis. Stat\nMed. 1997;16:753\n\u2013768.\n34 Higgins JP, Thompson SG. Quantifying heterogeneity in a meta-\nanalysis. Stat Med . 2002;21:1539 \u20131558 .\n35 Klaveren D van, Steyerberg EW, Perel P, Vergouwe Y. Assessing\ndiscriminative ability of risk models in clustered data. BMC Med\nRes Methodol . 2014;14:1 \u201310.\n36 Rieke N, Hancox J, Li W, et al. The future of digital health with\nfederated learning. NPJ Digit Med . 2020;3:1 \u20137.Articles\n12 www.thelancet.com Vol 98 December, 2023"
    },
    "relevant_docs": {
        "fdefadc1-5386-4521-ba8c-b8415a7ee656": [
            "ad0820a3-36ce-4ce6-a82b-91d7ce31e98f"
        ],
        "b8b3aeb6-72f9-4aef-b91b-9b3e1b376ba3": [
            "ad0820a3-36ce-4ce6-a82b-91d7ce31e98f"
        ],
        "c9cff0af-315c-40c6-88b3-8057e9a738e3": [
            "3a53c86f-f433-4d57-96e3-b04b759e11f2"
        ],
        "ee1c1b2d-f0be-47d9-824e-669cab8fe6d6": [
            "3a53c86f-f433-4d57-96e3-b04b759e11f2"
        ],
        "3c7e673a-bbaa-4043-b4d5-f962e3016fd9": [
            "5af8e241-d861-434f-89d7-485c3cffe5ca"
        ],
        "a96db4a4-3b5d-43de-8847-2025ebefccc2": [
            "5af8e241-d861-434f-89d7-485c3cffe5ca"
        ],
        "800d2057-eff4-43ce-be1d-5b986312dce7": [
            "4a09f481-f161-407f-bbd7-0dfb14819534"
        ],
        "3c75daca-8f2d-4b40-93cc-651a4c1e3e01": [
            "4a09f481-f161-407f-bbd7-0dfb14819534"
        ],
        "f8337577-849f-40d5-b3db-09c064b69ff5": [
            "2d4fef38-cf38-41b3-a161-e82d1fde493f"
        ],
        "ed1f6c92-7780-4fb5-82b5-9c2dc2e86723": [
            "2d4fef38-cf38-41b3-a161-e82d1fde493f"
        ],
        "bb3dee80-5e01-4ba2-af5d-b3e8c2860eea": [
            "125bcbea-fcb5-444e-b9c3-d2fa17f08e8b"
        ],
        "af127072-d920-404b-93b2-b8c99903b99c": [
            "125bcbea-fcb5-444e-b9c3-d2fa17f08e8b"
        ],
        "5d88b6c0-10ce-478f-a391-d3d2bcd60a77": [
            "9ddf8cec-22a0-4d7c-8961-b832457b15af"
        ],
        "b7a634a6-5b2b-4c96-855e-d4bde37ea318": [
            "9ddf8cec-22a0-4d7c-8961-b832457b15af"
        ],
        "1cd09ee7-8f41-4e94-a00b-3e94c8dce05d": [
            "98c84fb2-6a19-4536-92b7-38302cf271cc"
        ],
        "77594f0e-5efb-46c2-bb2d-1106356c0c6b": [
            "98c84fb2-6a19-4536-92b7-38302cf271cc"
        ],
        "ab3c4db3-8770-419c-9afb-852bc6efa1b5": [
            "7fae7150-f8ee-42fc-8e21-3df8c8651507"
        ],
        "b7d1e16b-dbcf-4f60-a4d2-314429a8e51b": [
            "7fae7150-f8ee-42fc-8e21-3df8c8651507"
        ],
        "48d0f02e-5861-41b5-8e7f-f5a6e00dac3f": [
            "d8bb0b8f-3e92-4c4a-a5ec-6cb525542db5"
        ],
        "9cf9828c-bde1-408a-b96e-6e9b19413daf": [
            "d8bb0b8f-3e92-4c4a-a5ec-6cb525542db5"
        ],
        "fddb491f-99e9-42a6-ac1b-85f9237a464a": [
            "d1f3111c-ff3f-4d93-94d5-9de2d76511a8"
        ],
        "daeef8a1-4a74-464a-be97-f1cc6fdfa0a5": [
            "d1f3111c-ff3f-4d93-94d5-9de2d76511a8"
        ],
        "dc694428-f64b-4a95-b5f6-06e152f294a2": [
            "66a9edc0-bdd9-4ce1-8d0f-42b9401e5f8f"
        ],
        "aa627c43-43cb-4d50-ae15-f3fda18b7715": [
            "66a9edc0-bdd9-4ce1-8d0f-42b9401e5f8f"
        ],
        "579352dc-625b-4e1b-a9ad-12be9698a63b": [
            "fe76f48e-e823-47d7-8949-dab04bc3bbdc"
        ],
        "df50194a-1c8a-41cb-9ca2-4261122c8443": [
            "fe76f48e-e823-47d7-8949-dab04bc3bbdc"
        ],
        "741870b4-aee2-4cac-a611-7a3309f6d95d": [
            "ac4d0ec7-c007-43ed-b961-45936ce014b8"
        ],
        "a39ccc99-a274-45c2-b963-99f315f430e8": [
            "ac4d0ec7-c007-43ed-b961-45936ce014b8"
        ],
        "05e0217a-d6a5-48e6-8de8-8b39cd431dab": [
            "7cc9f460-b210-4f15-ae73-d60115574341"
        ],
        "cd85269a-555e-4a8a-afc3-87d6dbba56ff": [
            "7cc9f460-b210-4f15-ae73-d60115574341"
        ],
        "29bff076-1264-46a9-9a9f-9287238af717": [
            "a797a259-8746-4df1-bc0d-276acf88869a"
        ],
        "11cb276f-671c-469a-a788-8e39cdba5f1f": [
            "a797a259-8746-4df1-bc0d-276acf88869a"
        ],
        "0b463137-7387-4072-b922-03a4f21466ec": [
            "ef44c4f0-f234-43b5-b975-77014638fbb4"
        ],
        "fbad2c5d-42fe-4612-b9bb-9158b3715275": [
            "ef44c4f0-f234-43b5-b975-77014638fbb4"
        ],
        "b603a3d3-f18a-4986-a740-305f0a26c5ae": [
            "f4574399-2533-473b-8a62-0d7856fa30d3"
        ],
        "bfd2d060-2f1e-4322-b8ad-8f34495918f7": [
            "f4574399-2533-473b-8a62-0d7856fa30d3"
        ],
        "58728f8a-a122-4929-ab8c-1719a7ea8b64": [
            "11ea6212-cef2-47ed-a9e5-ab9a40959bbf"
        ],
        "ed37dfe2-46ca-4005-96ec-8b58e1b1467f": [
            "11ea6212-cef2-47ed-a9e5-ab9a40959bbf"
        ],
        "0c9555da-dcc9-4f19-9955-970eb520a1aa": [
            "a6ae670b-3b61-47b2-adf5-8929e1b27a42"
        ],
        "e29c8074-925e-4a34-be8b-8cd75830ba4a": [
            "a6ae670b-3b61-47b2-adf5-8929e1b27a42"
        ],
        "29c31df6-4f41-4510-bdda-672b2b5f0d5f": [
            "e19ae042-3f4e-4731-9143-a0af3d0f7434"
        ],
        "972fd9ff-2eda-4b97-bccf-41d4749df1f7": [
            "e19ae042-3f4e-4731-9143-a0af3d0f7434"
        ],
        "cb52868d-bd4e-40a6-8a53-4b7a60118ba3": [
            "cd80a54b-528b-4d47-8350-564d8885da94"
        ],
        "10b32b48-2b63-439c-b56d-22506e45913a": [
            "cd80a54b-528b-4d47-8350-564d8885da94"
        ],
        "6b3ce94b-e7fb-4b69-a4ce-850f19306c7b": [
            "da288752-6dd6-4aae-b1d1-842263d955f9"
        ],
        "83d81df1-08ce-4a4f-91bc-70f9eda8cb4a": [
            "da288752-6dd6-4aae-b1d1-842263d955f9"
        ],
        "00711dd8-93b3-4ec7-be8f-0e82b4751674": [
            "0feeb27b-12dd-4891-8494-22ffe621365a"
        ],
        "9ab6089f-6ab2-4f75-ad41-7e7ed100da40": [
            "0feeb27b-12dd-4891-8494-22ffe621365a"
        ],
        "51e8b732-c5bf-4fec-b5ef-dfaac4f61fce": [
            "a92d0f0a-1ad0-4808-bf9e-c0b1f170fa1c"
        ],
        "5cc70a3c-a5b6-4b32-9427-b7a79685ba73": [
            "a92d0f0a-1ad0-4808-bf9e-c0b1f170fa1c"
        ],
        "86fdcc1f-3c9c-43c4-9a6e-ff7bf6464725": [
            "eb796704-570f-414b-9fbe-7b50ea54d8c7"
        ],
        "4fc436b0-7301-460c-a151-e047c261c876": [
            "eb796704-570f-414b-9fbe-7b50ea54d8c7"
        ],
        "e785b065-2d2d-4ed7-ab38-4ab7c554dcce": [
            "7e5bcc74-1b87-4ae6-9940-a4e174c6217a"
        ],
        "fc65f087-a9f9-488f-8146-f72d8e2904fd": [
            "7e5bcc74-1b87-4ae6-9940-a4e174c6217a"
        ],
        "d8210816-1758-4612-af3f-ce2e9f66166d": [
            "28437284-98b4-4530-b8ec-7a51e472c1d8"
        ],
        "59517675-877b-4126-bfe7-ae7b7379bca2": [
            "28437284-98b4-4530-b8ec-7a51e472c1d8"
        ],
        "68bf9842-5653-44bc-96f5-75e153ab5ed7": [
            "350096e9-f112-4a76-8ebc-6caa181ea212"
        ],
        "6e0df301-604b-4926-8b9c-e85b3bb11333": [
            "350096e9-f112-4a76-8ebc-6caa181ea212"
        ],
        "fd3eb574-9d9c-4b89-910f-a1995785c2b0": [
            "4749cc9a-f6e9-4f02-94c8-dd3ff56cbc39"
        ],
        "0fb4f0a8-695f-4fa5-827c-426162a91338": [
            "4749cc9a-f6e9-4f02-94c8-dd3ff56cbc39"
        ],
        "bd565145-9af9-482b-9b5d-21128c746f70": [
            "d861eb80-ed55-4c34-9db8-80afaea62b4f"
        ],
        "0c4fd24b-3152-4ffa-b6ca-5f6c8ff4f40b": [
            "d861eb80-ed55-4c34-9db8-80afaea62b4f"
        ],
        "dbe7f47f-2b3f-486c-ac84-ea593bee0a7a": [
            "c19aad16-c753-4a08-b20e-06f91da13fb8"
        ],
        "c9b542ab-a5a0-4310-a578-f7e72dbf87ad": [
            "c19aad16-c753-4a08-b20e-06f91da13fb8"
        ],
        "33e49c56-578b-4fcc-8f3a-67f1cf71ac7b": [
            "4c70e550-2cd0-4c75-a9e9-4a70e79ccb4e"
        ],
        "17d9c38c-c79e-4d6c-9330-7b5e71c28b73": [
            "4c70e550-2cd0-4c75-a9e9-4a70e79ccb4e"
        ],
        "ca5f01dd-1a6e-4822-9027-a1214e6983ee": [
            "1969c276-c59a-472a-a7ec-a007d89518d5"
        ],
        "4715a883-a7f7-42a7-8b6f-5567e0daf660": [
            "1969c276-c59a-472a-a7ec-a007d89518d5"
        ],
        "ef731992-9c64-4cdc-b7de-9944569dd444": [
            "eacc01be-6be5-4f4f-af3c-325b8c100d53"
        ],
        "dfa26bb0-15e7-48f6-99be-cfc9083aecc2": [
            "eacc01be-6be5-4f4f-af3c-325b8c100d53"
        ],
        "d6c2c799-6d89-414f-b1bc-7503eb408f3a": [
            "4df2bb47-325c-415c-887e-dccb946ceaa5"
        ],
        "5b19913a-98dd-4041-ace0-661c622a5caa": [
            "4df2bb47-325c-415c-887e-dccb946ceaa5"
        ],
        "b09bb4e8-2254-4d35-aec6-819e128b3e5e": [
            "c4c0067a-4c1c-4f02-8e82-0fb2e54d2d2b"
        ],
        "ce4721cc-c4ff-4c3b-9052-44c575ecd297": [
            "c4c0067a-4c1c-4f02-8e82-0fb2e54d2d2b"
        ],
        "3aab7f9b-0399-4ed1-9bc4-cdc18522b074": [
            "c1bf3771-b77b-43ec-a7e9-1b4a82857500"
        ],
        "df9e7150-a0c3-4a8f-8568-8cb458a08311": [
            "c1bf3771-b77b-43ec-a7e9-1b4a82857500"
        ],
        "eaa8f55e-7537-40cc-94ff-dc2f2005b4f5": [
            "1b5996f8-6bbc-40d2-8f6e-a8dd4e57bd16"
        ],
        "32c28d4e-4219-44c9-9835-c8354236859f": [
            "1b5996f8-6bbc-40d2-8f6e-a8dd4e57bd16"
        ],
        "a83c0429-4cff-4db4-8f10-1795bcbc1f01": [
            "64fc6885-478b-4e1d-9c04-c18bbe7e255f"
        ],
        "ae16ee96-78d1-4f52-87ae-052313bc2541": [
            "64fc6885-478b-4e1d-9c04-c18bbe7e255f"
        ],
        "d4ab33a8-73c1-4304-bf9e-51238441cdbe": [
            "e6496769-240c-44bc-a1c7-977c6e782069"
        ],
        "27fc4886-9192-4896-94c0-627dad7c3df7": [
            "e6496769-240c-44bc-a1c7-977c6e782069"
        ],
        "87068e7f-3f5e-4411-a231-5d324ee90153": [
            "ae46e6d8-fae0-4e93-bf46-f613b08adcd8"
        ],
        "5e7c6ff4-b9d8-43e2-bd06-630206386d41": [
            "ae46e6d8-fae0-4e93-bf46-f613b08adcd8"
        ],
        "813709cb-da5d-4197-9ed0-fc8c91f48760": [
            "18533a07-c0c6-44c1-b4c4-1cc5bd2e42f0"
        ],
        "c52910bf-b22a-4d20-92f8-f408a6651e2d": [
            "18533a07-c0c6-44c1-b4c4-1cc5bd2e42f0"
        ],
        "27f8c9cc-6a90-434a-ab1c-36e2cf25c94d": [
            "e4a938d4-503a-407f-8567-bcc5491c4916"
        ],
        "4cedc87a-d136-4594-807b-db528ca04729": [
            "e4a938d4-503a-407f-8567-bcc5491c4916"
        ],
        "e143f232-95bf-4a0b-9769-418bca253231": [
            "e32d365c-0ddb-491a-b285-cfc004b0c6c6"
        ],
        "14d5c7d6-ba30-48be-a663-dc76bd18f599": [
            "e32d365c-0ddb-491a-b285-cfc004b0c6c6"
        ],
        "73f0b1e4-542c-4bdf-a83d-5e1920981973": [
            "9ab8dfec-12a7-4742-92d2-0fcd88750b4e"
        ],
        "e7f70748-c2a0-4d52-801b-4a592184d9cc": [
            "9ab8dfec-12a7-4742-92d2-0fcd88750b4e"
        ],
        "95b66e27-08e3-4f72-9d70-8620dc50044a": [
            "65944e7b-99f7-4e0a-b58c-5e9115909c4d"
        ],
        "d90cf5a2-8c5a-4851-88ac-038800279253": [
            "65944e7b-99f7-4e0a-b58c-5e9115909c4d"
        ],
        "953ff682-9577-4ad8-96af-6acc94e7adcc": [
            "46c6f51a-bc22-4758-b7fe-d667e8c8dd50"
        ],
        "635be117-1243-4b31-80e0-6796ae87b835": [
            "46c6f51a-bc22-4758-b7fe-d667e8c8dd50"
        ],
        "f727c619-fc05-4664-9433-d3e0df045072": [
            "381b1008-d905-4858-84ec-797cba406947"
        ],
        "85316968-6d3c-4985-850c-0882cd4f60b9": [
            "381b1008-d905-4858-84ec-797cba406947"
        ],
        "67100f99-ffdb-4869-a65e-b955c6d4081f": [
            "38289252-26d3-4efd-a1dc-37106068250f"
        ],
        "c3dbb3de-0b2c-429b-94a8-6b8decad05af": [
            "38289252-26d3-4efd-a1dc-37106068250f"
        ],
        "796e97bd-fea9-46f1-8c25-f587d8674f61": [
            "c2abb8b9-ca59-4818-8636-260f01569950"
        ],
        "ddb439f8-c48c-4623-8332-3cfe95d0464b": [
            "c2abb8b9-ca59-4818-8636-260f01569950"
        ],
        "9d4e7ba4-ccba-40b8-be30-bae7e39710bf": [
            "e8658ec0-21d8-46ae-ae79-033afd994681"
        ],
        "4b6468de-9d77-48e6-86c1-f0ea749bb075": [
            "e8658ec0-21d8-46ae-ae79-033afd994681"
        ],
        "a9fe7ab0-89f6-4c7b-b1da-b7af8c5317ab": [
            "1f0a582d-41ed-498b-b21e-ec2e46407f09"
        ],
        "7aecad5b-40ef-430e-8255-fdea74c6b975": [
            "1f0a582d-41ed-498b-b21e-ec2e46407f09"
        ],
        "01a60666-dcfc-46d2-9d76-100287dd0cd8": [
            "677021b9-cf78-4c34-b2dc-8f465a669d7c"
        ],
        "62972067-e827-4f76-9bd0-9aeceef7b778": [
            "677021b9-cf78-4c34-b2dc-8f465a669d7c"
        ],
        "4659ddce-7893-47aa-8908-60508b93597a": [
            "9b80ea3b-88dc-40da-aa42-7c1127e524c2"
        ],
        "d5335fa5-f87c-4841-ad5f-142be63ce890": [
            "9b80ea3b-88dc-40da-aa42-7c1127e524c2"
        ],
        "ee8f76f6-9c44-4d42-9174-b4a14a121d41": [
            "a180838b-3bf9-4993-8ae6-2310aa2aec01"
        ],
        "d397d07e-41c8-442b-9399-de16696081c1": [
            "a180838b-3bf9-4993-8ae6-2310aa2aec01"
        ],
        "ed997b97-4adc-4e78-9d43-6cc77f0d5294": [
            "f272f7b2-a641-4f56-b79d-cd71d256c9b6"
        ],
        "8a2c84cf-9282-417d-a43f-31967bf382b9": [
            "f272f7b2-a641-4f56-b79d-cd71d256c9b6"
        ],
        "5b7d59e7-1b91-48a4-8cd4-9d309bd1b28a": [
            "e149ab30-b27a-4f0d-ba7a-e0add92e1af2"
        ],
        "27d6b506-1801-4701-a22a-9ec71236c656": [
            "e149ab30-b27a-4f0d-ba7a-e0add92e1af2"
        ],
        "39ad5a50-c794-4018-a27a-e1e2754254bc": [
            "e101c59b-ea5f-4c4a-8454-7d4e46d28caa"
        ],
        "593503b0-7fdf-4666-84bb-7364d12aafd7": [
            "e101c59b-ea5f-4c4a-8454-7d4e46d28caa"
        ],
        "8dad0839-c81c-4e78-925d-789233fe4ba9": [
            "4fe113ed-5d29-477f-ab05-941c24f26f52"
        ],
        "f180f098-3ce3-4b27-b19b-b2566d182db9": [
            "4fe113ed-5d29-477f-ab05-941c24f26f52"
        ],
        "22baa103-1278-4567-9615-16cfb6c8ff6b": [
            "16eef30e-7e55-4fca-a62e-8ddc66294470"
        ],
        "a2350142-2c74-4c23-b0df-b059b62c2906": [
            "16eef30e-7e55-4fca-a62e-8ddc66294470"
        ],
        "77cd36a0-5a4c-4e2d-8efc-4dc916327978": [
            "58112ae4-cade-4216-8e09-9b3ecd1461f8"
        ],
        "d337c508-08f3-4c34-80a7-f46e2e1db79c": [
            "58112ae4-cade-4216-8e09-9b3ecd1461f8"
        ],
        "f9fa1b53-cf38-4429-b844-d04bcd19e50a": [
            "51013c3f-abc1-40ff-a39c-8ffdcb4926fb"
        ],
        "f7098e06-76d3-4433-b38c-fd99a91f0e28": [
            "51013c3f-abc1-40ff-a39c-8ffdcb4926fb"
        ],
        "4218c815-44c8-40d9-8ec6-c6be37407eb6": [
            "d1b65255-e250-4003-98a2-262d70718bca"
        ],
        "c976af23-9e31-4be6-80fb-e18de53f2887": [
            "d1b65255-e250-4003-98a2-262d70718bca"
        ],
        "f3819fb5-eb88-481a-8be7-62538c4f79a1": [
            "3271b6cc-6c1d-4996-8c7d-d7ee2c5efb43"
        ],
        "3ef4b6d8-6407-4269-8f39-596f968914c7": [
            "3271b6cc-6c1d-4996-8c7d-d7ee2c5efb43"
        ],
        "8fd16141-14b8-426e-b6bd-562d7fe21291": [
            "28908a22-06c7-4379-8d5e-8e6236248d0b"
        ],
        "7c27536c-8213-466d-bfdc-43e6bb807eae": [
            "28908a22-06c7-4379-8d5e-8e6236248d0b"
        ],
        "b6615ff5-0a31-41f2-8e48-01aa8478d62f": [
            "173be34c-00a0-4786-ac3c-119f74f67480"
        ],
        "72326ffd-4744-48be-b1e4-93c3fdcb0c66": [
            "173be34c-00a0-4786-ac3c-119f74f67480"
        ],
        "20909419-bd1e-4fc8-a6e7-089162987cba": [
            "b694e288-1ca0-42d0-a380-3104a8a8bd21"
        ],
        "7fdf6387-5ead-467e-8ca1-c82038568de8": [
            "b694e288-1ca0-42d0-a380-3104a8a8bd21"
        ],
        "1fe765e3-d2ab-493a-bea8-de4628bcc420": [
            "54a98303-fa89-43f1-9ad1-57d2ca3e23d7"
        ],
        "e6b2a542-bc93-4c58-8edf-b017bdb0b39b": [
            "54a98303-fa89-43f1-9ad1-57d2ca3e23d7"
        ],
        "f96cf066-8346-4e7c-9909-57f14162b202": [
            "a682143c-e463-4e63-9528-0f1a874edd66"
        ],
        "49573868-aca3-4b9f-a047-cf167fd7c6a2": [
            "a682143c-e463-4e63-9528-0f1a874edd66"
        ],
        "40be646b-ecea-4086-adf7-f6ed8d81f308": [
            "349ccb5b-1f33-47ca-b61a-74f429cda664"
        ],
        "bd98f058-2358-4668-9580-76e3a3ffa308": [
            "349ccb5b-1f33-47ca-b61a-74f429cda664"
        ],
        "79170a04-72a1-4bdd-8f51-8c7e6639f841": [
            "2001f915-9d36-48ca-bd23-bc310d981532"
        ],
        "e655b2c1-5151-42ae-84d7-8c73b499b932": [
            "2001f915-9d36-48ca-bd23-bc310d981532"
        ],
        "78602528-d0c5-4ca4-90ae-97ba8571a8dc": [
            "56ee6367-46c2-4022-bd99-30077057b628"
        ],
        "8724a663-3cfa-4c0b-a393-d353a4484e08": [
            "56ee6367-46c2-4022-bd99-30077057b628"
        ],
        "587d88b6-2dc0-44d5-a215-df5fdc142b9b": [
            "4456ee43-d378-4af8-bb9a-2d54e4a4d079"
        ],
        "adac1e31-bc72-4169-828d-b4d9411a1e6d": [
            "4456ee43-d378-4af8-bb9a-2d54e4a4d079"
        ],
        "8f4b623c-5aed-4b0f-9f22-46c29faaa33d": [
            "c0502ce5-fc22-4f7f-be7f-184a9b9dc31f"
        ],
        "58f5eced-1312-4fab-a942-ba1592662144": [
            "c0502ce5-fc22-4f7f-be7f-184a9b9dc31f"
        ],
        "e2a07764-a98b-4ba0-a296-589bd8b7b150": [
            "25a56e53-93c6-45a3-9ec8-30a323466028"
        ],
        "118d1856-990c-4e5b-96c7-64749e8caa47": [
            "25a56e53-93c6-45a3-9ec8-30a323466028"
        ],
        "a4210546-5b8b-44a6-a063-8256af1ce2da": [
            "c3f0ce8f-a48c-44a1-be04-2bf6fc64faaf"
        ],
        "2c7de4f4-a274-4448-8828-2aaea284777f": [
            "c3f0ce8f-a48c-44a1-be04-2bf6fc64faaf"
        ],
        "e0d2bc5d-55b3-4970-be46-dd9993ad8b9c": [
            "eb11a42b-995e-420a-8fee-8005ae056d25"
        ],
        "23b815c7-e279-458f-8ef5-0714217210f7": [
            "eb11a42b-995e-420a-8fee-8005ae056d25"
        ],
        "861187fc-6477-435c-9ea6-31da640b81cd": [
            "ff596b04-a6f1-4167-a8a2-70e4cc22c002"
        ],
        "9c6f9d50-2f42-47b0-a77f-75c535241729": [
            "ff596b04-a6f1-4167-a8a2-70e4cc22c002"
        ],
        "bb6ab0f9-cb6f-489b-b008-5573b750e563": [
            "db3d9e57-f35c-4393-b4a9-a668693e8deb"
        ],
        "aa2e8c51-c218-4c4b-baf7-09d2b928a09e": [
            "db3d9e57-f35c-4393-b4a9-a668693e8deb"
        ],
        "9aaf1708-2f9e-4fc0-9d9c-f813911b3a7b": [
            "e1ebe309-db45-4dc3-9937-a58e6b6b3cb8"
        ],
        "66280694-34b1-4f7d-9ae5-016f0c7dfe87": [
            "e1ebe309-db45-4dc3-9937-a58e6b6b3cb8"
        ],
        "a5f97d4d-062d-4467-bdb3-1a12f0f51f1f": [
            "39795960-cc3e-484e-b76c-e98cc3b8fcd0"
        ],
        "661ecc31-6540-44e3-bc85-73318a46365e": [
            "39795960-cc3e-484e-b76c-e98cc3b8fcd0"
        ],
        "2529befc-a624-4701-822b-2db69560ea48": [
            "d9f61872-3e64-41ee-99b7-aa02adc98929"
        ],
        "1f15780c-8a1c-41be-977f-e60518c00303": [
            "d9f61872-3e64-41ee-99b7-aa02adc98929"
        ],
        "05f205fb-79e2-40ed-a9e3-d39e35d94311": [
            "e55be5f6-f8fd-42a1-82cb-c1bbbab68fb2"
        ],
        "404e0251-7537-430b-9fa8-82d9c0b205e1": [
            "e55be5f6-f8fd-42a1-82cb-c1bbbab68fb2"
        ],
        "fb1010a5-ada9-49f6-8cde-d727a4c9f0a8": [
            "eacaaa76-7d88-41b4-8f4c-520c3d839acb"
        ],
        "f1280492-6191-4782-b20a-f0005102cccf": [
            "eacaaa76-7d88-41b4-8f4c-520c3d839acb"
        ],
        "ae3cd914-f6dc-43d3-9398-ac480f7f75d4": [
            "7bc5a5a6-08d1-4746-8247-a1c7a617519c"
        ],
        "adf72a86-544a-44d5-ba59-c5d3edea1c87": [
            "7bc5a5a6-08d1-4746-8247-a1c7a617519c"
        ],
        "6e73562f-20c7-4286-8974-e9633c42a6f3": [
            "43a28897-e3d9-4796-95b2-58dd9e7d9554"
        ],
        "535c11f1-b2a4-4fc6-85d4-f94debf9873b": [
            "43a28897-e3d9-4796-95b2-58dd9e7d9554"
        ],
        "dd005e44-87c9-4fe3-9af0-67a88f7d0173": [
            "920b16c9-fc64-4232-bd02-d34d064df8b1"
        ],
        "31257999-ce86-4a7d-9971-4057985fea01": [
            "920b16c9-fc64-4232-bd02-d34d064df8b1"
        ],
        "c4f5ddf2-ad61-4cf7-ad36-e2087c885820": [
            "12b54ac9-6c5c-4a58-b670-ded0db26d79b"
        ],
        "0a183e86-0499-4a66-9511-bc002981e23d": [
            "12b54ac9-6c5c-4a58-b670-ded0db26d79b"
        ],
        "27c75eb8-3e91-4f30-88b0-0c465f76ba37": [
            "c2cb04e1-2717-4651-b46b-93062a5c54c4"
        ],
        "59a7b1f2-6eb0-4f42-a490-98127b2ff364": [
            "c2cb04e1-2717-4651-b46b-93062a5c54c4"
        ],
        "856322f1-6eeb-4a26-836b-df557d780aef": [
            "870c085d-0179-4765-9f2c-2fbae662ddbd"
        ],
        "1a1ffe43-f73d-4346-8e18-b67a9ba367e4": [
            "870c085d-0179-4765-9f2c-2fbae662ddbd"
        ],
        "41bdf854-9c81-4841-a5f7-10e1c34e2cc8": [
            "7fb3c980-4308-4e5b-bb3f-449ffe430e3d"
        ],
        "448faaa6-a69c-4bb6-8261-87f944d229ea": [
            "7fb3c980-4308-4e5b-bb3f-449ffe430e3d"
        ],
        "a6f047c9-55c6-49d2-9130-cb29bb8617b6": [
            "a606df13-973b-470f-8e9b-c45dd9d78e57"
        ],
        "8073cd55-23a1-459e-b2b2-e05eca344658": [
            "a606df13-973b-470f-8e9b-c45dd9d78e57"
        ],
        "6b7c95cb-60ca-417d-b4e0-ac4b8015d9f3": [
            "9deb4fdd-fb8c-4a24-b0a6-99613a54fab8"
        ],
        "73b41da3-9982-40f6-94ad-bc4ac1dab7da": [
            "9deb4fdd-fb8c-4a24-b0a6-99613a54fab8"
        ],
        "21246a5c-77a6-4e7b-b8dc-efe7f6c795cc": [
            "8e4ef6f9-7bf9-4a10-959b-33a7d6dbb29c"
        ],
        "f4342617-f76b-48fe-ab3a-9dddb3aae3c0": [
            "8e4ef6f9-7bf9-4a10-959b-33a7d6dbb29c"
        ],
        "b28ce2be-327d-4453-9f88-a82a2bcc76a9": [
            "74123ac3-9c0e-4d81-aeec-05e28a1977c4"
        ],
        "1497b3f9-18d4-4f66-bb12-fbb6543aefb9": [
            "74123ac3-9c0e-4d81-aeec-05e28a1977c4"
        ],
        "4296cf94-345e-465e-9ba1-3c242d7dd1e9": [
            "b57119a9-3197-44be-b12e-5bda13be3681"
        ],
        "4aef2652-c1f8-4af8-a79c-5001cbbbb99c": [
            "b57119a9-3197-44be-b12e-5bda13be3681"
        ],
        "9b5eddf9-28bc-4c5d-96c6-8472169e5ce1": [
            "4ea1c1ee-ba77-4459-beb3-68e9f3307b16"
        ],
        "97b1d459-a3fe-4f9c-8bbc-564800ed2c77": [
            "4ea1c1ee-ba77-4459-beb3-68e9f3307b16"
        ],
        "fb691a43-1cd5-41b3-ab15-efe87ed4f522": [
            "8d51d153-db7b-4254-af00-ab5f22d2ffe2"
        ],
        "8075d975-2185-43d7-aab9-b628b1108316": [
            "8d51d153-db7b-4254-af00-ab5f22d2ffe2"
        ],
        "c145509d-30ff-40c1-a5ad-1fad73f01445": [
            "fc831167-d308-4693-9c0f-2ac64738ce4d"
        ],
        "ee902dd9-5c4c-4348-9a2f-e5df82f7cf07": [
            "fc831167-d308-4693-9c0f-2ac64738ce4d"
        ],
        "e53e4dfe-5289-43e1-b5d6-8cbcfabffb47": [
            "d010d848-e7ee-474d-a3a2-1b44684c5222"
        ],
        "2c38636b-8cc0-41e5-8f2a-111fe346992b": [
            "d010d848-e7ee-474d-a3a2-1b44684c5222"
        ],
        "91d0419e-c453-4393-910f-57e4367bd543": [
            "32d17fd3-efcd-40e6-a393-01bc0af95d1a"
        ],
        "6a6a2716-8b81-4f5e-9180-ad4b61019617": [
            "32d17fd3-efcd-40e6-a393-01bc0af95d1a"
        ],
        "554d2f45-27c5-45ec-89ca-87755c4d3d69": [
            "6a10ad7f-afdb-4e38-9bcc-8252cb65d638"
        ],
        "cd4ed145-c713-402d-915c-f7e856a423df": [
            "6a10ad7f-afdb-4e38-9bcc-8252cb65d638"
        ],
        "615236f6-b353-4e85-86e7-38ea535b722a": [
            "1393ac3c-d923-4a2c-8251-37371ffb10b4"
        ],
        "b76787bc-48d6-4ba0-a407-aa32d5a08178": [
            "1393ac3c-d923-4a2c-8251-37371ffb10b4"
        ],
        "cef3bb2f-7723-4a20-96de-6ac9e08eb664": [
            "7831e7ff-57f1-438d-92d5-133bb6b73972"
        ],
        "3c5f3cf3-9c02-4bfa-ba51-3d99d073fffa": [
            "7831e7ff-57f1-438d-92d5-133bb6b73972"
        ],
        "38bfeef8-49f4-435c-966a-b18b5afadd40": [
            "9ada469b-0250-4f68-ad80-df322a2b2dfd"
        ],
        "8f1b552e-3f4b-4986-8617-9e7b718def3c": [
            "9ada469b-0250-4f68-ad80-df322a2b2dfd"
        ],
        "104b2359-451c-42ef-80cd-079f03fb9541": [
            "e51df1c9-6248-4021-927b-10d693e307a5"
        ],
        "c1b9f9d0-33b0-4c49-9a6c-4261ac012304": [
            "e51df1c9-6248-4021-927b-10d693e307a5"
        ],
        "8af400c0-2b18-4e8c-93ac-678fc7d13d9f": [
            "05653326-8a1c-434b-8b19-9ed5736a073b"
        ],
        "886f777f-83b4-42bf-a30d-17627aad74f8": [
            "05653326-8a1c-434b-8b19-9ed5736a073b"
        ],
        "64dab92e-6978-4b7d-af22-e19883f159cd": [
            "686ff445-f163-491d-9ad1-5e39c22cd460"
        ],
        "7f3b88d2-b084-4805-b9c9-dd66159ea757": [
            "686ff445-f163-491d-9ad1-5e39c22cd460"
        ],
        "3cfaa3ef-a627-41a8-b586-1f79ac63da4e": [
            "3ae524ed-d322-41f8-b292-292bb1eaa39c"
        ],
        "90d0b3f5-7515-4a82-95c8-9147abd45068": [
            "3ae524ed-d322-41f8-b292-292bb1eaa39c"
        ],
        "c23cbf8e-18a6-4538-a6a5-da690dfb09ba": [
            "13c0e0b9-114f-4f14-82ea-d0d43d1cbf4b"
        ],
        "10d73967-4101-4e6e-9b31-00d124ae3fbd": [
            "13c0e0b9-114f-4f14-82ea-d0d43d1cbf4b"
        ],
        "4cb83885-0c84-41b6-a3c9-1e83afc8b62e": [
            "d24cfd3b-8f7f-400c-a9ea-bb643453ab90"
        ],
        "84394d90-965f-47ad-8757-fcb3041dd731": [
            "d24cfd3b-8f7f-400c-a9ea-bb643453ab90"
        ],
        "e136b2d5-912d-496e-809e-4d7f9080db0d": [
            "78a90d0f-40fc-413f-aec5-2880a0af819c"
        ],
        "90408559-9f2b-4c81-a4d6-9ad455389f37": [
            "78a90d0f-40fc-413f-aec5-2880a0af819c"
        ],
        "e1498ea3-1847-443d-bcfa-3f92cdaee0f2": [
            "a3ea05b8-a178-4328-84d1-d591ca6200b7"
        ],
        "dafafca3-25e4-4023-9f01-ad1c95e33ab0": [
            "a3ea05b8-a178-4328-84d1-d591ca6200b7"
        ],
        "e43688ce-d2d8-4b0f-adbe-06afcee89b02": [
            "6ee9da30-54cf-47fb-a278-70d7b5cc193e"
        ],
        "bc8a59c9-5ba9-472c-aed5-8d871cc5d117": [
            "6ee9da30-54cf-47fb-a278-70d7b5cc193e"
        ],
        "93fd5596-d893-4614-8089-5d31a3a0a45d": [
            "0fce7844-5392-43b7-98ca-1f37b1e98f98"
        ],
        "3291ca83-c7b4-4468-9800-c0420a552f98": [
            "0fce7844-5392-43b7-98ca-1f37b1e98f98"
        ],
        "ededd164-aac5-4151-82c4-f5f559dc4a97": [
            "1fcdc33b-4fd9-4506-9b36-5c5da9a27f17"
        ],
        "c52b2c70-df93-419e-9576-06e3f541f85a": [
            "1fcdc33b-4fd9-4506-9b36-5c5da9a27f17"
        ],
        "a7ca805c-68af-48fe-a93d-1cc29e13819d": [
            "34c4287f-d0c9-4723-9066-8ca7b66a7422"
        ],
        "69e29397-690c-4ce1-b7eb-6f2f7ef11c96": [
            "34c4287f-d0c9-4723-9066-8ca7b66a7422"
        ],
        "836a55eb-5056-4dde-a559-f4bd63241e1e": [
            "d8c6e8c4-5ce6-4e58-a663-062150a30cf7"
        ],
        "8fb15e49-9be1-4d03-8b7e-995dd2dbe398": [
            "d8c6e8c4-5ce6-4e58-a663-062150a30cf7"
        ],
        "9fbe61d6-57a1-4ee6-b188-5e7a40192541": [
            "856012c7-c00f-4748-a3dd-8e65056bc7a7"
        ],
        "e5a05106-1949-4ca8-b166-fdec55371bda": [
            "856012c7-c00f-4748-a3dd-8e65056bc7a7"
        ],
        "ae6e8c34-d80c-400d-943c-452fa2112012": [
            "dea6da70-e2ec-4e2b-8d4b-89b47d14b6e2"
        ],
        "3470d8d1-7c21-4cdb-903e-5afba4b8757a": [
            "dea6da70-e2ec-4e2b-8d4b-89b47d14b6e2"
        ],
        "ba0e6068-1742-4108-8cec-235943d90192": [
            "9ee9be3f-8e38-4b17-bd1a-b553c562be9d"
        ],
        "556529e6-a984-43b8-ab5a-7532962ac7da": [
            "9ee9be3f-8e38-4b17-bd1a-b553c562be9d"
        ],
        "88e37aa4-4260-4217-a09f-b39f322b02dc": [
            "ff76f18e-52e7-43a2-9075-87b120c1eeb1"
        ],
        "74d24df4-a5e1-47a8-8ac2-04b28b5fb17c": [
            "ff76f18e-52e7-43a2-9075-87b120c1eeb1"
        ],
        "0aee4500-2d05-4610-a097-9bfe3e26bbe6": [
            "be39f30f-dfc8-4b9a-a151-1cbcc3dd5719"
        ],
        "866e5c3b-4074-49e7-9718-ffb4e02393f9": [
            "be39f30f-dfc8-4b9a-a151-1cbcc3dd5719"
        ],
        "baeb7719-f9f3-4b2e-b9d3-c568af460d75": [
            "51f3224e-c4ef-47cc-9742-dcb9c4715541"
        ],
        "231b7413-36ff-4f0e-ba70-862cca0471d9": [
            "51f3224e-c4ef-47cc-9742-dcb9c4715541"
        ],
        "24e0daf4-3a65-4e35-bb0d-020d4ecf074b": [
            "dbb46945-021c-44ec-9571-ff4c468e9929"
        ],
        "7210af41-8fcd-4caa-9fe1-e11721c35e30": [
            "dbb46945-021c-44ec-9571-ff4c468e9929"
        ],
        "89d9b1d3-d3f1-4ad2-9149-5f110574bbe0": [
            "92e7ed56-f631-434c-b5e5-ec5d2b0626cf"
        ],
        "cc098fea-84c0-463a-88d8-9f5ffbd5417f": [
            "92e7ed56-f631-434c-b5e5-ec5d2b0626cf"
        ],
        "e3c8c1b0-9cad-4554-8c92-e8fd22cf813a": [
            "8b5fc768-6588-47da-8513-c34c0cfa6915"
        ],
        "4326aadb-da7a-4cf0-a415-ee4faf8ea657": [
            "8b5fc768-6588-47da-8513-c34c0cfa6915"
        ],
        "721ba703-8977-4279-aa92-deafd98247f1": [
            "b99d2d35-94fd-47ce-88ee-0be6d484d638"
        ],
        "e26b49e3-51b8-4cb4-8613-a6aa00327c3f": [
            "b99d2d35-94fd-47ce-88ee-0be6d484d638"
        ],
        "c4abc08c-da4f-498a-9a50-72f17082385a": [
            "7995c51c-0309-4a33-81d6-d5473deb7163"
        ],
        "80167ffd-d2a8-4e9f-b6df-3b288c8a5133": [
            "7995c51c-0309-4a33-81d6-d5473deb7163"
        ],
        "bff4dbe2-ef57-4ce8-b187-1f269b7a5882": [
            "e6562a7d-d8c9-4238-ae6e-7a2c99154281"
        ],
        "224fce3e-0fe8-40ee-a444-e46cce478cb8": [
            "e6562a7d-d8c9-4238-ae6e-7a2c99154281"
        ],
        "ed5e8163-a894-46ef-8431-6525e7bd34dc": [
            "e515f468-7395-4d6f-bbdb-074349cb016b"
        ],
        "d001f16f-f2f9-469e-8b49-d48f99fdd427": [
            "e515f468-7395-4d6f-bbdb-074349cb016b"
        ],
        "b94c9cfb-fcd3-4c27-9154-14734c74177c": [
            "108d8f2e-b0f0-4658-9d8e-e1764ff52d28"
        ],
        "592eb0ae-dbc0-4c11-940e-860a06b23f57": [
            "108d8f2e-b0f0-4658-9d8e-e1764ff52d28"
        ],
        "5f915c74-6299-41f7-945b-953a496111f6": [
            "ce9df98f-c18c-4d9e-a547-5b03d7a8fcfb"
        ],
        "150c7599-a4fb-46db-8189-fa5580b74497": [
            "ce9df98f-c18c-4d9e-a547-5b03d7a8fcfb"
        ],
        "ee1000c5-ac1f-4632-a7b8-921ae85d2451": [
            "efe65447-e65d-408c-bfd3-936f42915f26"
        ],
        "20f73c85-61c9-4b15-9ee3-b4aeb63560d2": [
            "efe65447-e65d-408c-bfd3-936f42915f26"
        ],
        "6d66be93-5ab6-40d2-a631-52a129c70b1b": [
            "e04354f4-c862-42d6-b96c-0566a1a6c709"
        ],
        "ea4ff6e7-5d6b-4b4b-bb2f-72c08a2fe154": [
            "e04354f4-c862-42d6-b96c-0566a1a6c709"
        ],
        "6404b684-3ed1-4582-9726-10e0c8e2a3f0": [
            "e939a4ce-bba4-443e-b3a9-4d18ac9ce0c8"
        ],
        "4ebab669-f2fb-4d21-8526-49dd3ad5b1c0": [
            "e939a4ce-bba4-443e-b3a9-4d18ac9ce0c8"
        ],
        "eec07bc8-a2fe-40ee-b8f8-ddb20ced60dd": [
            "1b2bedd8-aa0a-4c58-a952-e04b2f6366d7"
        ],
        "f3baf48a-5c5c-42ff-8d68-2cd833f676a1": [
            "1b2bedd8-aa0a-4c58-a952-e04b2f6366d7"
        ],
        "da166ca7-3f85-4e08-9515-a1724cd7b836": [
            "50f6892c-20c7-4c97-a077-be4a2602b0d6"
        ],
        "71b09207-530d-46b6-bc9e-faa23fb9290f": [
            "50f6892c-20c7-4c97-a077-be4a2602b0d6"
        ],
        "fc2ba4ae-1fce-4f08-8b12-9148ff46ac37": [
            "9fa3a647-d409-4d62-ad67-d8d2adfb37e2"
        ],
        "94ccee83-299b-4751-86a9-25f09dc5c29c": [
            "9fa3a647-d409-4d62-ad67-d8d2adfb37e2"
        ],
        "bb3937a9-c457-43b6-a927-7be113378ccf": [
            "dd29fac7-18f3-4a41-81dd-ebf825238856"
        ],
        "336aa023-503d-4d52-8591-2593363f0b15": [
            "dd29fac7-18f3-4a41-81dd-ebf825238856"
        ],
        "6b748eef-522c-411c-b03b-702ec72d8eb7": [
            "abab9326-5a42-4b1f-95fc-9527e8fee2aa"
        ],
        "7f77ad0f-a828-4dd3-8d65-b36610f5fe0e": [
            "abab9326-5a42-4b1f-95fc-9527e8fee2aa"
        ],
        "bd88861b-37d7-4920-8d02-71e81ac078dc": [
            "dc51b7f4-f722-4876-af5f-30d2e1108215"
        ],
        "a118884e-0844-49c6-b074-2cf3fba8db24": [
            "dc51b7f4-f722-4876-af5f-30d2e1108215"
        ],
        "11aac493-93c4-413f-8f40-32587e77cc9d": [
            "01e8d9f2-afbb-4cda-9ecb-a4b73b7e3410"
        ],
        "6b51f5a0-4ac9-4917-a906-a74c2b315cf0": [
            "01e8d9f2-afbb-4cda-9ecb-a4b73b7e3410"
        ],
        "c4df6e78-bc37-4712-a206-5b20a61669be": [
            "6d9ab157-3e1d-4027-8af5-c3fe180eaaa1"
        ],
        "e42f5e53-b429-40c6-b670-e9b771e33fb9": [
            "6d9ab157-3e1d-4027-8af5-c3fe180eaaa1"
        ],
        "5f6f15d9-f13d-475b-99a7-c09c0989af82": [
            "804c9dc9-b198-4486-8ccf-65108dfbb6bb"
        ],
        "c101a532-847c-4e80-8f5f-a45e860995d9": [
            "804c9dc9-b198-4486-8ccf-65108dfbb6bb"
        ],
        "6f2a6aaf-4b02-4ceb-a989-5963c2940b78": [
            "2c1ab317-014b-4afb-887b-fe09039484ec"
        ],
        "63d39e61-645b-43cb-8d1e-d70a3dfec031": [
            "2c1ab317-014b-4afb-887b-fe09039484ec"
        ],
        "cc1007b9-4dad-4c74-91f6-7907ffed1444": [
            "06ec0d92-c795-4a23-9db4-04317e569e2a"
        ],
        "9f01096d-b723-416f-b138-bbae8fddd3f2": [
            "06ec0d92-c795-4a23-9db4-04317e569e2a"
        ],
        "833713f0-200a-4371-a643-b5bae5af33d2": [
            "de33c8c8-d33d-4eda-858e-3fe227f0621f"
        ],
        "26537f5d-b641-47bc-a977-3f42a6fcd236": [
            "de33c8c8-d33d-4eda-858e-3fe227f0621f"
        ],
        "00421cc6-11e7-4798-9aaf-866a2e467f29": [
            "2a05d9a6-1fab-445c-a9ef-794817645db8"
        ],
        "80c18fb1-1af7-414e-b4b0-8d86c4d293ef": [
            "2a05d9a6-1fab-445c-a9ef-794817645db8"
        ],
        "d7fa089f-18df-44c1-81a8-ae891404abd6": [
            "446dc478-b255-4c65-ac05-a2bbc79c2e77"
        ],
        "26cb7140-5dfd-4edb-8c58-d859cfdfb973": [
            "446dc478-b255-4c65-ac05-a2bbc79c2e77"
        ],
        "527bb7ef-2e96-46d9-ae3e-daa2e504a80d": [
            "7bbb8d9e-cd05-4ff9-acbd-ebd77241acb4"
        ],
        "60d835f8-cf1c-4bea-8e7f-91c23276c7e8": [
            "7bbb8d9e-cd05-4ff9-acbd-ebd77241acb4"
        ],
        "c35e19bb-d5e6-400c-97ab-6d390b33338d": [
            "a9149a8f-694b-4cf8-b18d-a4ccff1b6957"
        ],
        "5474dffc-211b-4a25-a822-43d6d60c3bf2": [
            "a9149a8f-694b-4cf8-b18d-a4ccff1b6957"
        ],
        "64aa3eaa-b06e-47fc-92c5-8fe45c4be1a5": [
            "69fcb657-09eb-42b5-9147-372584004e59"
        ],
        "daad9b85-6a4f-48d0-9fd0-b717ab6df408": [
            "69fcb657-09eb-42b5-9147-372584004e59"
        ],
        "bb1a9f3b-5057-4486-a2df-a1ce094b5ba9": [
            "d43a81ad-1760-4022-80fd-41a6d534f673"
        ],
        "6365c693-4f5b-4b58-9ed5-fed5bde5147f": [
            "d43a81ad-1760-4022-80fd-41a6d534f673"
        ],
        "a730eb4a-d5e4-46cd-a4d8-ac50a64b44d0": [
            "1e8956e5-a52e-4568-a4c0-a9e95184e913"
        ],
        "0d4f9c80-2d6b-41f5-97fb-1df8c3aa10f9": [
            "1e8956e5-a52e-4568-a4c0-a9e95184e913"
        ],
        "5bd81556-efbd-48fc-9459-d6e2f72e876a": [
            "7567e824-cd5e-411c-b218-0ded5cdfecef"
        ],
        "f508c582-43aa-4e85-9d7e-8d3897ba80c2": [
            "7567e824-cd5e-411c-b218-0ded5cdfecef"
        ],
        "4579a9e6-064d-4fd2-94f0-031d22b6f468": [
            "821d0fb8-34c9-4bee-9335-fd79c846c3c5"
        ],
        "0c933105-750a-49ff-b635-2c8623296be7": [
            "821d0fb8-34c9-4bee-9335-fd79c846c3c5"
        ],
        "d4050685-0887-4efb-a28a-426bc7fc4d80": [
            "53108509-33f0-4f9c-b63b-a9d2052d66e9"
        ],
        "aa6ea049-dab9-42a2-a59b-a0c5f29d1747": [
            "53108509-33f0-4f9c-b63b-a9d2052d66e9"
        ],
        "0b6b57e0-edbb-431d-97ff-d3d3e76ad21a": [
            "7da59371-5fbb-41e6-b179-b782a26a4804"
        ],
        "4074b2b6-f32f-4a70-9e9d-ce42124b4db2": [
            "7da59371-5fbb-41e6-b179-b782a26a4804"
        ],
        "0f63060c-461b-419c-b84a-13b8703d9ee4": [
            "73f4df85-088e-4332-93e0-0a1319006cca"
        ],
        "03333244-f1aa-4c8b-9dc7-64bd4b16976f": [
            "73f4df85-088e-4332-93e0-0a1319006cca"
        ],
        "2ff87597-a4c2-4ef9-bea9-79abfb7df808": [
            "14cd157d-302f-4369-82b4-1e1931357725"
        ],
        "6b77e508-7a34-4f4b-a4f3-1c8abdb354e6": [
            "14cd157d-302f-4369-82b4-1e1931357725"
        ],
        "791c517e-81a4-44eb-a3a1-f142a90f5a55": [
            "fbb1ab2b-3d84-45be-b39e-68f48d584514"
        ],
        "1373d884-b18d-466f-a966-9de01bdaaf9a": [
            "fbb1ab2b-3d84-45be-b39e-68f48d584514"
        ],
        "bf38c8f6-97bf-457d-a910-dc3a65c300b5": [
            "8ee2a4d7-1a8a-479c-a6a0-42b084d864be"
        ],
        "f139797d-3499-4622-af90-68ab9114e33f": [
            "8ee2a4d7-1a8a-479c-a6a0-42b084d864be"
        ],
        "bfe55540-8d3b-4dc6-a6f3-67e1d59d1018": [
            "c08d39f5-078b-4d21-91f1-8d80cc6b5d73"
        ],
        "d87f6f1e-8560-4f45-89c5-d3cb05725252": [
            "c08d39f5-078b-4d21-91f1-8d80cc6b5d73"
        ],
        "e4d63c6c-be52-47af-96f3-41b58edb5aa3": [
            "ec38cd34-aa8a-4dd8-9dce-d80c6cdb3d61"
        ],
        "5186638c-1957-4a0b-b2ee-8f9308666001": [
            "ec38cd34-aa8a-4dd8-9dce-d80c6cdb3d61"
        ],
        "9d8eb893-37a2-421e-aa98-551dc9f0ee9c": [
            "5a3877dd-9170-48bb-8c58-1641195a7c1e"
        ],
        "dd6319ce-88fe-4ce9-88fb-2946aa82036f": [
            "5a3877dd-9170-48bb-8c58-1641195a7c1e"
        ],
        "7d93be58-ab51-4b3a-b25a-78a09787cde2": [
            "dd5890fd-7926-4d1e-84b8-bc3f3e91990b"
        ],
        "c4d80a00-0f67-456f-9cbf-8f6cf20c51fd": [
            "dd5890fd-7926-4d1e-84b8-bc3f3e91990b"
        ],
        "24b4c47f-e30a-4912-8173-88325e7c541b": [
            "92955ec2-0c31-42d7-bbf7-51a8f246680a"
        ],
        "0ebb882b-40b6-48bf-81c6-35932f151e8f": [
            "92955ec2-0c31-42d7-bbf7-51a8f246680a"
        ],
        "da6d049e-c155-4d30-9e4c-4bc2b60fcaf2": [
            "eae273a8-806f-48df-9a2d-640426c698c3"
        ],
        "e44c3f33-1484-47ab-bc4f-ab169400ca3f": [
            "eae273a8-806f-48df-9a2d-640426c698c3"
        ],
        "a77ee52b-3044-456f-98b7-6bcafc3ccc2d": [
            "eed98ce0-39f5-468f-a6bc-35998f04278d"
        ],
        "99bf3524-18d9-4ebe-8960-0de946cced40": [
            "eed98ce0-39f5-468f-a6bc-35998f04278d"
        ],
        "be3db9a1-9e55-4b9e-943f-ffea1df276e6": [
            "a7c9c245-3c5e-45f0-aebb-e54231f1a1b0"
        ],
        "71b4b5ea-3c06-4777-a017-7ce161d9655b": [
            "a7c9c245-3c5e-45f0-aebb-e54231f1a1b0"
        ],
        "dede9cea-7c85-43ef-89fe-0285ed028c2a": [
            "c17d7649-d2c3-412a-96bb-1e8e982bf78a"
        ],
        "60dc53ff-2421-4b3f-8e4d-5528419e6eab": [
            "c17d7649-d2c3-412a-96bb-1e8e982bf78a"
        ],
        "3a0557e6-8be8-4a44-9219-96ef07ba8bb6": [
            "9f059ff5-2eb3-44ac-a4a0-fce64579f2e0"
        ],
        "167a2798-67ba-4751-b0a8-a5d55a1d6707": [
            "9f059ff5-2eb3-44ac-a4a0-fce64579f2e0"
        ],
        "7cb70657-8312-4889-99cb-cf0642e6b0e1": [
            "6babadfb-176b-494f-afc1-5d097325dc55"
        ],
        "cfdb9076-7777-4477-a97e-e48f3062ac08": [
            "6babadfb-176b-494f-afc1-5d097325dc55"
        ],
        "b4be092a-1a6f-467c-bccb-3ac7076588e4": [
            "31805a36-aabc-4885-9aac-ad6b1a460135"
        ],
        "5cefa67c-7be5-4188-b61c-6ef6ce63750d": [
            "31805a36-aabc-4885-9aac-ad6b1a460135"
        ],
        "51f936bf-b7eb-418a-b07d-0aff454af1ea": [
            "6a9f2c26-4833-4c6f-abb8-c51b5f569137"
        ],
        "52937241-4951-4924-8b5f-c904ddcc858d": [
            "6a9f2c26-4833-4c6f-abb8-c51b5f569137"
        ],
        "e5ed741b-eb0d-427a-8819-c9f30e81fade": [
            "baef78d2-6922-4d4c-818b-1c8f8335a857"
        ],
        "c3c964bc-7af2-4102-87a6-09e103721c3f": [
            "baef78d2-6922-4d4c-818b-1c8f8335a857"
        ],
        "87fe4d33-5b9e-434c-a93a-4798c14b7386": [
            "07c07394-394b-4712-ba71-e7bb6693dbab"
        ],
        "b4f42ff1-8bba-4210-98fa-1b9ea878c756": [
            "07c07394-394b-4712-ba71-e7bb6693dbab"
        ],
        "0183a14d-4e57-40f7-ac09-958833f04766": [
            "68c134dc-a062-48b4-9fc6-2c4219499010"
        ],
        "7c239a0d-324b-4f0f-bfcf-020ae218f6ae": [
            "68c134dc-a062-48b4-9fc6-2c4219499010"
        ],
        "d98a39d8-1599-4831-af42-76c6e9501205": [
            "22720652-1ff3-48b9-876d-17e10a79b424"
        ],
        "48908f72-951e-4be2-880e-7b464be0b9ff": [
            "22720652-1ff3-48b9-876d-17e10a79b424"
        ],
        "71bd8594-4e00-4d59-afd3-625204b50206": [
            "ba245602-906f-4066-894d-2ed1734fab1e"
        ],
        "bd4c8bf0-bec9-494b-8f9e-e3f363184c2a": [
            "ba245602-906f-4066-894d-2ed1734fab1e"
        ],
        "78841751-511e-41c4-bb27-4fa4e63e5d25": [
            "1194c0a3-4a7f-46b2-9836-122e2bcc3e7c"
        ],
        "d9bcb5a5-d0f1-404f-aa2a-00bf6fcb34fa": [
            "1194c0a3-4a7f-46b2-9836-122e2bcc3e7c"
        ],
        "97edaf4f-08b5-4b27-a87d-5b4a15328c0f": [
            "1c68d4e4-fb97-415d-9898-2d4ac517ddc0"
        ],
        "d307226e-f7ae-4ac4-a792-6fc5430cd8fe": [
            "1c68d4e4-fb97-415d-9898-2d4ac517ddc0"
        ],
        "3a7960b6-bfb3-408a-9732-6b79c85456d8": [
            "d28fad82-10e6-44dd-943d-7681f6baa4c0"
        ],
        "390a5c57-d691-45f8-8280-56c33760a9be": [
            "d28fad82-10e6-44dd-943d-7681f6baa4c0"
        ],
        "9f188e2e-7465-477d-8fbd-cb0cb31fe670": [
            "af2a3a00-f6ee-4f8e-8493-de49e367015e"
        ],
        "61990196-4838-4d50-bf1a-7054003195cd": [
            "af2a3a00-f6ee-4f8e-8493-de49e367015e"
        ],
        "c83d196a-cc9f-4945-9839-501ce7b49dd5": [
            "e5bd64bd-e8e1-4cb2-9f54-798afede9a4a"
        ],
        "7751fa17-f1bd-441f-a940-24fc7b5ccca7": [
            "e5bd64bd-e8e1-4cb2-9f54-798afede9a4a"
        ],
        "2ad54142-6259-4836-a99c-c0008ce27cf7": [
            "346aa2e5-5fc2-4933-9ab1-0b2ecfed1e96"
        ],
        "99ff9c2c-0f90-482d-9779-7bbfdca0239e": [
            "346aa2e5-5fc2-4933-9ab1-0b2ecfed1e96"
        ],
        "22272e91-d3b5-476d-ae44-d3fde41348ec": [
            "5c927126-fec3-4773-b67c-a3d503c46bd7"
        ],
        "053a09e4-bdc9-471e-94a7-6c9e860096e7": [
            "5c927126-fec3-4773-b67c-a3d503c46bd7"
        ],
        "817d95d2-4ee5-420a-9a8c-b896025ee814": [
            "e8d12c06-3d34-477e-b3ff-cfad4c4c7206"
        ],
        "fb773fe8-61bb-4330-80b8-f678a9dab1fe": [
            "e8d12c06-3d34-477e-b3ff-cfad4c4c7206"
        ],
        "16125b5a-2423-4c28-bd57-56f173e31365": [
            "c6e3a5ac-5f12-48ae-b5ef-113bea634d25"
        ],
        "20cfc0a6-5970-475a-ae62-8fc77f9437c5": [
            "c6e3a5ac-5f12-48ae-b5ef-113bea634d25"
        ],
        "84af0462-0c15-4b43-8e59-e57fd058166d": [
            "b183149e-359e-4921-a5ac-9830603947dc"
        ],
        "88e01ebb-1946-4d4b-b1c8-7ba7d986a128": [
            "b183149e-359e-4921-a5ac-9830603947dc"
        ],
        "c2fdb392-1284-4b7b-b219-40390805d967": [
            "d69d94ba-381d-4c74-8ec7-0a80bf0135f2"
        ],
        "b48176a6-7dcc-4a80-a5f4-e0abb63b85b2": [
            "d69d94ba-381d-4c74-8ec7-0a80bf0135f2"
        ],
        "a26a9ccd-29b5-42b2-875d-b1a286ab0d2a": [
            "9149fb6f-b6a0-43d9-a7d1-e153021afd76"
        ],
        "dfb65897-3eb1-4fa9-8438-26fe6222b76e": [
            "9149fb6f-b6a0-43d9-a7d1-e153021afd76"
        ],
        "3965eac5-366e-45ae-b7f9-843cd6cc8d8f": [
            "95a0c2e5-2407-4614-94aa-c600a79229a3"
        ],
        "bac03fbd-21e1-48d2-8a56-6d206b004c7a": [
            "95a0c2e5-2407-4614-94aa-c600a79229a3"
        ],
        "305e5f32-d2c8-4641-a872-eae226ce4dce": [
            "644cccac-1820-4a01-b1fe-cbd81d74e355"
        ],
        "d7345aaf-24d1-4ed2-9919-ceba06e38413": [
            "644cccac-1820-4a01-b1fe-cbd81d74e355"
        ],
        "ec674120-98eb-42d7-a53a-de67b4ee5840": [
            "48461d0d-28f5-4431-9e93-17fffdba3368"
        ],
        "391a4990-d591-4fb8-925c-7581bd766ce3": [
            "48461d0d-28f5-4431-9e93-17fffdba3368"
        ],
        "dde7bc1d-078d-423b-b24c-9d0a362b6dec": [
            "446177f8-bab6-494f-aefc-7af550fba5b9"
        ],
        "c43e9d3b-cd3f-4ef6-9c28-58fcb0a059e3": [
            "446177f8-bab6-494f-aefc-7af550fba5b9"
        ],
        "26317eaf-3e78-4cad-8982-d8db906ad057": [
            "1cd0cdf3-d656-4de2-9455-41c2279b42fa"
        ],
        "dea87981-bc80-46ac-b62e-de8655e332c8": [
            "1cd0cdf3-d656-4de2-9455-41c2279b42fa"
        ],
        "0357aae3-7164-4757-a4e8-c7eb4a3aa593": [
            "c9b6dd7b-2c3e-43cf-9f4d-2a96b9f631ca"
        ],
        "e81bc760-f57f-4c25-b56f-7009c78ebba7": [
            "c9b6dd7b-2c3e-43cf-9f4d-2a96b9f631ca"
        ],
        "5e55218a-dc30-4af3-ba95-38d484c2402b": [
            "79b4fcb0-bfdf-4e2b-98ba-a28ad747843f"
        ],
        "ccf10143-449f-4439-ae1e-0513606dfeca": [
            "79b4fcb0-bfdf-4e2b-98ba-a28ad747843f"
        ],
        "b5c4ed7d-d2c7-4d66-a084-0f7b07175fb6": [
            "04aa873f-faea-49eb-9d7c-e2f925aa1f89"
        ],
        "91b12e78-49f4-445d-a885-e96217be022b": [
            "04aa873f-faea-49eb-9d7c-e2f925aa1f89"
        ],
        "92cbc0b1-8691-4798-93ca-ea11bb5ee29e": [
            "0c3dd57f-8d01-4409-9310-3d27291e63cc"
        ],
        "034d1aea-be41-40f0-8b08-3482d59a5368": [
            "0c3dd57f-8d01-4409-9310-3d27291e63cc"
        ],
        "e5dfd173-172a-4993-9593-3c3608dc20b8": [
            "464c77d7-139d-4499-9f99-b62f19d9914d"
        ],
        "c51579ab-b46c-43df-8e1e-23613e34d329": [
            "464c77d7-139d-4499-9f99-b62f19d9914d"
        ],
        "63bfc778-548c-44e1-bd0d-41bd29194d38": [
            "5f325b1a-5e61-41e6-9bcd-36b9499ad88d"
        ],
        "6aa7445c-0584-4149-a953-9ff75a4627a9": [
            "5f325b1a-5e61-41e6-9bcd-36b9499ad88d"
        ],
        "753bc970-94bd-4ece-aac5-c28054343f48": [
            "7ed7b0dd-bec9-4b6a-9a64-b1a4df0913c9"
        ],
        "ed5ff73e-6af1-4400-b168-67aa93323f99": [
            "7ed7b0dd-bec9-4b6a-9a64-b1a4df0913c9"
        ],
        "72f2bf37-f6b3-45e7-bc0f-bf1d5fb06a8b": [
            "a70c55ea-c3c2-41fe-8f4b-457722afbd06"
        ],
        "b4011397-e70a-4ba1-b1a7-a8c93be8cc99": [
            "a70c55ea-c3c2-41fe-8f4b-457722afbd06"
        ],
        "7fc7e696-3b2b-453a-9362-c7f582b32c51": [
            "21f137e3-9d64-4c4b-9a05-a4e20a086d73"
        ],
        "27860a9a-f096-4dc6-bfbf-35db0f89fdb2": [
            "21f137e3-9d64-4c4b-9a05-a4e20a086d73"
        ],
        "a601e64a-a608-486a-ac74-3ebb8f879524": [
            "37ee2ce5-7e29-4f74-9a0e-d88294bfc47e"
        ],
        "bac7e518-ba40-4999-8426-c6339909e7e6": [
            "37ee2ce5-7e29-4f74-9a0e-d88294bfc47e"
        ],
        "f1361daf-3bd8-4c9c-aada-8a9ed95ba98f": [
            "071cea47-f792-4cbf-ab2f-3d9d82901d62"
        ],
        "715b0d2a-d546-4a6d-a68c-7681008fdb98": [
            "071cea47-f792-4cbf-ab2f-3d9d82901d62"
        ],
        "7d09bd3c-f996-4f37-b5b3-6770c603bdab": [
            "eb6e9722-96bb-4352-88b0-0f52c1c6aab0"
        ],
        "dff95118-9eb3-473b-9bdc-3d6d97a98655": [
            "eb6e9722-96bb-4352-88b0-0f52c1c6aab0"
        ],
        "a472a9bf-49b6-4a80-bf9b-40b86847f6df": [
            "42265957-6c1d-41fb-94b2-00b4ba81a6c1"
        ],
        "cd552f0f-4f27-4125-9a3a-cb546bc283b3": [
            "42265957-6c1d-41fb-94b2-00b4ba81a6c1"
        ],
        "a5ae788a-5a15-4695-96cc-e0d6fc6d17b3": [
            "f62021c2-9c5e-42f7-986a-9b5528c9a149"
        ],
        "7af1eb53-2fce-459e-b999-4f8497ba81a5": [
            "f62021c2-9c5e-42f7-986a-9b5528c9a149"
        ],
        "06b20ee2-3f1d-4ecd-9e10-e4fe67250136": [
            "16d7d6c6-d456-48a0-b2f3-d5f7c5db248b"
        ],
        "76a4ef90-2718-4967-aa3f-92daadb698ba": [
            "16d7d6c6-d456-48a0-b2f3-d5f7c5db248b"
        ],
        "f1e73128-be61-46ed-b6df-26a286e2ff87": [
            "b8d9bb19-327f-41a8-b71d-c37c9c518d63"
        ],
        "5d153f15-368e-4669-bfdf-996c3fde464a": [
            "b8d9bb19-327f-41a8-b71d-c37c9c518d63"
        ],
        "34522455-1723-466c-a65a-5913d3c0511c": [
            "ea86d536-b052-45f7-91a0-ff61f0272f9f"
        ],
        "c394f616-7c6b-427b-bce5-7c0d89973ec8": [
            "ea86d536-b052-45f7-91a0-ff61f0272f9f"
        ],
        "5bba1f81-a2ec-4a78-bfc8-bb285c44d3f9": [
            "305779f3-f006-4656-805c-536242a7c040"
        ],
        "0f75c514-8376-456e-af67-2a031c6231e4": [
            "305779f3-f006-4656-805c-536242a7c040"
        ],
        "e5340873-7dd3-4ee4-be9d-17fbf6cc2433": [
            "dafcdc9d-8e92-4147-92bf-ceca55defaf1"
        ],
        "93b2ce25-b542-4c24-ba5b-75fbfc33f0cb": [
            "dafcdc9d-8e92-4147-92bf-ceca55defaf1"
        ],
        "a5cfc2be-baf8-41be-9199-426ec299067f": [
            "5a60e651-325d-494f-8687-8f875871653c"
        ],
        "f88280cf-ae29-4cc4-985d-a9f71766cd4b": [
            "5a60e651-325d-494f-8687-8f875871653c"
        ],
        "ca485435-fbfc-4e67-81a8-709aeb11d821": [
            "5204df8d-781f-4147-bc4d-58e825dc00b2"
        ],
        "fcd85c96-b0fc-4f5c-9bbf-0f2fbe99bb8e": [
            "5204df8d-781f-4147-bc4d-58e825dc00b2"
        ],
        "513206c6-a666-4698-a148-99891d8966ad": [
            "0dca5567-2953-4d3a-a880-e202c4178f49"
        ],
        "7e423907-0fbb-4ab8-a2dd-7c9d059fc587": [
            "0dca5567-2953-4d3a-a880-e202c4178f49"
        ],
        "73d7ed2b-0afc-493b-9c25-88c2f5f1ddaf": [
            "c3894a95-e0fa-466a-8ba5-b377a0b7bfea"
        ],
        "1674c69b-c370-4ec8-a8bf-a08fac84cc14": [
            "c3894a95-e0fa-466a-8ba5-b377a0b7bfea"
        ],
        "ab6b8c31-48ba-4aab-84fe-eaddbbae66ab": [
            "2c6ac779-e678-4c8b-b4c6-5d0707b65f21"
        ],
        "e3fa1d35-8e26-4960-a000-0520303bae57": [
            "2c6ac779-e678-4c8b-b4c6-5d0707b65f21"
        ],
        "c7f2c36a-7b1b-4cbd-b0d8-402285a9de29": [
            "ffac8e9a-c9c5-419d-b143-0f08ea400706"
        ],
        "34f3f54a-adc3-479f-91cd-4e5bcc3c7a7a": [
            "ffac8e9a-c9c5-419d-b143-0f08ea400706"
        ],
        "60999e51-3d1d-4dfc-b180-e8a5dd1d6ec9": [
            "9c84a534-9351-46a4-b869-34d44ef18de7"
        ],
        "b5aa0c62-5492-40a1-b936-5be18960fb77": [
            "9c84a534-9351-46a4-b869-34d44ef18de7"
        ],
        "0393f2f3-4bac-4080-8ab8-df036a658b41": [
            "7a3c54f0-e4e7-45e8-9e62-ba132e22cf07"
        ],
        "681d72ce-8320-4372-9f6b-28b0e99d5520": [
            "7a3c54f0-e4e7-45e8-9e62-ba132e22cf07"
        ],
        "d4248622-64fa-4562-9aa3-4433ab25bedf": [
            "5de200b0-8ce9-47b1-8e9b-9467bf6453a7"
        ],
        "a4f8e29a-13d2-46ff-8513-3ac3e8b37a57": [
            "5de200b0-8ce9-47b1-8e9b-9467bf6453a7"
        ],
        "13220965-850a-47ae-9f07-dd36450e9328": [
            "5cc55ac9-91aa-464e-8fb9-482189330381"
        ],
        "3ab47285-590f-41a2-ab4b-31a5d43acdc9": [
            "5cc55ac9-91aa-464e-8fb9-482189330381"
        ],
        "155f420d-7d1e-4ab8-8fda-a5e9fc70bea2": [
            "9bc1f2b1-4183-48db-bbbc-b21a1000753e"
        ],
        "42bb915e-04f5-49a0-8cea-58b014a9dc03": [
            "9bc1f2b1-4183-48db-bbbc-b21a1000753e"
        ],
        "c051bb67-47f4-4272-9701-f69ef465feb1": [
            "11f36c35-c274-4133-991b-07b67b55356f"
        ],
        "6036b774-5a65-4e20-8351-4fb83f3813b3": [
            "11f36c35-c274-4133-991b-07b67b55356f"
        ],
        "1dbe1d19-dc75-4d91-a6df-e4599fa72def": [
            "9c53e5a7-21a5-42f7-8cc4-3e836a7afb32"
        ],
        "b2ac4dd1-484d-4f78-9796-ec434f76dcb8": [
            "9c53e5a7-21a5-42f7-8cc4-3e836a7afb32"
        ],
        "03a6c567-4e35-4a59-8887-298a93e815d0": [
            "cd3eecba-8979-4ee5-9a1c-28c0ca8b154c"
        ],
        "e22f9b88-36ce-47e0-a1e6-a8d45648feab": [
            "cd3eecba-8979-4ee5-9a1c-28c0ca8b154c"
        ],
        "29d17e29-2984-4ad6-b7a7-51c2705e9c25": [
            "a7bb6a81-ad47-417c-921e-4421bc2a30ee"
        ],
        "004acf5c-3513-4e2f-b6d5-32d2730e938d": [
            "a7bb6a81-ad47-417c-921e-4421bc2a30ee"
        ],
        "c3ed82bd-b1bc-48a5-8a2d-34bbb7c4e6e8": [
            "ed7a9951-cc93-4dbd-b728-4bc43837ead2"
        ],
        "1db8d5e8-fe4c-4033-9201-1bceb809b487": [
            "ed7a9951-cc93-4dbd-b728-4bc43837ead2"
        ],
        "6d7a20fd-4fe6-4679-9b79-951d7d700f17": [
            "7263006d-e2fb-47a3-b8a9-4521c3783b99"
        ],
        "aad1722f-57ec-45ab-b8a2-5c5fbcd1f9cc": [
            "7263006d-e2fb-47a3-b8a9-4521c3783b99"
        ],
        "0a5e3873-bfa2-43c6-a0d3-529c812f7bcb": [
            "b3235467-9611-4557-92a0-fae372e56093"
        ],
        "845d5d2e-a747-4265-b400-31cb693e42ee": [
            "b3235467-9611-4557-92a0-fae372e56093"
        ],
        "39c407d4-3f37-4c60-a642-3438d7381bfc": [
            "3825ed20-17e8-459f-8673-a0cf518a51fd"
        ],
        "48e8da45-dd8a-42a3-b873-27209264899d": [
            "3825ed20-17e8-459f-8673-a0cf518a51fd"
        ],
        "05548231-c177-456d-b3e5-58b9e2b62fb0": [
            "1d6adf53-9ec3-47cc-9643-72611e61b00c"
        ],
        "ebbc936e-c559-4051-92e2-c91ea5b6e7c7": [
            "1d6adf53-9ec3-47cc-9643-72611e61b00c"
        ],
        "0c244b80-dfe6-43f6-a405-50164bb52e53": [
            "fac66c59-efce-47b3-82ce-bf22cd306770"
        ],
        "c2e3a2be-8fda-44b0-b8a3-a637f34fac8d": [
            "fac66c59-efce-47b3-82ce-bf22cd306770"
        ],
        "691479f1-5cb1-41bd-998e-3cee8e655067": [
            "5bf88380-d9ee-4ea7-916e-7f7e3ae85d42"
        ],
        "6d50fc0a-1e7e-46fc-af95-4b134bedf5ee": [
            "5bf88380-d9ee-4ea7-916e-7f7e3ae85d42"
        ],
        "b2503c70-3f97-4905-a5bd-3d46c3798df2": [
            "5b0a97fe-d70e-42b0-bece-47a3a3026560"
        ],
        "cc7665d2-ebd2-4eac-9181-2aaf7c61d7ae": [
            "5b0a97fe-d70e-42b0-bece-47a3a3026560"
        ],
        "8994bfcf-fe30-47bc-8654-f7dbb9292162": [
            "70e3f3d2-5f32-4edf-b9eb-cf2a87505a26"
        ],
        "5a52ed90-c136-4901-98c5-c57df79db3cf": [
            "70e3f3d2-5f32-4edf-b9eb-cf2a87505a26"
        ],
        "404149e8-b54e-4a93-87bf-520fe06d3863": [
            "1bd0cfd1-0a94-499b-b2be-2e2d2d2fc780"
        ],
        "ebf71163-c4e8-49be-9bba-8baf3afef799": [
            "1bd0cfd1-0a94-499b-b2be-2e2d2d2fc780"
        ],
        "4c70a4a8-3956-4ccf-81fe-165261098a57": [
            "9765b803-ba3b-4c24-b26d-96e475cbc431"
        ],
        "aff4e09f-426e-400c-acd6-af03e5840bb6": [
            "9765b803-ba3b-4c24-b26d-96e475cbc431"
        ],
        "620a9d40-7935-4d0e-adbe-5d755474675a": [
            "95313052-639f-4d04-b29f-9fe768de9eee"
        ],
        "c8bc29b9-afb9-4f78-b07e-c126f0a6d36c": [
            "95313052-639f-4d04-b29f-9fe768de9eee"
        ],
        "1aaf467c-4f2b-4ba0-b5e4-d3c5f3f9d9a3": [
            "fe52d870-f0a6-4b74-96d3-05d039ecc09d"
        ],
        "27d7b20c-23bd-44c6-8caf-fb42ccd2ff1a": [
            "fe52d870-f0a6-4b74-96d3-05d039ecc09d"
        ],
        "5337f313-890e-42dc-8f31-8120177ff249": [
            "aaa351dd-6c11-41d5-ad19-e2953bdd405d"
        ],
        "3a07366a-2b65-49bd-ab20-dce48451ecb8": [
            "aaa351dd-6c11-41d5-ad19-e2953bdd405d"
        ],
        "31e609dc-23a9-412e-bf5a-93ce8566dcdd": [
            "1c73f977-a021-431b-8843-7e72d0d28066"
        ],
        "32049bd5-4c5e-446d-9ed3-de756e3b34e8": [
            "1c73f977-a021-431b-8843-7e72d0d28066"
        ],
        "6c08d155-6f1f-4a48-8fd7-fc4e8d6cac80": [
            "56b1019b-e4b6-4dc9-ab71-b2a31a183b37"
        ],
        "91178bb4-f803-4d98-bdff-bd886b333098": [
            "56b1019b-e4b6-4dc9-ab71-b2a31a183b37"
        ],
        "f3c96f6b-c7ba-4e12-955d-00aa97a049aa": [
            "84098651-f2fd-4807-a2a7-c7326e579d9d"
        ],
        "6ab262f4-ddd0-4de5-8b94-f255b00d9619": [
            "84098651-f2fd-4807-a2a7-c7326e579d9d"
        ],
        "792c3e8f-78c6-488a-a718-7da989a19f28": [
            "a5e82535-32f9-4a7d-8962-2e664f6eb9bb"
        ],
        "030491e8-2d10-436f-876f-4813d515f90a": [
            "a5e82535-32f9-4a7d-8962-2e664f6eb9bb"
        ],
        "0ca3025a-6c06-43ad-bd43-b6a7f47bff80": [
            "1cfd446b-158d-42de-b8a2-a596870ef055"
        ],
        "106b631b-dea3-4c3a-977c-ca1a1d8f1f85": [
            "1cfd446b-158d-42de-b8a2-a596870ef055"
        ],
        "96892f8a-29d2-4779-8252-289afe702d06": [
            "ee48c04d-b1e1-4191-937f-f2a71c4064e0"
        ],
        "8ebc7840-6c24-4ce2-9f99-f79ee0a98a6a": [
            "ee48c04d-b1e1-4191-937f-f2a71c4064e0"
        ],
        "9c3fab0d-8820-4e32-aef8-ccaa7cddd504": [
            "81e6d335-5e82-4c83-a815-0a99119c2618"
        ],
        "21f54d72-86e5-4f95-9c42-8f6854efe873": [
            "81e6d335-5e82-4c83-a815-0a99119c2618"
        ],
        "c046292f-18da-4932-ac7c-e426ce48753d": [
            "d75fe20c-deec-4707-82c6-aa78b8b5caa4"
        ],
        "d9a986d1-38d4-42e7-8c17-15ddce4b1d4d": [
            "d75fe20c-deec-4707-82c6-aa78b8b5caa4"
        ],
        "c9708796-e9ae-47f7-87e1-344458378b22": [
            "aa7b0851-9e5c-433e-85bb-7f780ddf92b5"
        ],
        "ae67b1e6-4bdd-47db-b9b6-2844463304af": [
            "aa7b0851-9e5c-433e-85bb-7f780ddf92b5"
        ],
        "28550d16-2506-447a-9fa3-58ffa067fb67": [
            "f8118ae6-c772-42f9-a01f-9958c426526c"
        ],
        "2afd6fff-7a68-4384-9abf-371d34b7c76d": [
            "f8118ae6-c772-42f9-a01f-9958c426526c"
        ],
        "a5ccb6d4-4799-4c77-8c8c-b327f96705c6": [
            "de8ec535-1502-450e-bce5-097e39d0ef29"
        ],
        "917f6be6-f03b-469e-b9a6-ae49e2d6edd9": [
            "de8ec535-1502-450e-bce5-097e39d0ef29"
        ],
        "1a30fed0-52cb-4edf-ac51-b1647a19fbed": [
            "e66a6b8f-4593-49d3-aa0f-934c21232b1a"
        ],
        "f4bac326-6c51-4d3b-979a-e2002d3deecb": [
            "e66a6b8f-4593-49d3-aa0f-934c21232b1a"
        ],
        "096a9342-9aa9-4b7d-8466-875e99952841": [
            "ba07f41b-0143-40d1-8afe-662d4fabdd4e"
        ],
        "833a4bca-fae6-4f4f-bb97-f319283f28d4": [
            "ba07f41b-0143-40d1-8afe-662d4fabdd4e"
        ],
        "3820e76e-5434-44bc-aa7a-68778f9e8dda": [
            "0d03c145-0507-4e65-83c5-3ca6f2df8e33"
        ],
        "9d3f1583-7a5d-4c0e-bb1b-d7a4fc6fe828": [
            "0d03c145-0507-4e65-83c5-3ca6f2df8e33"
        ],
        "ec17b88d-4386-4e9d-8e11-80851dd0eeb2": [
            "e73f60f1-fae4-44a1-8330-edaee28a39e6"
        ],
        "5744b04a-9a86-41ff-86b0-16abae906ba2": [
            "e73f60f1-fae4-44a1-8330-edaee28a39e6"
        ]
    },
    "mode": "text"
}